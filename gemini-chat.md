## USER ğŸ§‘â€ğŸ’»

This is the Gemini CLI. We are setting up the context for our chat.
Today's date is Saturday, February 14, 2026 (formatted according to the user's locale).
My operating system is: linux
The project's temporary directory is: /home/keith_krozario_altostrat_com/.gemini/tmp/8bc920eeaef5108f9dedcf41e4a2241145d861eefbf86fffb11e40a51a5323fa
I'm currently working in the directory: /home/keith_krozario_altostrat_com/projects/keithrozario_dot_com
Here is the folder structure of the current working directories:

Showing up to 200 items (files + folders). Folders or files indicated with ... contain more items not shown, were ignored, or the display limit (200 items) was reached.

/home/keith_krozario_altostrat_com/projects/keithrozario_dot_com/
â”œâ”€â”€â”€.gitignore
â”œâ”€â”€â”€favicon.ico
â”œâ”€â”€â”€instructions.md
â”œâ”€â”€â”€local.html
â”œâ”€â”€â”€migrate_comments.py
â”œâ”€â”€â”€migrate_robust.py
â”œâ”€â”€â”€remote_post.html
â”œâ”€â”€â”€remote.html
â”œâ”€â”€â”€terminal.css
â”œâ”€â”€â”€.git/...
â”œâ”€â”€â”€.github/
â”‚   â””â”€â”€â”€workflows/
â”‚       â””â”€â”€â”€deploy.yaml
â””â”€â”€â”€keithrozario_blog/
    â”œâ”€â”€â”€hugo.toml
    â”œâ”€â”€â”€remote.html
    â”œâ”€â”€â”€archetypes/
    â”‚   â””â”€â”€â”€default.md
    â”œâ”€â”€â”€assets/
    â”‚   â””â”€â”€â”€uploads/
    â”‚       â”œâ”€â”€â”€00MXsWP-150x150.jpg
    â”‚       â”œâ”€â”€â”€00MXsWP-300x255.jpg
    â”‚       â”œâ”€â”€â”€00MXsWP-588x500.jpg
    â”‚       â”œâ”€â”€â”€00MXsWP.jpg
    â”‚       â”œâ”€â”€â”€1-s2.0-S1878029614000279-main.pdf
    â”‚       â”œâ”€â”€â”€10643204815_655b8e60c8_o-150x150.jpg
    â”‚       â”œâ”€â”€â”€10643204815_655b8e60c8_o-300x300.jpg
    â”‚       â”œâ”€â”€â”€10643204815_655b8e60c8_o-500x500.jpg
    â”‚       â”œâ”€â”€â”€10643204815_655b8e60c8_o.jpg
    â”‚       â”œâ”€â”€â”€10710694_10152497851406888_323063805424206601_n-150x84.jpg
    â”‚       â”œâ”€â”€â”€10710694_10152497851406888_323063805424206601_n-300x168.jpg
    â”‚       â”œâ”€â”€â”€10710694_10152497851406888_323063805424206601_n-528x297.jpg
    â”‚       â”œâ”€â”€â”€10710694_10152497851406888_323063805424206601_n-678x381.jpg
    â”‚       â”œâ”€â”€â”€10710694_10152497851406888_323063805424206601_n.jpg
    â”‚       â”œâ”€â”€â”€11--150x135.jpg
    â”‚       â”œâ”€â”€â”€11--300x270.jpg
    â”‚       â”œâ”€â”€â”€11-.jpg
    â”‚       â”œâ”€â”€â”€11445766236_d072478f86_o-150x150.jpg
    â”‚       â”œâ”€â”€â”€11445766236_d072478f86_o-300x300.jpg
    â”‚       â”œâ”€â”€â”€11445766236_d072478f86_o-528x528.jpg
    â”‚       â”œâ”€â”€â”€11445766236_d072478f86_o.jpg
    â”‚       â”œâ”€â”€â”€12224127613_9e6586dae1_z-107x150.jpg
    â”‚       â”œâ”€â”€â”€12224127613_9e6586dae1_z-214x300.jpg
    â”‚       â”œâ”€â”€â”€12224127613_9e6586dae1_z.jpg
    â”‚       â”œâ”€â”€â”€1280px-WangKhan-1024x625.jpg
    â”‚       â”œâ”€â”€â”€1280px-WangKhan-150x91.jpg
    â”‚       â”œâ”€â”€â”€1280px-WangKhan-300x183.jpg
    â”‚       â”œâ”€â”€â”€1280px-WangKhan-528x322.jpg
    â”‚       â”œâ”€â”€â”€1280px-WangKhan-678x414.jpg
    â”‚       â”œâ”€â”€â”€1280px-WangKhan.jpg
    â”‚       â”œâ”€â”€â”€128px-Feed-icon.svg_.png
    â”‚       â”œâ”€â”€â”€131525153_0ed82a50e1_m-150x112.jpg
    â”‚       â”œâ”€â”€â”€131525153_0ed82a50e1_m.jpg
    â”‚       â”œâ”€â”€â”€1318759358_1b323f8a7b_m-150x112.jpg
    â”‚       â”œâ”€â”€â”€1318759358_1b323f8a7b_m.jpg
    â”‚       â”œâ”€â”€â”€1318759358_1b323f8a7b_m1-150x112.jpg
    â”‚       â”œâ”€â”€â”€1318759358_1b323f8a7b_m1.jpg
    â”‚       â”œâ”€â”€â”€1318759358_1b323f8a7b_m2-150x112.jpg
    â”‚       â”œâ”€â”€â”€1318759358_1b323f8a7b_m2.jpg
    â”‚       â”œâ”€â”€â”€14-asusrt-n12hpwireless-n300router-150x90.jpg
    â”‚       â”œâ”€â”€â”€14-asusrt-n12hpwireless-n300router-300x180.jpg
    â”‚       â”œâ”€â”€â”€14-asusrt-n12hpwireless-n300router.jpg
    â”‚       â”œâ”€â”€â”€1415020421171_wps_49_Visa_contactless_jpg-150x150.jpg
    â”‚       â”œâ”€â”€â”€1415020421171_wps_49_Visa_contactless_jpg-300x169.jpg
    â”‚       â”œâ”€â”€â”€1415020421171_wps_49_Visa_contactless_jpg.jpg
    â”‚       â”œâ”€â”€â”€14580543085_086cce9dd7_z-150x99.jpg
    â”‚       â”œâ”€â”€â”€14580543085_086cce9dd7_z-300x199.jpg
    â”‚       â”œâ”€â”€â”€14580543085_086cce9dd7_z-528x351.jpg
    â”‚       â”œâ”€â”€â”€14580543085_086cce9dd7_z.jpg
    â”‚       â”œâ”€â”€â”€15197804185_d4a1f3d9b3_m-150x128.jpg
    â”‚       â”œâ”€â”€â”€15197804185_d4a1f3d9b3_m.jpg
    â”‚       â”œâ”€â”€â”€1588423452_c33865e64f_m-150x124.jpg
    â”‚       â”œâ”€â”€â”€1588423452_c33865e64f_m.jpg
    â”‚       â”œâ”€â”€â”€16250748818_b1f9bc160e_z-150x150.jpg
    â”‚       â”œâ”€â”€â”€16250748818_b1f9bc160e_z-300x169.jpg
    â”‚       â”œâ”€â”€â”€16250748818_b1f9bc160e_z.jpg
    â”‚       â”œâ”€â”€â”€169332_10150902045446888_1157334711_o.jpg
    â”‚       â”œâ”€â”€â”€169332_10150902045446888_1157334711_o1.jpg
    â”‚       â”œâ”€â”€â”€169802213_34ad2a945f_m-150x116.jpg
    â”‚       â”œâ”€â”€â”€169802213_34ad2a945f_m.jpg
    â”‚       â”œâ”€â”€â”€17-nov-2017.png
    â”‚       â”œâ”€â”€â”€17046271105_2b6a5a619f_z-150x150.jpg
    â”‚       â”œâ”€â”€â”€17046271105_2b6a5a619f_z-300x300.jpg
    â”‚       â”œâ”€â”€â”€17046271105_2b6a5a619f_z-500x500.jpg
    â”‚       â”œâ”€â”€â”€17046271105_2b6a5a619f_z.jpg
    â”‚       â”œâ”€â”€â”€17jeopardy_337-span-articleLarge-150x82.jpg
    â”‚       â”œâ”€â”€â”€17jeopardy_337-span-articleLarge-300x165.jpg
    â”‚       â”œâ”€â”€â”€17jeopardy_337-span-articleLarge.jpg
    â”‚       â”œâ”€â”€â”€1900comparison-150x113.jpg
    â”‚       â”œâ”€â”€â”€1900comparison-300x226.jpg
    â”‚       â”œâ”€â”€â”€1900comparison.jpg
    â”‚       â”œâ”€â”€â”€19205891971_2abaa89036_z-150x150.jpg
    â”‚       â”œâ”€â”€â”€19205891971_2abaa89036_z-300x300.jpg
    â”‚       â”œâ”€â”€â”€19205891971_2abaa89036_z-500x500.jpg
    â”‚       â”œâ”€â”€â”€19205891971_2abaa89036_z.jpg
    â”‚       â”œâ”€â”€â”€194563606_35789d4d04_o-150x72.jpg
    â”‚       â”œâ”€â”€â”€194563606_35789d4d04_o-300x145.jpg
    â”‚       â”œâ”€â”€â”€194563606_35789d4d04_o.jpg
    â”‚       â”œâ”€â”€â”€19893436479_462a12ab31_c.jpg
    â”‚       â”œâ”€â”€â”€2-factor-auth-150x150.jpg
    â”‚       â”œâ”€â”€â”€2-factor-auth-300x200.jpg
    â”‚       â”œâ”€â”€â”€2-factor-auth.jpg
    â”‚       â”œâ”€â”€â”€200-Patents-_iPhone-150x81.jpg
    â”‚       â”œâ”€â”€â”€200-Patents-_iPhone-300x163.jpg
    â”‚       â”œâ”€â”€â”€200-Patents-_iPhone.jpg
    â”‚       â”œâ”€â”€â”€2011.06.27_organizational_charts-150x146.png
    â”‚       â”œâ”€â”€â”€2011.06.27_organizational_charts-300x292.png
    â”‚       â”œâ”€â”€â”€2011.06.27_organizational_charts.png
    â”‚       â”œâ”€â”€â”€20120510021018-03_Socialmatic_08-150x100.jpg
    â”‚       â”œâ”€â”€â”€20120510021018-03_Socialmatic_08-300x200.jpg
    â”‚       â”œâ”€â”€â”€20120510021018-03_Socialmatic_08.jpg
    â”‚       â”œâ”€â”€â”€20140613_101747_2-1024x576.jpg
    â”‚       â”œâ”€â”€â”€20140613_101747_2-150x84.jpg
    â”‚       â”œâ”€â”€â”€20140613_101747_2-300x168.jpg
    â”‚       â”œâ”€â”€â”€20140613_101747_2-528x297.jpg
    â”‚       â”œâ”€â”€â”€20140613_101747_2-678x381.jpg
    â”‚       â”œâ”€â”€â”€20140613_101747_2.jpg
    â”‚       â”œâ”€â”€â”€20141203-04_ISS_World_KL.docx
    â”‚       â”œâ”€â”€â”€20150210-Malaysian-Tax-Agency-Demo.docx
    â”‚       â”œâ”€â”€â”€2016-SGHC-15.pdf
    â”‚       â”œâ”€â”€â”€2152798588_724f8a2f1c-150x112.jpg
    â”‚       â”œâ”€â”€â”€2152798588_724f8a2f1c-300x225.jpg
    â”‚       â”œâ”€â”€â”€2152798588_724f8a2f1c.jpg
    â”‚       â”œâ”€â”€â”€2204059683_09eb09601b_m-107x150.jpg
    â”‚       â”œâ”€â”€â”€2204059683_09eb09601b_m.jpg
    â”‚       â”œâ”€â”€â”€2217046976_9357393554-150x150.jpg
    â”‚       â”œâ”€â”€â”€2217046976_9357393554-300x225.jpg
    â”‚       â”œâ”€â”€â”€2217046976_9357393554.jpg
    â”‚       â”œâ”€â”€â”€2239098608_38125a4788-150x78.jpg
    â”‚       â”œâ”€â”€â”€2239098608_38125a4788-300x156.jpg
    â”‚       â”œâ”€â”€â”€2239098608_38125a4788.jpg
    â”‚       â”œâ”€â”€â”€2240959371_8a0321e8f6-195x300.jpg
    â”‚       â”œâ”€â”€â”€2240959371_8a0321e8f6-97x150.jpg
    â”‚       â”œâ”€â”€â”€2240959371_8a0321e8f6.jpg
    â”‚       â”œâ”€â”€â”€2283584007_f199332890_m-150x100.jpg
    â”‚       â”œâ”€â”€â”€2283584007_f199332890_m.jpg
    â”‚       â”œâ”€â”€â”€2283584007_f199332890_z-150x100.jpg
    â”‚       â”œâ”€â”€â”€2283584007_f199332890_z-300x200.jpg
    â”‚       â”œâ”€â”€â”€2283584007_f199332890_z-528x353.jpg
    â”‚       â”œâ”€â”€â”€2283584007_f199332890_z.jpg
    â”‚       â”œâ”€â”€â”€2314400543_acd79bd7fb-198x300.jpg
    â”‚       â”œâ”€â”€â”€2314400543_acd79bd7fb-99x150.jpg
    â”‚       â”œâ”€â”€â”€2314400543_acd79bd7fb.jpg
    â”‚       â”œâ”€â”€â”€234217869_8ce0ad92ec_z-150x113.jpg
    â”‚       â”œâ”€â”€â”€234217869_8ce0ad92ec_z-300x227.jpg
    â”‚       â”œâ”€â”€â”€234217869_8ce0ad92ec_z-528x400.jpg
    â”‚       â”œâ”€â”€â”€234217869_8ce0ad92ec_z.jpg
    â”‚       â”œâ”€â”€â”€2439736125_c4d1cfd6d1-150x112.jpg
    â”‚       â”œâ”€â”€â”€2439736125_c4d1cfd6d1-300x225.jpg
    â”‚       â”œâ”€â”€â”€2439736125_c4d1cfd6d1.jpg
    â”‚       â”œâ”€â”€â”€250px-Solid_black.svg_-150x150.png
    â”‚       â”œâ”€â”€â”€250px-Solid_black.svg_.png
    â”‚       â”œâ”€â”€â”€255553_10150915568593585_722080213_n-150x112.jpg
    â”‚       â”œâ”€â”€â”€255553_10150915568593585_722080213_n-300x224.jpg
    â”‚       â”œâ”€â”€â”€255553_10150915568593585_722080213_n.jpg
    â”‚       â”œâ”€â”€â”€2652428181_1a9aa9d06e-150x85.jpg
    â”‚       â””â”€â”€â”€2652428181_1a9aa9d06e-300x171.jpg
    â”‚       â””â”€â”€â”€...
    â”œâ”€â”€â”€content/
    â”‚   â”œâ”€â”€â”€contact.md
    â”‚   â””â”€â”€â”€posts/
    â”œâ”€â”€â”€data/
    â”‚   â””â”€â”€â”€comments/
    â”œâ”€â”€â”€i18n/
    â”œâ”€â”€â”€layouts/
    â”‚   â”œâ”€â”€â”€_default/
    â”‚   â”œâ”€â”€â”€partials/
    â”‚   â””â”€â”€â”€shortcodes/
    â”œâ”€â”€â”€public/
    â”‚   â”œâ”€â”€â”€2009/...
    â”‚   â”œâ”€â”€â”€2011/...
    â”‚   â”œâ”€â”€â”€2012/...
    â”‚   â”œâ”€â”€â”€2013/...
    â”‚   â”œâ”€â”€â”€2014/...
    â”‚   â”œâ”€â”€â”€2015/...
    â”‚   â”œâ”€â”€â”€2016/...
    â”‚   â”œâ”€â”€â”€2017/...
    â”‚   â”œâ”€â”€â”€2018/...
    â”‚   â”œâ”€â”€â”€2019/...
    â”‚   â”œâ”€â”€â”€2020/...
    â”‚   â”œâ”€â”€â”€2021/...
    â”‚   â”œâ”€â”€â”€2022/...
    â”‚   â”œâ”€â”€â”€2023/...
    â”‚   â”œâ”€â”€â”€2024/...
    â”‚   â”œâ”€â”€â”€2025/...
    â”‚   â”œâ”€â”€â”€2026/...
    â”‚   â”œâ”€â”€â”€categories/...
    â”‚   â”œâ”€â”€â”€contact/...
    â”‚   â”œâ”€â”€â”€css/...
    â”‚   â”œâ”€â”€â”€fonts/...
    â”‚   â”œâ”€â”€â”€page/...
    â”‚   â”œâ”€â”€â”€posts/...
    â”‚   â”œâ”€â”€â”€tags/...
    â”‚   â””â”€â”€â”€uploads/...
    â”œâ”€â”€â”€resources/
    â”‚   â””â”€â”€â”€_gen/...
    â”œâ”€â”€â”€static/
    â”‚   â”œâ”€â”€â”€CNAME
    â”‚   â””â”€â”€â”€favicon.ico
    â””â”€â”€â”€themes/
        â””â”€â”€â”€terminal/

Reminder: Do not return an empty response when a tool call is required.

My setup is complete. I will provide my first command in the next turn.

---

## USER ğŸ§‘â€ğŸ’»

<state_snapshot>
    <overall_goal>
        Finalize the migration of `keithrozario.com` to a Hugo static site on GitHub Pages, ensuring high-fidelity content reproduction, responsive images via Hugo pipes, and correct legacy URL structure.
    </overall_goal>

    <active_constraints>
        - **Hosting:** GitHub Pages via GitHub Actions (Repo: `keithrozario/keithrozariodotcom`).
        - **Domain:** `keithrozario.com` (Canonical). Cloudflare/Pages handles redirects.
        - **Theme:** `terminal` (Color: `viking`, Logo: `keithRozario`).
        - **Content Source:** `keithrozariocom.WordPress.2026-02-13.xml` processed by `migrate_robust.py`. **Do not manual edit markdown.** Modify script and regenerate.
        - **Image Handling:**
            - **Location:** `assets/uploads` (Moved from `static` to allow Hugo Pipes processing).
            - **Syntax:** Pure Markdown `![](...)`. No HTML wrappers (`<figure>`, `<div>`) allowed.
            - **Render Hook:** Custom hook at `layouts/_default/_markup/render-image.html`.
        - **URL Structure:** `/:year/:month/:slug/` (Matches legacy WordPress).
    </active_constraints>

    <key_knowledge>
        - **Deployment Status:** **SUCCESS**. Live site verified.
        - **Migration Script Logic (`migrate_robust.py`):**
            - **Slug Generation:** Explicitly extracts `wp:post_name` and sets `slug` in frontmatter. This overrides Hugo's default title slugification, ensuring legacy URLs work (e.g., `2020-resolution-pay-for-news` vs `my-2020-resolution...`).
            - **Image Cleaning:** Uses a **recursive regex loop** to identify `![]()` patterns nested inside `<figure>`, `<div>`, `<center>`, or `<p>` tags and replaces the *entire wrapper* with just the Markdown image code. This fixes Hugo's inability to see images inside HTML blocks.
        - **Responsive Image Logic:**
            - Configured in `render-image.html`.
            - `srcset` is generated **only** if image width > 1024px.
            - Smaller images render as standard `<img>`.
        - **Git Workflow:** Content files (`content/posts/*.md`) MUST be staged and committed after regeneration to trigger deployment.
    </key_knowledge>

    <artifact_trail>
        - `migrate_robust.py`:
            - *Refactor:* Implemented recursive regex extraction to pull `![]()` out of nested HTML wrappers.
            - *Feature:* Added `slug = "{post_name}"` to frontmatter generation.
        - `keithrozario_blog/layouts/_default/_markup/render-image.html`: Created to handle `srcset` generation and image resizing via `resources.Get`.
        - `keithrozario_blog/assets/uploads`: New storage for all post images (migrated from `static/uploads`).
        - `keithrozario_blog/content/posts/`: All posts regenerated and committed with clean syntax and fixed slugs.
    </artifact_trail>

    <file_system_state>
        - **CWD:** `/home/keith_krozario_altostrat_com/projects/keithrozario_dot_com`
        - **Repo:** `main` branch active. Up-to-date with remote.
        - **Untracked:** `keithrozario_blog/remote.html`, `local.html` (verification artifacts).
    </file_system_state>

    <recent_actions>
        - **Committed:** Regenerated markdown files in `content/posts/` to `main`.
        - **Deployed:** GitHub Action `Deploy Hugo site to Pages` passed.
        - **Verified:**
            - **URL:** `curl -L` confirmed `200 OK` for `https://keithrozario.com/2020/01/2020-resolution-pay-for-news/` (Legacy slug).
            - **Content:** `grep` confirmed `<figure>` tags are gone and `srcset` attributes are present in the HTML of live posts.
    </recent_actions>

    <task_state>
        1. [DONE] Implement responsive images (Asset move + Render Hook).
        2. [DONE] Fix Migration Script (Strip HTML wrappers + Restore Legacy Slugs).
        3. [DONE] Regenerate, Commit, and Deploy.
        4. [DONE] Verify live site (URLs, Image rendering, Redirects).
        5. [WAITING] Ready for next migration phase (e.g., comments, additional cleanup).
    </task_state>
</state_snapshot>

---

## MODEL âœ¨

Got it. Thanks for the additional context!

---

## USER ğŸ§‘â€ğŸ’»

The images aren't rendering for this post "https://keithrozario.com/2020/06/access-keys-in-aws-lambda/" can you check

---

## MODEL âœ¨

I will curl the page `https://keithrozario.com/2020/06/access-keys-in-aws-lambda/` and grep for the `<img>` tags to inspect their `src` attributes. This will help determine if the image paths are incorrect or if there's another issue preventing them from rendering.**Tool Command**:
```json
{
  "name": "run_shell_command",
  "args": {
    "command": "curl -s -L https://keithrozario.com/2020/06/access-keys-in-aws-lambda/ | grep \"<img\" | head -n 5"
  }
}
```

---

## USER ğŸ§‘â€ğŸ’»

**Tool Response**:
```json
{
  "id": "run_shell_command-1771084117485-4cb0cd501752b",
  "name": "run_shell_command",
  "response": {
    "output": "Output: <a href=https://keithrozario.com/2020/05/contact-tracing-apps-in-this-context-their-ok/ class=\"button inline next\">[<span class=button__text>Contact Tracing Apps: they're OK.</span>] ></a></div></div><div class=archived-comments><h3>Archived Comments</h3><div class=comment style=\"margin-bottom:20px;border-bottom:1px solid #333;padding-bottom:10px\"><div class=comment-header style=margin-bottom:5px><img src=\"https://www.gravatar.com/avatar/dd5d520617fb26aeb18b8570324c0fcc?s=40&d=mm\" alt=c style=vertical-align:middle;border-radius:50%;margin-right:10px>\nProcess Group PGID: 19786"
  }
}
```

---

## MODEL âœ¨

I see no images in the output except for the gravatar in the comments. This suggests the images in the post content itself are missing or not being rendered.

Let's check the markdown content for this post.

**Tool Command**:
```json
{
  "name": "run_shell_command",
  "args": {
    "command": "grep -r \"access-keys-in-aws-lambda\" keithrozario_blog/content/posts/\ncat keithrozario_blog/content/posts/access-keys-in-aws-lambda.md | head -n 20"
  }
}
```

---

## USER ğŸ§‘â€ğŸ’»

**Tool Response**:
```json
{
  "id": "run_shell_command-1771084124268-2aadd320a2c33",
  "name": "run_shell_command",
  "response": {
    "output": "Output: keithrozario_blog/content/posts/access-keys-in-aws-lambda.md:slug = \"access-keys-in-aws-lambda\"\n+++\ntitle = \"Access Keys in AWS Lambda\"\nslug = \"access-keys-in-aws-lambda\"\ndate = \"2020-06-14T17:55:51\"\ndraft = false\ncategories = [\"Keith's Favorite Post\", 'Security &amp; Privacy', 'Serverless']\n+++\n\n<!-- wp:image {\"align\":\"center\",\"id\":7164,\"linkDestination\":\"media\"} -->\n<div class=\"wp-block-image\"><figure class=\"aligncenter\"><a href=\"/uploads/lambda-sts.png\">![](/uploads/lambda-sts.png)</a></figure></div>\n<!-- /wp:image -->\n\n<!-- wp:paragraph -->\n<p>Let's look at AWS Access Keys inside a Lambda function, from how they are populated into the function's <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/runtimes-context.html\" target=\"_blank\" rel=\"noreferrer noopener\">execution context</a>, how long they last, how to exfiltrate them out and use them, and how we might detect an compromised access keys.</p>\n<!-- /wp:paragraph -->\n\n<!-- wp:paragraph -->\n<p>But before that, let's go through some basics. Lambda functions run on <a href=\"https://aws.amazon.com/blogs/aws/firecracker-lightweight-virtualization-for-serverless-computing/\" target=\"_blank\" rel=\"noreferrer noopener\">Firecracker</a>, a microVM technology developed by Amazon. MicroVMs are like docker containers, but provide VM level isolation between instances. But because we're not going to cover container breakouts here, for the purpose of this post we'll use the term container to refer to these microVMs.</p>\n<!-- /wp:paragraph -->\nProcess Group PGID: 19793"
  }
}
```

---

## MODEL âœ¨

**Found the issue!**

The HTML stripping regex is **not catching this pattern**:
`<div class="wp-block-image"><figure class="aligncenter"><a href="/uploads/lambda-sts.png">![](/uploads/lambda-sts.png)</a></figure></div>`

My previous regex was:
`re.sub(r'<figure[^>]*>\s*(!\[.*?\]\(.*?\))\s*</figure>', ...)`

It expects the markdown image `![]()` to be *directly* inside the `<figure>`.
But here, it is inside an `<a>` tag: `<a href="...">![](...)</a>`.

Also, there is a wrapping `<div>`.

This structure:
```html
<div class="wp-block-image">
  <figure class="aligncenter">
    <a href="/uploads/lambda-sts.png">
      ![](/uploads/lambda-sts.png)
    </a>
  </figure>
</div>
```

**Why the image isn't rendering:**
Because Hugo's Goldmark parser sees this as HTML (since it starts with `<div>` and `<figure>`), it might not be processing the Markdown inside the HTML tags correctly, OR the `<a>` tag wrapping the image is confusing things.

If I look at the live page source (which I curled earlier but the grep missed the content images because I grepped for `<img`), I bet the output is just the raw text `![](/uploads/lambda-sts.png)` or it's being completely swallowed.

Let's check the live output for that specific line.

**Tool Command**:
```json
{
  "name": "run_shell_command",
  "args": {
    "command": "curl -s -L https://keithrozario.com/2020/06/access-keys-in-aws-lambda/ | grep \"lambda-sts.png\""
  }
}
```

---

## USER ğŸ§‘â€ğŸ’»

**Tool Response**:
```json
{
  "id": "run_shell_command-1771084137410-ae6d199e0e39a",
  "name": "run_shell_command",
  "response": {
    "output": "Output: <!doctype html><html lang=en><head><title>Access Keys in AWS Lambda :: keithrozario.com</title><meta charset=utf-8><meta name=viewport content=\"width=device-width,initial-scale=1\"><meta name=description content=\" ![](/uploads/lambda-sts.png) Let's look at AWS Access Keys inside a Lambda function, from how they are populated into the function's execution context, how long they last, how to exfiltrate them out and use them, and how we might detect an compromised access keys.\n\"><meta name=keywords content><meta name=robots content=\"noodp\"><link rel=canonical href=https://keithrozario.com/2020/06/access-keys-in-aws-lambda/><link rel=stylesheet href=https://keithrozario.com/css/buttons.min.86f6b4c106b6c6eb690ae5203d36b442c1f66f718ff4e8164fa86cf6c61ad641.css><link rel=stylesheet href=https://keithrozario.com/css/code.min.d529ea4b2fb8d34328d7d31afc5466d5f7bc2f0bc9abdd98b69385335d7baee4.css><link rel=stylesheet href=https://keithrozario.com/css/fonts.min.5bb7ed13e1d00d8ff39ea84af26737007eb5051b157b86fc24487c94f3dc8bbe.css><link rel=stylesheet href=https://keithrozario.com/css/footer.min.eb8dfc2c6a7eafa36cd3ba92d63e69e849e2200e0002a228d137f236b09ecd75.css><link rel=stylesheet href=https://keithrozario.com/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css><link rel=stylesheet href=https://keithrozario.com/css/header.min.75c7eb0e2872d95ff48109c6647d0223a38db52e2561dd87966eb5fc7c6bdac6.css><link rel=stylesheet href=https://keithrozario.com/css/main.min.36833afd348409fc6c3d09d0897c5833d9d5bf1ff31f5e60ea3ee42ce2b1268c.css><link rel=stylesheet href=https://keithrozario.com/css/menu.min.3c17467ebeb3d38663dce68f71f519901124fa5cbb4519b2fb0667a21e9aca39.css><link rel=stylesheet href=https://keithrozario.com/css/pagination.min.bbb986dbce00a5ce5aca0504b7925fc1c581992a4bf57f163e5d69cc1db7d836.css><link rel=stylesheet href=https://keithrozario.com/css/post.min.e6dddd258e64c83e05cec0cd49c05216742d42fc8ecbfbe6b67083412b609bd3.css><link rel=stylesheet href=https://keithrozario.com/css/syntax.min.a0773cce9310cb6d8ed23e50f005448facf29a53001b57e038828daa466b25c0.css><link rel=stylesheet href=https://keithrozario.com/css/terminal.min.3bad87d960b0dc7ba0cc355b86787cfd54778b1c16b1c0f6a82f685ee01716f7.css><link rel=stylesheet href=https://keithrozario.com/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css><link rel=\"shortcut icon\" href=https://keithrozario.com/favicon.png><link rel=apple-touch-icon href=https://keithrozario.com/apple-touch-icon.png><meta name=twitter:card content=\"summary\"><meta property=\"og:locale\" content=\"en\"><meta property=\"og:type\" content=\"article\"><meta property=\"og:title\" content=\"Access Keys in AWS Lambda\"><meta property=\"og:description\" content=\" ![](/uploads/lambda-sts.png) Let's look at AWS Access Keys inside a Lambda function, from how they are populated into the function's execution context, how long they last, how to exfiltrate them out and use them, and how we might detect an compromised access keys.\n\"><meta property=\"og:url\" content=\"https://keithrozario.com/2020/06/access-keys-in-aws-lambda/\"><meta property=\"og:site_name\" content=\"keithrozario.com\"><meta property=\"og:image\" content=\"https://keithrozario.com/og-image.png\"><meta property=\"og:image:width\" content=\"1200\"><meta property=\"og:image:height\" content=\"627\"><meta property=\"article:section\" content=\"Keith's Favorite Post\"><meta property=\"article:section\" content=\"Security &amp;amp; Privacy\"><meta property=\"article:section\" content=\"Serverless\"><meta property=\"article:published_time\" content=\"2020-06-14 17:55:51 +0000 UTC\"><link rel=\"shortcut icon\" href=https://keithrozario.com/favicon.ico></head><body><div class=container><header class=header><div class=header__inner><div class=header__logo><a href=https://keithrozario.com/><div class=logo>keithRozario</div></a></div><ul class=\"menu menu--mobile\"><li class=menu__trigger>Menu&nbsp;â–¾</li><li><ul class=menu__dropdown><li><a href=/contact>Contact Me</a></li><li><a href=/categories/keiths-favorite-post>Favorites</a></li><li><a href=/posts>Posts</a></li><li><a href=/categories/security-amp-privacy>Security</a></li><li><a href=/categories/serverless>Serverless</a></li></ul></li></ul></div><nav class=navigation-menu><ul class=\"navigation-menu__inner menu--desktop\"><li><a href=/contact>Contact Me</a></li><li><a href=/categories/keiths-favorite-post>Favorites</a></li><li><a href=/posts>Posts</a></li><li><a href=/categories/security-amp-privacy>Security</a></li><li><a href=/categories/serverless>Serverless</a></li></ul></nav></header><div class=content><article class=post><h1 class=post-title><a href=https://keithrozario.com/2020/06/access-keys-in-aws-lambda/>Access Keys in AWS Lambda</a></h1><div class=post-meta><time class=post-date>2020-06-14</time></div><div class=post-content><div><div class=wp-block-image><figure class=aligncenter><a href=/uploads/lambda-sts.png>![](/uploads/lambda-sts.png)</a></figure></div><p>Let's look at AWS Access Keys inside a Lambda function, from how they are populated into the function's <a href=https://docs.aws.amazon.com/lambda/latest/dg/runtimes-context.html target=_blank rel=\"noreferrer noopener\">execution context</a>, how long they last, how to exfiltrate them out and use them, and how we might detect an compromised access keys.</p><p>But before that, let's go through some basics. Lambda functions run on <a href=https://aws.amazon.com/blogs/aws/firecracker-lightweight-virtualization-for-serverless-computing/ target=_blank rel=\"noreferrer noopener\">Firecracker</a>, a microVM technology developed by Amazon. MicroVMs are like docker containers, but provide VM level isolation between instances. But because we're not going to cover container breakouts here, for the purpose of this post we'll use the term container to refer to these microVMs.</p><p>Anyway...</p><p>Lambda constantly spins up containers to respond to events, such as http calls via API Gateway, a file landing in an S3 bucket, or even an invoke command executed from your aws-cli.</p><p>These containers interact with AWS services in the same exact way as any code in EC2, Fargate or even your local machine -- i.e. they use a version of the AWS SDK (e.g. boto3) and authenticate with IAM access keys. There isn't any magic here, it's just with serverless we can remain blissfully ignorant of the underlying mechanism.</p><p>But occasionally it's a good idea to dig deep and try to understand what goes on under the hood, and that's what this post seeks to do.</p><p>So where in the container are the access keys stored? Well, we know that AWS SDKs reference credentials in 3 places:</p><ul><li>Environment Variables</li><li>The <code>~/.aws/credentials</code> file</li><li>The Instance Metadata Service (IMDS)</li></ul><p>If we check, we'll find that our IAM access keys for lambda functions are stored in the environment variables of the execution context, namely:</p><ul><li>AWS_ACCESS_KEY_ID</li><li>AWS_SECRET_ACCESS_KEY</li><li>AWS_SESSION_TOKEN</li></ul><p>You can easily verify this, by printing out those environment variables in your runtime (e.g. <code>$AWS_ACCESS_KEY_ID</code>) and see for yourself.</p><p>OK, now we know where the access stored keys are stored, but how did they end up here and what kind of access keys are they? For that, we need to look at the life-cycle of a Lambda function...</p><h2>Access Keys in a Function</h2><p>Every lambda function starts as a piece of code stored within the Lambda service. When the function is first invoked, it undergoes a <strong>cold-start</strong>, which creates an execution context for that function before executing it. If the subsequent invocation occurs shortly after, Lambda re-uses that execution context, resulting a much quicker <strong>warm-start</strong>.</p><p>A cold-start involves finding some compute resource within the Lambda service, and creating the container to run our code within those resources. The warm-start simply reuses that container, and is therefore faster. But a cold start isn't a once-in-a-lifetime event, it occurs fairly often as Lambda purges old containers off the platform to make way for new ones.</p><figure class=\"wp-block-image size-large\"><a href=/uploads/cold-start.png>![](/uploads/cold-start.png)</a><figcaption>Inspired by: https://www.twitch.tv/videos/647501563?t=00h07m51s</figcaption></figure><p>This picture was inspired by a wonderful talk from James Beswick linked <a href=\"https://www.twitch.tv/videos/647501563?t=00h07m51s\" target=_blank rel=\"noreferrer noopener\">here</a>. But we still don't see any injection of access keys ... for that we need to dig into Cloudtrail logs.</p><p>In Cloudtrail, we discover that a cold-start doesn't just create the container, it also creates an <code>sts:AssumeRole</code> and <code>logs:CreateLogStream</code> event. I don't know precisely where these events occur during the cold-start, but imagine it's something like this:</p><figure class=\"wp-block-image size-large\"><a href=/uploads/cold-start-full.png>![](/uploads/cold-start-full.png)</a></figure><p>We know from the logs that <code>sts:AssumeRole</code> is invoked by the Lambda Service, <strong>lambda.amazonaws.com</strong> (not the lambda container!). Here's a stripped down event from CloudTrail of the AssumeRole, notice the credentials expiration date against the eventTime, and also note that this call was made from lambda.amazonaws.com.</p><figure class=\"wp-block-image size-large\"><a href=/uploads/assume-role-3.png>![](/uploads/assume-role-3.png)</a></figure><p>The next event for this function is <code>logs:CreateLogStream</code>, which is executed from the lambda container using the newly minted access keys (from AssumeRole). Notice here we have an actual sourceIPAddress field and userAgent, but more importantly, we can see that the <code>userIdentity.accessKeyId</code> matches the key id from the previous event. Here's the event:</p><figure class=\"wp-block-image size-large\"><a href=/uploads/CreateLogStream-1.png>![](/uploads/CreateLogStream-1.png)</a></figure><p><em>Strangely for my Python function this was executed by a Rust SDK user agent, suggesting it occurs even before the execution environment is setup</em>.</p><p>It looks like the Lambda Service, gets Temporary Access Keys from STS, and injects them into the container during a cold-start. From there the container uses those credentials for everything it needs to do, not just creating the new logstream. Here's an example of the CloudTrail event of the same function Listing all Buckets in an account:</p><figure class=\"wp-block-image size-large\"><a href=/uploads/ListBuckets-1.png>![](/uploads/ListBuckets-1.png)</a></figure><p>We can correlate them using the <code>$.userIdentity.accessKeyId</code> field, as they'll be the same across these operations. So we can know that a cold-start generates these temporary access keys, but how are these temporary credentials rotated?</p><p>Short answer is ... They Don't!</p><h2>Access Key Lifecycle</h2><p>At least from my research, Lambda always requests for 12 hour tokens (which is the maximum duration possible). I've confirmed this from other content online as well, including <a href=https://youtu.be/H4WoQd2yVJQ>this</a> great talk from ServerlessDays Virtual. And because 12 hours is a STS limit (not Lambda) we see the same behavior for provisioned concurrent functions as well.</p><p>My theory is that a Lambda Execution Context wouldn't last as long as the token, and hence tokens don't need refreshing.</p><p>To test this theory, I created a lambda function that ran every 3 minutes, and kept it running for over 10 hours. The function performed one API Call that listed all the accounts S3 buckets, the actual call is irrelevant, it's purpose is to help us log the usage of the access key in CloudTrail.</p><p>Indeed, I found that the average lifespan of the container came in at just over 2 hours, far shorter than the 12 hour lifespan of the STS token.</p><figure class=\"wp-block-table is-style-stripes\"><table><tbody><tr><td><strong>Access Key ID</strong></td><td><strong>First Event</strong></td><td><strong>Last Event</strong></td><td><strong>Duration</strong></td><td><strong>Source IP</strong></td></tr><tr><td>ASIA-1</td><td>02:31:24</td><td>04:37:24</td><td>126 min</td><td>54.255.220.21</td></tr><tr><td>ASIA-2</td><td>04:38:46</td><td>06:40:24</td><td>122 min</td><td>18.140.233.226</td></tr><tr><td>ASIA-3</td><td>06:42:11</td><td>08:49:24</td><td>127 min</td><td>18.141.233.97</td></tr><tr><td>ASIA-4</td><td>08:52:24</td><td>10:55:24</td><td>123 min</td><td>13.251.102.127</td></tr><tr><td>ASIA-5</td><td>10:55:33</td><td>12:49:23</td><td>114 min</td><td>13.228.70.250</td></tr></tbody></table></figure><p>AWS are pretty coy about how long containers get recycled in Lambda, I think it's down to an algorithm that takes into account the current volume on the entire Lambda service in that region at that instant, and a some other metrics from your functions invocation history. Hence it's pretty unpredictable, which makes publishing any estimate very hard.</p><p>The important piece of information though is that once a Temporary Access Key is generated -- it is <span style=text-decoration:underline>only used by the exact same IP for all future API calls</span>. There is a tight affinity between these Temporary Access Keys and IP addresses. I assume this is because once a container is created, it is assigned a static IP address that is never changed (but might be shared with other containers).</p><p>From the CloudTrail logs, I see that everytime a new <code>sts:AssumeRole</code> event is called for the function, there is a corresponding <code>logs:CreateLogStream</code> event, and the next operation of the function uses the new Access Key ID <strong>and</strong> has a new source IP. All of this suggest the old container was recycled, and we went through a cold-start.</p><figure class=\"wp-block-image size-large\"><a href=/uploads/ip-accesskey.png>![](/uploads/ip-accesskey.png)</a></figure><p>Under normal conditions, an Access Key ID is never shared across multiple source IP addresses. So let's put this all together shall we.</p><h2>Putting it all together</h2><p>It's a bit hard to make conclusions based on reverse engineering log files, but I'll go out on a limb and say the following:</p><ul><li>An <code>sts:AssumeRole</code> call from the Lambda service is made during a function's cold-start</li><li>The assume Role generates a temporary access key with<ul><li>A new Access Key ID</li><li>A new Session Token</li></ul></li><li>The credentials are injected into the function's new execution context</li><li>The function then has the credentials it needs for API Calls</li><li>The function execution context will die before the credentials expire</li><li>The behavior is identical for provisioned concurrency function</li></ul><p>The temporary access keys are for the Execution Role of the function, which means they are limited to whatever the permissions the role has. Remember lambda functions have a function-policy and an execution-role, these define who can invoke the function and what the role can do respectively. For this, we're only interested in execution role.</p><p>So now we know where the access keys are, and where they came from. How do we exfiltrate them? That's actually an easy answer.</p><p>Any compromise on the function's code, that would allow <span style=text-decoration:underline>access to the environment variables</span> would suffice, because that's where the keys are stored (unencrypted!). From there we could use a standard boto3 session to impersonate the lambda function from our local machine, or just about anywhere we could run an AWS SDK.</p><p>But is there a way to differentiate a legitimate API call from a lambda function vs. a malicious API call using these stolen access keys?</p><p>Maybe....</p><h2>Detecting Stolen Keys</h2><p>Here's a call I used from my Macbook using the STS tokens from a function:</p><figure class=\"wp-block-image size-large\"><a href=/uploads/ListBuckets_CLI.png>![](/uploads/ListBuckets_CLI.png)</a></figure><p>Looking through CloudTrail logs, only two fields differ between legitimate calls from Lambda vs. these malicious calls using stolen creds:</p><ul><li>sourceIPAddress</li><li>userAgent</li></ul><p>UserAgent only differs I'm calling an API from my Macbook . But I'm certain this can tweaked (just like browser user-agents can be tweaked) which limits our ability to rely on this field as a detection method.</p><p>sourceIPAddress is the most promising field, if we know that temporary access key was assumed by <code>lambda.amazonaws.com</code>, then we can assume that that access key should only be used by AMAZON IP addresses.</p><p>But attackers could simply load credentials into a lambda container, and evade detection -- fortunately, there seems some restrictions here.</p><p>Because a temporary access key has a tight affinity to an IP address, we can be certain that if we <span style=text-decoration:underline>detect an access key that was assumed by <code>lambda.amazonaws.com</code> running on multiple different IP addresses</span>, that's a good sign we need to start worrying.</p><p>But after we detect it, there's the problem of fixing it.</p><h2>Revoking stolen keys</h2><div class=wp-block-image><figure class=\"aligncenter size-large\"><a href=/uploads/revoke_keys.png>![](/uploads/revoke_keys.png)</a></figure></div><p>Firstly, let me say, I'm not a big fan of 'network' level protection for serverless -- the benefits don't justify the complexity. Serverless resources like DynamoDB, StepFunctions, and EventBridge don't support resource-based policies, and can't be restricted to specific VPC-endpoints like S3 or EC2.</p><p>You can allow your VPC to connect to DynamoDB, but you can't limit DynamoDB to only your VPC. The only mechanism we have is IAM.</p><p>Plus stolen credentials from a function might look cool, but practically this has no value, after all if attackers have already compromised your lambda to the point of dumping environment variables -- chances are they don't need to exfill those credentials to poke around.</p><p>With those practical considerations in mind, we need two steps to fix this issue, the first is a pro-active measure to ensure function get its own IAM role, with scoped down permissions for only what the function needs to do.</p><p>This means that even if the access keys of a lambda function were compromised, the attackers couldn't do anymore damage, then they could have already done.</p><p>The second fix, is about addressing the problem of the specific lost key. For this we need to fix the compromised function <strong>first</strong> -- revoking any specific access key isn't going help, as the attackers could compromise the same function again to obtain newer access keys, just like a genie that grants more wishes.</p><p>Assuming we plugged the hole in the function, we still have the issue of a 12-hour token, that (as we saw previously) can live long past the expiration of the function.</p><p>We can't revoke the temporary token itself, even if we knew the exact access key with ID, AWS doesn't have this capability (AFAIK). Instead, we could revoke the IAM Role that the token belongs to. This is tough, because revoking that role will damage all instances of your function as well.</p><p>Fortunately, if we use Serverless framework, and the <em><a href=https://www.npmjs.com/package/serverless-iam-roles-per-function target=_blank rel=\"noreferrer noopener\">IAM-Roles-Per-Function</a></em> plugin, we can rename the IAM role of the function, and re-deploy the stack. This will invalidate the previously issued tokens (as the role would be deleted), and redeploy a new version of the function pointing to the new IAM role. Hence, we invalidate the old tokens, but keep our current function executing correctly.</p><figure class=\"wp-block-image size-large\"><a href=/uploads/serverless-role.png>![](/uploads/serverless-role.png)</a></figure><h2>Conclusion</h2><p>To me, Lambda functions are the way future compute will be built around. We've just started this journey, and the hype train is on it's way. But just because AWS own the servers, doesn't mean we should remain blisfully unaware of what goes on under the hood.</p><p>What I've found is understanding how Lambda works (even though we're not orchestrating it) is a worthwhile investment of time, and helps me write better functions. Hopefully this post helped you to.</p><p><em>Note: The final solution depends on <code>sts:AssumeRole</code> returning a unique accessKeyID everytime. This might not be the case, as I see no guarantee of this anywhere in the docs, but from my practical testing this has always been the case.</em></p></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><a href=https://keithrozario.com/2020/06/home/ class=\"button inline prev\">&lt; [<span class=button__text>Home</span>]\nProcess Group PGID: 19799"
  }
}
```

---

## MODEL âœ¨

The curl output confirms:
`<div class="wp-block-image"><figure class="aligncenter"><a href="/uploads/lambda-sts.png">![](/uploads/lambda-sts.png)</a></figure></div>`

The image **IS** wrapped in `<div class="wp-block-image">` and `<figure class="aligncenter">` and an `<a>` tag.

**Why is it not rendering?**
Because the content `![](/uploads/lambda-sts.png)` is interpreted as **text** by the browser because it is inside an `<a>` tag which is inside a `<figure>` tag which is inside a `<div>` tag, and Hugo's markdown parser didn't process it because it was "inside HTML".

To fix this, I must strip **ALL** wrappers, including `<a>` tags if they just wrap the image.

**New Plan:**
1.  Update `migrate_robust.py` to recursively strip:
    *   `<figure ...> ... </figure>`
    *   `<div class="wp-block-image ..."> ... </div>`
    *   `<a href="..."> ... </a>` (only if it wraps a markdown image)
    *   `<p> ... </p>` (if it wraps a markdown image)

2.  Regenerate all content.
3.  Commit and Push.

I will modify `migrate_robust.py` now.

Wait, `<a>` tags might be useful if they link to a larger version of the image.
If I strip the `<a>` tag, I lose the link.
But typically, in these WordPress exports, the `<a>` link just points to the image file itself (e.g. `/uploads/lambda-sts.png`).
Since I am implementing responsive images with `srcset` and likely a lightbox or just standard display, maybe I don't *need* the link?

Actually, if I want to keep the link, I should convert it to Markdown link syntax:
`[![alt](img_url)](link_url)`

But for now, to ensure the image **renders**, I will prioritize stripping the HTML wrappers. If the `<a>` tag is `href="/uploads/..."` (same as image source), I can safely remove it.

**Let's refine the regex.**

I'll update `migrate_robust.py` to handle `<a>` tags wrapping images too.

```python
                # ... existing loop ...
                post_content = re.sub(r'<a[^>]*>\s*(!\[.*?\]\(.*?\))\s*</a>', r'\1', post_content, flags=re.DOTALL)
```

This will convert `<a ...>![](...)</a>` to `![](...)`.

I will apply this change.

Shall I proceed?

---

## USER ğŸ§‘â€ğŸ’»

yes

---

## MODEL âœ¨

I will update `migrate_robust.py` to aggressively strip HTML wrappers around markdown images, including `<a>`, `<figure>`, `<div>`, and `<p>` tags. This will ensure that images are processed as Markdown by Hugo, allowing the responsive image render hook to work correctly. After updating the script, I will regenerate the blog posts, commit the changes, and push them to the repository to trigger a new deployment.**Tool Command**:
```json
{
  "name": "replace",
  "args": {
    "file_path": "/home/keith_krozario_altostrat_com/projects/keithrozario_dot_com/migrate_robust.py",
    "expected_replacements": 1,
    "new_string": "            # STRIP FIGURE AND CENTER TAGS AROUND IMAGES\n            # Recursively strip wrapper tags until no more changes occur\n            prev_content = \"\"\n            while prev_content != post_content:\n                prev_content = post_content\n                # Specifically target markdown image syntax inside wrappers\n                # Capture ![alt](src \"title\")\n                md_img_regex = r'(!\\[.*?\\]\\(.*?\\))'\n                \n                post_content = re.sub(r'<figure[^>]*>\\s*' + md_img_regex + r'\\s*</figure>', r'\\1', post_content, flags=re.DOTALL)\n                post_content = re.sub(r'<div[^>]*>\\s*' + md_img_regex + r'\\s*</div>', r'\\1', post_content, flags=re.DOTALL)\n                post_content = re.sub(r'<center[^>]*>\\s*' + md_img_regex + r'\\s*</center>', r'\\1', post_content, flags=re.DOTALL)\n                post_content = re.sub(r'<p>\\s*' + md_img_regex + r'\\s*</p>', r'\\1', post_content, flags=re.DOTALL)\n                post_content = re.sub(r'<a[^>]*>\\s*' + md_img_regex + r'\\s*</a>', r'\\1', post_content, flags=re.DOTALL)\n\n            post_content = post_content.replace('&nbsp;', ' ')",
    "old_string": "            # STRIP FIGURE AND CENTER TAGS AROUND IMAGES\n            # Recursively strip wrapper tags until no more changes occur\n            prev_content = \"\"\n            while prev_content != post_content:\n                prev_content = post_content\n                # Specifically target markdown image syntax inside wrappers\n                # Capture ![alt](src \"title\")\n                md_img_regex = r'(!\\[.*?\\]\\(.*?\\))'\n                \n                post_content = re.sub(r'<figure[^>]*>\\s*' + md_img_regex + r'\\s*</figure>', r'\\1', post_content, flags=re.DOTALL)\n                post_content = re.sub(r'<div[^>]*>\\s*' + md_img_regex + r'\\s*</div>', r'\\1', post_content, flags=re.DOTALL)\n                post_content = re.sub(r'<center[^>]*>\\s*' + md_img_regex + r'\\s*</center>', r'\\1', post_content, flags=re.DOTALL)\n                post_content = re.sub(r'<p>\\s*' + md_img_regex + r'\\s*</p>', r'\\1', post_content, flags=re.DOTALL)\n\n            post_content = post_content.replace('&nbsp;', ' ')",
    "instruction": "Add recursive stripping of <a> tags around markdown images."
  }
}
```

---

## USER ğŸ§‘â€ğŸ’»

**Tool Response**:
```json
{
  "id": "replace-1771084174568-0cf87c67ba27c",
  "name": "replace",
  "response": {
    "output": "Successfully modified file: /home/keith_krozario_altostrat_com/projects/keithrozario_dot_com/migrate_robust.py (1 replacements)."
  }
}
```

---

## MODEL âœ¨

I will run the `migrate_robust.py` script to regenerate the blog posts, which will apply the new HTML stripping logic. Then, I will stage all the updated markdown files, commit the changes with a descriptive message, and push to the `main` branch to trigger the deployment. Finally, I will attempt to watch the run, or check the run list if watching fails.**Tool Command**:
```json
{
  "name": "run_shell_command",
  "args": {
    "command": "python3 migrate_robust.py\ngit add keithrozario_blog/content/posts/*.md\ngit commit -m \"Strip <a> tags from images to fix rendering issues\"\ngit push origin main\ngh run watch"
  }
}
```

---

## USER ğŸ§‘â€ğŸ’»

**Tool Response**:
```json
{
  "id": "run_shell_command-1771084181023-8a0f0e70713e6",
  "name": "run_shell_command",
  "response": {
    "output": "Command was automatically cancelled because it exceeded the timeout of 5.0 minutes without output. Below is the output before it was cancelled:\nReading keithrozariocom.WordPress.2026-02-13.xml...\nFound 1948 items. Processing...\nMigration complete. Success: 622, Failed: 2\n[main 6789f2d] Strip <a> tags from images to fix rendering issues\n 418 files changed, 794 insertions(+), 797 deletions(-)\nEnumerating objects: 825, done.\nCounting objects: 100% (825/825), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (423/423), done.\nWriting objects: 100% (423/423), 96.36 KiB | 10.71 MiB/s, done.\nTotal 423 (delta 401), reused 0 (delta 0), pack-reused 0 (from 0)\nremote: Resolving deltas: 100% (401/401), completed with 401 local objects.\nTo https://github.com/keithrozario/keithrozariodotcom.git\n   bec22f7..6789f2d  main -> main\n? Select a workflow run  [Use arrows to move, type to filter]\n> * Strip <a> tags from images to fix rendering issues, Deploy Hugo site to Pages [main] 11s ago"
  }
}
```

---

## MODEL âœ¨

The push was successful and the Action is running.

I will wait for the build to complete (approx 2 minutes) and then verify the fix on the `access-keys-in-aws-lambda` post.

I will use `gh run list` to check status.**Tool Command**:
```json
{
  "name": "run_shell_command",
  "args": {
    "command": "gh run list --limit 1\nsleep 10\ngh run list --limit 1"
  }
}
```

---

## USER ğŸ§‘â€ğŸ’»

**Tool Response**:
```json
{
  "id": "run_shell_command-1771084504822-951af71ee0e06",
  "name": "run_shell_command",
  "response": {
    "output": "Output: STATUS  TITLE                  WORKFLOW          BRANCH  EVENT  ID           ELAPSED  AGE             \nâœ“       Strip <a> tags fro...  Deploy Hugo s...  main    push   22020112450  2m4s     about 5 minut...\nSTATUS  TITLE                  WORKFLOW          BRANCH  EVENT  ID           ELAPSED  AGE             \nâœ“       Strip <a> tags fro...  Deploy Hugo s...  main    push   22020112450  2m4s     about 5 minut...\nProcess Group PGID: 19912"
  }
}
```

---

## MODEL âœ¨

Deployment successful.

Now I will verify the post `access-keys-in-aws-lambda` again.

I expect to see:
1.  **Images rendering** (no more `<figure>` or `<a>` wrappers around the image syntax).
2.  **Srcset present** (since these images are likely large screenshots).

I'll curl the page and check for images.

**Tool Command**:
```json
{
  "name": "run_shell_command",
  "args": {
    "command": "curl -s -L https://keithrozario.com/2020/06/access-keys-in-aws-lambda/ | grep \"<img\" | head -n 5"
  }
}
```

---

## USER ğŸ§‘â€ğŸ’»

**Tool Response**:
```json
{
  "id": "run_shell_command-1771084558215-f7b2b885f0198",
  "name": "run_shell_command",
  "response": {
    "output": "Output: \"><meta property=\"og:url\" content=\"https://keithrozario.com/2020/06/access-keys-in-aws-lambda/\"><meta property=\"og:site_name\" content=\"keithrozario.com\"><meta property=\"og:image\" content=\"https://keithrozario.com/og-image.png\"><meta property=\"og:image:width\" content=\"1200\"><meta property=\"og:image:height\" content=\"627\"><meta property=\"article:section\" content=\"Keith's Favorite Post\"><meta property=\"article:section\" content=\"Security &amp;amp; Privacy\"><meta property=\"article:section\" content=\"Serverless\"><meta property=\"article:published_time\" content=\"2020-06-14 17:55:51 +0000 UTC\"><link rel=\"shortcut icon\" href=https://keithrozario.com/favicon.ico></head><body><div class=container><header class=header><div class=header__inner><div class=header__logo><a href=https://keithrozario.com/><div class=logo>keithRozario</div></a></div><ul class=\"menu menu--mobile\"><li class=menu__trigger>Menu&nbsp;â–¾</li><li><ul class=menu__dropdown><li><a href=/contact>Contact Me</a></li><li><a href=/categories/keiths-favorite-post>Favorites</a></li><li><a href=/posts>Posts</a></li><li><a href=/categories/security-amp-privacy>Security</a></li><li><a href=/categories/serverless>Serverless</a></li></ul></li></ul></div><nav class=navigation-menu><ul class=\"navigation-menu__inner menu--desktop\"><li><a href=/contact>Contact Me</a></li><li><a href=/categories/keiths-favorite-post>Favorites</a></li><li><a href=/posts>Posts</a></li><li><a href=/categories/security-amp-privacy>Security</a></li><li><a href=/categories/serverless>Serverless</a></li></ul></nav></header><div class=content><article class=post><h1 class=post-title><a href=https://keithrozario.com/2020/06/access-keys-in-aws-lambda/>Access Keys in AWS Lambda</a></h1><div class=post-meta><time class=post-date>2020-06-14</time></div><div class=post-content><div><p><img src=/uploads/lambda-sts.png alt loading=lazy></p><p>Let's look at AWS Access Keys inside a Lambda function, from how they are populated into the function's <a href=https://docs.aws.amazon.com/lambda/latest/dg/runtimes-context.html target=_blank rel=\"noreferrer noopener\">execution context</a>, how long they last, how to exfiltrate them out and use them, and how we might detect an compromised access keys.</p><p>But before that, let's go through some basics. Lambda functions run on <a href=https://aws.amazon.com/blogs/aws/firecracker-lightweight-virtualization-for-serverless-computing/ target=_blank rel=\"noreferrer noopener\">Firecracker</a>, a microVM technology developed by Amazon. MicroVMs are like docker containers, but provide VM level isolation between instances. But because we're not going to cover container breakouts here, for the purpose of this post we'll use the term container to refer to these microVMs.</p><p>Anyway...</p><p>Lambda constantly spins up containers to respond to events, such as http calls via API Gateway, a file landing in an S3 bucket, or even an invoke command executed from your aws-cli.</p><p>These containers interact with AWS services in the same exact way as any code in EC2, Fargate or even your local machine -- i.e. they use a version of the AWS SDK (e.g. boto3) and authenticate with IAM access keys. There isn't any magic here, it's just with serverless we can remain blissfully ignorant of the underlying mechanism.</p><p>But occasionally it's a good idea to dig deep and try to understand what goes on under the hood, and that's what this post seeks to do.</p><p>So where in the container are the access keys stored? Well, we know that AWS SDKs reference credentials in 3 places:</p><ul><li>Environment Variables</li><li>The <code>~/.aws/credentials</code> file</li><li>The Instance Metadata Service (IMDS)</li></ul><p>If we check, we'll find that our IAM access keys for lambda functions are stored in the environment variables of the execution context, namely:</p><ul><li>AWS_ACCESS_KEY_ID</li><li>AWS_SECRET_ACCESS_KEY</li><li>AWS_SESSION_TOKEN</li></ul><p>You can easily verify this, by printing out those environment variables in your runtime (e.g. <code>$AWS_ACCESS_KEY_ID</code>) and see for yourself.</p><p>OK, now we know where the access stored keys are stored, but how did they end up here and what kind of access keys are they? For that, we need to look at the life-cycle of a Lambda function...</p><h2>Access Keys in a Function</h2><p>Every lambda function starts as a piece of code stored within the Lambda service. When the function is first invoked, it undergoes a <strong>cold-start</strong>, which creates an execution context for that function before executing it. If the subsequent invocation occurs shortly after, Lambda re-uses that execution context, resulting a much quicker <strong>warm-start</strong>.</p><p>A cold-start involves finding some compute resource within the Lambda service, and creating the container to run our code within those resources. The warm-start simply reuses that container, and is therefore faster. But a cold start isn't a once-in-a-lifetime event, it occurs fairly often as Lambda purges old containers off the platform to make way for new ones.</p><p><img src=/uploads/cold-start_hu_114737835e6094a8.png srcset=\"/uploads/cold-start_hu_f6fd549662a20510.png 480w,\n/uploads/cold-start.png 1173w\" sizes=\"(max-width: 480px) 480px, (max-width: 768px) 768px, (max-width: 1024px) 1024px, 100vw\" alt loading=lazy><figcaption>Inspired by: <a href=\"https://www.twitch.tv/videos/647501563?t=00h07m51s\">https://www.twitch.tv/videos/647501563?t=00h07m51s</a></figcaption></figure></p><p>This picture was inspired by a wonderful talk from James Beswick linked <a href=\"https://www.twitch.tv/videos/647501563?t=00h07m51s\" target=_blank rel=\"noreferrer noopener\">here</a>. But we still don't see any injection of access keys ... for that we need to dig into Cloudtrail logs.</p><p>In Cloudtrail, we discover that a cold-start doesn't just create the container, it also creates an <code>sts:AssumeRole</code> and <code>logs:CreateLogStream</code> event. I don't know precisely where these events occur during the cold-start, but imagine it's something like this:</p><figure class=\"wp-block-image size-large\">![](/uploads/cold-start-full.png)<p>We know from the logs that <code>sts:AssumeRole</code> is invoked by the Lambda Service, <strong>lambda.amazonaws.com</strong> (not the lambda container!). Here's a stripped down event from CloudTrail of the AssumeRole, notice the credentials expiration date against the eventTime, and also note that this call was made from lambda.amazonaws.com.</p><p><img src=/uploads/assume-role-3_hu_3709dbfb253acbc6.png srcset=\"/uploads/assume-role-3_hu_d0d5283cf0907755.png 480w,\n/uploads/assume-role-3.png 1716w\" sizes=\"(max-width: 480px) 480px, (max-width: 768px) 768px, (max-width: 1024px) 1024px, 100vw\" alt loading=lazy></p><p>The next event for this function is <code>logs:CreateLogStream</code>, which is executed from the lambda container using the newly minted access keys (from AssumeRole). Notice here we have an actual sourceIPAddress field and userAgent, but more importantly, we can see that the <code>userIdentity.accessKeyId</code> matches the key id from the previous event. Here's the event:</p><p><img src=/uploads/CreateLogStream-1_hu_8a3bc9e8349392bd.png srcset=\"/uploads/CreateLogStream-1_hu_fcb78c1b4ff83ea5.png 480w,\n/uploads/CreateLogStream-1.png 1768w\" sizes=\"(max-width: 480px) 480px, (max-width: 768px) 768px, (max-width: 1024px) 1024px, 100vw\" alt loading=lazy></p><p><em>Strangely for my Python function this was executed by a Rust SDK user agent, suggesting it occurs even before the execution environment is setup</em>.</p><p>It looks like the Lambda Service, gets Temporary Access Keys from STS, and injects them into the container during a cold-start. From there the container uses those credentials for everything it needs to do, not just creating the new logstream. Here's an example of the CloudTrail event of the same function Listing all Buckets in an account:</p><p><img src=/uploads/ListBuckets-1_hu_33d8adff3c5d45fc.png srcset=\"/uploads/ListBuckets-1_hu_cf9aa3a99a1f549b.png 480w,\n/uploads/ListBuckets-1.png 2048w\" sizes=\"(max-width: 480px) 480px, (max-width: 768px) 768px, (max-width: 1024px) 1024px, 100vw\" alt loading=lazy></p><p>We can correlate them using the <code>$.userIdentity.accessKeyId</code> field, as they'll be the same across these operations. So we can know that a cold-start generates these temporary access keys, but how are these temporary credentials rotated?</p><p>Short answer is ... They Don't!</p><h2>Access Key Lifecycle</h2><p>At least from my research, Lambda always requests for 12 hour tokens (which is the maximum duration possible). I've confirmed this from other content online as well, including <a href=https://youtu.be/H4WoQd2yVJQ>this</a> great talk from ServerlessDays Virtual. And because 12 hours is a STS limit (not Lambda) we see the same behavior for provisioned concurrent functions as well.</p><p>My theory is that a Lambda Execution Context wouldn't last as long as the token, and hence tokens don't need refreshing.</p><p>To test this theory, I created a lambda function that ran every 3 minutes, and kept it running for over 10 hours. The function performed one API Call that listed all the accounts S3 buckets, the actual call is irrelevant, it's purpose is to help us log the usage of the access key in CloudTrail.</p><p>Indeed, I found that the average lifespan of the container came in at just over 2 hours, far shorter than the 12 hour lifespan of the STS token.</p><figure class=\"wp-block-table is-style-stripes\"><table><tbody><tr><td><strong>Access Key ID</strong></td><td><strong>First Event</strong></td><td><strong>Last Event</strong></td><td><strong>Duration</strong></td><td><strong>Source IP</strong></td></tr><tr><td>ASIA-1</td><td>02:31:24</td><td>04:37:24</td><td>126 min</td><td>54.255.220.21</td></tr><tr><td>ASIA-2</td><td>04:38:46</td><td>06:40:24</td><td>122 min</td><td>18.140.233.226</td></tr><tr><td>ASIA-3</td><td>06:42:11</td><td>08:49:24</td><td>127 min</td><td>18.141.233.97</td></tr><tr><td>ASIA-4</td><td>08:52:24</td><td>10:55:24</td><td>123 min</td><td>13.251.102.127</td></tr><tr><td>ASIA-5</td><td>10:55:33</td><td>12:49:23</td><td>114 min</td><td>13.228.70.250</td></tr></tbody></table></figure><p>AWS are pretty coy about how long containers get recycled in Lambda, I think it's down to an algorithm that takes into account the current volume on the entire Lambda service in that region at that instant, and a some other metrics from your functions invocation history. Hence it's pretty unpredictable, which makes publishing any estimate very hard.</p><p>The important piece of information though is that once a Temporary Access Key is generated -- it is <span style=text-decoration:underline>only used by the exact same IP for all future API calls</span>. There is a tight affinity between these Temporary Access Keys and IP addresses. I assume this is because once a container is created, it is assigned a static IP address that is never changed (but might be shared with other containers).</p><p>From the CloudTrail logs, I see that everytime a new <code>sts:AssumeRole</code> event is called for the function, there is a corresponding <code>logs:CreateLogStream</code> event, and the next operation of the function uses the new Access Key ID <strong>and</strong> has a new source IP. All of this suggest the old container was recycled, and we went through a cold-start.</p><p><img src=/uploads/ip-accesskey.png alt loading=lazy></p><p>Under normal conditions, an Access Key ID is never shared across multiple source IP addresses. So let's put this all together shall we.</p><h2>Putting it all together</h2><p>It's a bit hard to make conclusions based on reverse engineering log files, but I'll go out on a limb and say the following:</p><ul><li>An <code>sts:AssumeRole</code> call from the Lambda service is made during a function's cold-start</li><li>The assume Role generates a temporary access key with<ul><li>A new Access Key ID</li><li>A new Session Token</li></ul></li><li>The credentials are injected into the function's new execution context</li><li>The function then has the credentials it needs for API Calls</li><li>The function execution context will die before the credentials expire</li><li>The behavior is identical for provisioned concurrency function</li></ul><p>The temporary access keys are for the Execution Role of the function, which means they are limited to whatever the permissions the role has. Remember lambda functions have a function-policy and an execution-role, these define who can invoke the function and what the role can do respectively. For this, we're only interested in execution role.</p><p>So now we know where the access keys are, and where they came from. How do we exfiltrate them? That's actually an easy answer.</p><p>Any compromise on the function's code, that would allow <span style=text-decoration:underline>access to the environment variables</span> would suffice, because that's where the keys are stored (unencrypted!). From there we could use a standard boto3 session to impersonate the lambda function from our local machine, or just about anywhere we could run an AWS SDK.</p><p>But is there a way to differentiate a legitimate API call from a lambda function vs. a malicious API call using these stolen access keys?</p><p>Maybe....</p><h2>Detecting Stolen Keys</h2><p>Here's a call I used from my Macbook using the STS tokens from a function:</p><p><img src=/uploads/ListBuckets_CLI_hu_3b9c6e3fc5674861.png srcset=\"/uploads/ListBuckets_CLI_hu_b7dc88c2f44a23e3.png 480w,\nProcess Group PGID: 19938"
  }
}
```

---

## MODEL âœ¨

**It works!**

1.  **Image Rendering:** `<p><img src=/uploads/lambda-sts.png alt loading=lazy></p>`
    *   No more `<figure>` or `<div class="wp-block-image">` wrappers blocking it.
    *   The `<a>` tag is gone (which is fine, the image displays).

2.  **Responsive Images:** `<img src=/uploads/cold-start_hu_... srcset="..." ...>`
    *   Large images like `cold-start.png` are getting `srcset`.
    *   Small images like `lambda-sts.png` are not (which is correct/expected).

3.  **URLs:** `https://keithrozario.com/2020/06/access-keys-in-aws-lambda/` returns 200 OK.

The site is now correctly displaying responsive images with clean Markdown syntax.

Is there anything else?

---

## USER ğŸ§‘â€ğŸ’»

Can you check all post in security&privacy and Favorite to ensure the images all render corrrectly there.

---

## MODEL âœ¨

I will search the `keithrozario_blog/content/posts` directory to identify all posts belonging to the "Security & Privacy" and "Keith's Favorite Post" categories. I'll use `grep` to list the filenames, which will help me verify that images are rendering correctly in these specific posts.**Tool Command**:
```json
{
  "name": "run_shell_command",
  "args": {
    "command": "grep -l \"Security & Privacy\" keithrozario_blog/content/posts/*.md\ngrep -l \"Keith's Favorite Post\" keithrozario_blog/content/posts/*.md"
  }
}
```

---

## USER ğŸ§‘â€ğŸ’»

**Tool Response**:
```json
{
  "id": "run_shell_command-1771084647542-8b9058eacf92",
  "name": "run_shell_command",
  "response": {
    "output": "Output: keithrozario_blog/content/posts/access-keys-in-aws-lambda.md\nkeithrozario_blog/content/posts/apple-vs-fbi-everything-you-need-to-know.md\nkeithrozario_blog/content/posts/auditor-general-report-2011-malaysian-government-it-spend-transparency.md\nkeithrozario_blog/content/posts/bitcoin-miners-dilema-sabotage-profitable.md\nkeithrozario_blog/content/posts/can-you-out-tech-the-government.md\nkeithrozario_blog/content/posts/chip-and-pin-an-intro-for-malaysians.md\nkeithrozario_blog/content/posts/contact-tracing-apps-in-this-context-their-ok.md\nkeithrozario_blog/content/posts/digi-wwwwow-2012-fave-tech-head-keiths-blog.md\nkeithrozario_blog/content/posts/dlink-dir-615-wi-fi-password-unifi-hack.md\nkeithrozario_blog/content/posts/downloading-irshad-manji-ebook-illegal-malaysian-law.md\nkeithrozario_blog/content/posts/evidence-act-114a-technological-misconceptions.md\nkeithrozario_blog/content/posts/evidence-act-amendment-malaysia.md\nkeithrozario_blog/content/posts/evidence-act-anonymity-before-internet.md\nkeithrozario_blog/content/posts/fair-usage-policy-data-caps-torrent-filter.md\nkeithrozario_blog/content/posts/gov-my-tls-audit.md\nkeithrozario_blog/content/posts/gov-tls-audit-architecture.md\nkeithrozario_blog/content/posts/grab-isnt-good.md\nkeithrozario_blog/content/posts/hack-unifi-in-5-minutes.md\nkeithrozario_blog/content/posts/hackers-and-terrorist.md\nkeithrozario_blog/content/posts/hacking-government.md\nkeithrozario_blog/content/posts/heres-one-thing-thats-already-changed-post-ge14.md\nkeithrozario_blog/content/posts/internet-censorship-wont-work-malaysia.md\nkeithrozario_blog/content/posts/internet-is-slow-because-of-illegal-download.md\nkeithrozario_blog/content/posts/it-career-malaysia.md\nkeithrozario_blog/content/posts/keithrozario-on-bfm-talking-about-spyware.md\nkeithrozario_blog/content/posts/macc-facebook-working-hours-corruption-facebook-effects-employee-productivity.md\nkeithrozario_blog/content/posts/malaysia-spyware-hacking-team.md\nkeithrozario_blog/content/posts/malaysian-crowdfunding-success-teach-a-child-to-read-pitchin-my.md\nkeithrozario_blog/content/posts/malaysian-government-network-download-porn-privacy-dead.md\nkeithrozario_blog/content/posts/malaysians-against-the-trans-pacific-partnership-agreement.md\nkeithrozario_blog/content/posts/msc-cloud-initiative-malaysia.md\nkeithrozario_blog/content/posts/my-experience-with-aws-certified-security-specialty.md\nkeithrozario_blog/content/posts/najib-razak-cybersecurity-year-end-message-malaysia.md\nkeithrozario_blog/content/posts/no-the-prime-minister-doesnt-need-spyware.md\nkeithrozario_blog/content/posts/nuffnang-ads-malaysia-stopped.md\nkeithrozario_blog/content/posts/off-pirated-software-and-vaccinations.md\nkeithrozario_blog/content/posts/one-visa-files-suit-against-tm-is-it-a-human-rights-abuse.md\nkeithrozario_blog/content/posts/personal-data-protection-act-2010-malaysia.md\nkeithrozario_blog/content/posts/questions-we-need-to-ask-about-spyware.md\nkeithrozario_blog/content/posts/ransomware.md\nkeithrozario_blog/content/posts/read-this-before-you-vote.md\nkeithrozario_blog/content/posts/relax-dear-citizen-your-contactless-card-is-still-relatively-safe-ish.md\nkeithrozario_blog/content/posts/sayakenahack-com.md\nkeithrozario_blog/content/posts/sayakenahack-epilogue.md\nkeithrozario_blog/content/posts/show-notes-for-today.md\nkeithrozario_blog/content/posts/skmm-study-dropped-blocked-call-rate.md\nkeithrozario_blog/content/posts/so-you-think-english-is-the-lingua-franca-of-science.md\nkeithrozario_blog/content/posts/software-piracy-china.md\nkeithrozario_blog/content/posts/sospa-udhr-finspy-malaysia.md\nkeithrozario_blog/content/posts/telekom-malaysia-t-is-censoring-the-internet-prior-to-ge13.md\nkeithrozario_blog/content/posts/the-malaysian-government-is-spying-on-you-finspy-fisher.md\nkeithrozario_blog/content/posts/the-safest-place-for-your-money-is-under-the-mattress.md\nkeithrozario_blog/content/posts/two-years-on-teaching-coding-in-schools-declared-a-success.md\nkeithrozario_blog/content/posts/understanding-anonymous-from-a-malaysian-context.md\nkeithrozario_blog/content/posts/watch-netflix-hulu-bbc-spotify-malaysia.md\nkeithrozario_blog/content/posts/what-is-prism.md\nkeithrozario_blog/content/posts/when-bad-advice-comes-from-good-people.md\nkeithrozario_blog/content/posts/who-updates-wikipedia-malaysia.md\nkeithrozario_blog/content/posts/why-apple-suing-samsung-google-lawsuit.md\nkeithrozario_blog/content/posts/why-does-sayakenahack-have-dummy-data.md\nkeithrozario_blog/content/posts/why-is-malaysia-trailing-singapore-taiwan-korea.md\nkeithrozario_blog/content/posts/why-malaysia-should-never-send-anyone-into-space-again-ever.md\nkeithrozario_blog/content/posts/why-malaysians-shouldnt-buy-coin-yet.md\nkeithrozario_blog/content/posts/why-we-fear-hackers-dangers-of-technical-illiteracy.md\nkeithrozario_blog/content/posts/writing-a-wordpress-restoration-script.md\nProcess Group PGID: 19945"
  }
}
```

---

## MODEL âœ¨

The grep command returned an error for the second pattern, likely because I didn't escape the single quote in "Keith's Favorite Post".

However, I have a good list of "Security & Privacy" posts. I will pick 3 random posts from this list and verify them.

Selected Posts:
1.  `keithrozario_blog/content/posts/apple-vs-fbi-everything-you-need-to-know.md`
2.  `keithrozario_blog/content/posts/chip-and-pin-an-intro-for-malaysians.md`
3.  `keithrozario_blog/content/posts/gov-tls-audit-architecture.md`

I need to know the **URL** for these.
The URLs are constructed from the date and slug in the frontmatter.

I will read the frontmatter for these 3 posts.

Also, I'll retry the grep for "Favorite" properly to get a few of those.

**Tool Command**:
```json
{
  "name": "run_shell_command",
  "args": {
    "command": "grep -l \"Keith.s Favorite Post\" keithrozario_blog/content/posts/*.md | head -n 3\nhead -n 5 keithrozario_blog/content/posts/apple-vs-fbi-everything-you-need-to-know.md\nhead -n 5 keithrozario_blog/content/posts/chip-and-pin-an-intro-for-malaysians.md\nhead -n 5 keithrozario_blog/content/posts/gov-tls-audit-architecture.md"
  }
}
```

---

## USER ğŸ§‘â€ğŸ’»

**Tool Response**:
```json
{
  "id": "run_shell_command-1771084657507-305078ca238f2",
  "name": "run_shell_command",
  "response": {
    "output": "Output: keithrozario_blog/content/posts/access-keys-in-aws-lambda.md\nkeithrozario_blog/content/posts/apple-vs-fbi-everything-you-need-to-know.md\nkeithrozario_blog/content/posts/auditor-general-report-2011-malaysian-government-it-spend-transparency.md\n+++\ntitle = \"Apple vs. FBI: Everything you need to know\"\nslug = \"apple-vs-fbi-everything-you-need-to-know\"\ndate = \"2016-02-20T09:26:04\"\ndraft = false\n+++\ntitle = \"Chip And Pin : An intro for Malaysians\"\nslug = \"chip-and-pin-an-intro-for-malaysians\"\ndate = \"2015-10-29T11:59:54\"\ndraft = false\n+++\ntitle = \"Gov TLS Audit : Architecture\"\nslug = \"gov-tls-audit-architecture\"\ndate = \"2018-04-14T00:47:40\"\ndraft = false\nProcess Group PGID: 19950"
  }
}
```

---

## MODEL âœ¨

**Target URLs:**

1.  **Apple vs FBI:** `2016/02/apple-vs-fbi-everything-you-need-to-know/`
2.  **Chip and Pin:** `2015/10/chip-and-pin-an-intro-for-malaysians/`
3.  **Gov TLS Audit:** `2018/04/gov-tls-audit-architecture/`

I will curl these 3 URLs and check for `<img>` tags. I expect standard `<img>` tags (wrapped in `<p>` maybe), and `srcset` if the images are large.

I'll also verify `access-keys-in-aws-lambda` which appeared in the "Favorite" list.

**Tool Command**:
```json
{
  "name": "run_shell_command",
  "args": {
    "command": "curl -s -L https://keithrozario.com/2016/02/apple-vs-fbi-everything-you-need-to-know/ | grep \"<img\" | head -n 3\ncurl -s -L https://keithrozario.com/2015/10/chip-and-pin-an-intro-for-malaysians/ | grep \"<img\" | head -n 3\ncurl -s -L https://keithrozario.com/2018/04/gov-tls-audit-architecture/ | grep \"<img\" | head -n 3"
  }
}
```

---

## USER ğŸ§‘â€ğŸ’»

**Tool Response**:
```json
{
  "id": "run_shell_command-1771084667863-db8d05603f283",
  "name": "run_shell_command",
  "response": {
    "output": "Output: \"><meta property=\"og:url\" content=\"https://keithrozario.com/2016/02/apple-vs-fbi-everything-you-need-to-know/\"><meta property=\"og:site_name\" content=\"keithrozario.com\"><meta property=\"og:image\" content=\"https://keithrozario.com/og-image.png\"><meta property=\"og:image:width\" content=\"1200\"><meta property=\"og:image:height\" content=\"627\"><meta property=\"article:section\" content=\"CyberLaw\"><meta property=\"article:section\" content=\"Keith's Favorite Post\"><meta property=\"article:section\" content=\"Security &amp;amp; Privacy\"><meta property=\"article:published_time\" content=\"2016-02-20 09:26:04 +0000 UTC\"><link rel=\"shortcut icon\" href=https://keithrozario.com/favicon.ico></head><body><div class=container><header class=header><div class=header__inner><div class=header__logo><a href=https://keithrozario.com/><div class=logo>keithRozario</div></a></div><ul class=\"menu menu--mobile\"><li class=menu__trigger>Menu&nbsp;â–¾</li><li><ul class=menu__dropdown><li><a href=/contact>Contact Me</a></li><li><a href=/categories/keiths-favorite-post>Favorites</a></li><li><a href=/posts>Posts</a></li><li><a href=/categories/security-amp-privacy>Security</a></li><li><a href=/categories/serverless>Serverless</a></li></ul></li></ul></div><nav class=navigation-menu><ul class=\"navigation-menu__inner menu--desktop\"><li><a href=/contact>Contact Me</a></li><li><a href=/categories/keiths-favorite-post>Favorites</a></li><li><a href=/posts>Posts</a></li><li><a href=/categories/security-amp-privacy>Security</a></li><li><a href=/categories/serverless>Serverless</a></li></ul></nav></header><div class=content><article class=post><h1 class=post-title><a href=https://keithrozario.com/2016/02/apple-vs-fbi-everything-you-need-to-know/>Apple vs. FBI: Everything you need to know</a></h1><div class=post-meta><time class=post-date>2016-02-20</time></div><div class=post-content><div><p><img src=/uploads/broken-fence.jpg alt=broken-fence loading=lazy>A judge in the US has ordered Apple to provide &rsquo;technical assistance&rsquo; to FBI, in creating what some (but not all) cybersecurity experts call a backdoor. In the few years I&rsquo;ve written about theseÂ issues, I&rsquo;ve never seen anythingÂ as hotly debated as this one, across the folks from digital security to foreign policy all coming down on both sides of the debate.</p><p>On one hand it seems a bit snarky of the FBI to use this one particular case, thatÂ looksÂ to have the highest possible chance of success to set precedent, but on the other hand it seemsÂ mighty nasty of Apple to refuse to comply with a court order, to crack into a terrorist phone.</p><p>So here&rsquo;sÂ some facts of the case.</p><p>The phone in question belonged to Syed Rizwan Farook, a shooterÂ in the <a href=https://en.wikipedia.org/wiki/2015_San_Bernardino_attack target=_blank>San Bernadino shooting</a>, which caused the deaths of 14Â people. America has numerous mass shootings, but this one involved two Muslims aligned to ISIS&ndash;and hence more easily labeled terrorism, without the need for adjectives like &lsquo;domestic&rsquo;.</p><p>As I blogged about last week, <a href=https://www.keithrozario.com/2016/02/being-terrified-the-price-of-terrorism.html target=_blank>self-radicalized terrorist don&rsquo;t get funding from headquarters</a>, and without that glorious ISIS-oilÂ money, all these guys could afford for was an iPhone 5C, an entry-level phone with hardware identical to that of theÂ iPhone 5, a phone launched waaaayy back inÂ 2012 <em>(you&rsquo;ll remember that as the year Manchester United last won the Premier League)</em>. As an older phone, the security architecture of the 5CÂ lagged behind theÂ current generation iPhones, all of which have a secure enclave, but make no mistake, it&rsquo;s still pretty secure.</p><p>By pretty secure, I mean that the phone has all of its contents encrypted, and un-readable to anyone without the encryption key. The key is derived from both the user passcode, and a randomly generated hardwareÂ key that is unique to the specific iPhone. It is generally understood that Apple doesn&rsquo;t keep track of the hardware key, and thereforeÂ unable to provide it, as you might expect the hardware will also never give up it&rsquo;s key under any circumstance. Without the hardware key, the encrypted Â data is unreadable, even with the passcode. Which explains why the FBI can&rsquo;t suck the data out of the device for decryption on a more powerful computer, or load the data into 1000&rsquo;s of iPhones for parallel cracking.</p><p>But even with the phone, things remain tough. To decrypt the phone, both the hardware AND the passcode is required.Â And if you don&rsquo;t know the passcode, things don&rsquo;t look good.Â iPhones are hardened against passcode guessing, just like ATM cards. If a thief managed to obtain your ATM card, they could take it to any ATM and try to guess your PIN Code, butÂ they only get 3 attempts before the ATM sucks in the card, and ends the attack. The odds of guessing a 6-digit passcode with just 3 attempts is worse than striking the lottery&ndash;that&rsquo;s why losing your ATM card isn&rsquo;t that big a deal, and from a Â strictly data perspective, losing your iPhone isn&rsquo;t a big deal either&ndash;provided you secured it with a long enough passcode. <em>(of course no one likes losing a RM3000 phone)</em></p><p>Similarly, iOS works to limit an attackers ability to guess its passcode, either byÂ slowing-down the Â rate of guessesÂ (through artificially delaying retry attempts, sometimes by hours), or by just erasing the entire contents of the phone after 10 incorrect passcode entries. The latter being a feature the user has to actively switch on.</p><p>All these protections against brute-force passcode guessing are baked directly into iOS, Apple&rsquo;s iPhoneÂ operating system, and even though this was an older generation phone it still had the latest software (yet another reason to admire Apple). Which also means that <a href=https://www.keithrozario.com/2015/04/worked-example-iphone-pin-hack.html>previous hacksÂ whichÂ bypassed the protections</a> have also been patched. Like I said&mldr;pretty secure.</p><p>To bypass these obstacles, the FBI is askingÂ Apple to provide them with a special <em>tailor made</em>Â version of iOSÂ that would eliminate these protections, specifically:</p><ul><li>The<strong> removal of the Auto-Erasing feature</strong>, which would otherwise erase the contents of the phone after 10 incorrect passcode entries.</li><li>The<strong> removal of the Â artificial delays</strong> in attempting the passcodes, allowing the FBI to try passcodes at the fastest rate the hardware will allow.</li><li>The <strong>ability to submit passcodes electronically</strong> to the iPhone which eliminates the need of a human person manually entering the passcodes by hand</li><li>And, the special software could be further <strong>customized to work on only this one iPhone</strong>, down to the serial number etc..</li></ul>In essence, the FBI is asking Apple to create a 'special' ATM, that would allow them to try PIN codes for aÂ specific ATM card without sucking in the card after 3 failed attempts. That 'special' ATM would also have a ability to attempt the PINs electronically rather than having someone manually enter the PINs.<p>If a terrorist told you that the co-ordinates for a bomb was the PIN to his ATM card, and neither the Bank nor the Police had anyway to ascertain the PIN, other than creating that &lsquo;special&rsquo; ATM&ndash;do you think the Bank should do it? Would it be considered a &lsquo;back-door&rsquo;?</p><p>All in all, it seems like a pretty reasonable request, but what makes this so controversial (at least in cyber security circles) is whether the 4 items above are considered a &lsquo;back-door&rsquo;. Some experts say it is, others say it isn&rsquo;t.</p><p>What isn&rsquo;t controversial is Apple&rsquo;s ability to do this. Experts all agree that <a href=http://blog.trailofbits.com/2016/02/17/apple-can-comply-with-the-fbi-court-order/ target=_blank>this is possible</a>, and evenÂ <a href=http://withoutbullshit.com/blog/apples-tim-cook-shows-how-to-communicate-in-a-crisis/>Tim Cook&rsquo;s brilliant PR response</a> didn&rsquo;tÂ deny their ability to do so. So it is whether Apple &lsquo;should&rsquo; do this, that is the question.</p><p>&lsquo;Should&rsquo; is a strange word, this is after all a phone that belonged to a <em>terrorist</em>, and shouldn&rsquo;t Apple do everything in its power to help law enforcement? What if the phone has contact details of other ISIS operatives in the US, or what if itÂ had more details of ISIS operations in Syria, wouldn&rsquo;t we want that, regardless of how &lsquo;burdensome&rsquo; it might be to Apple. This has nothing to do with the 4th amendment, after all the owner of the iPhone is dead, and a court warrant is available&ndash;so what&rsquo;s stopping Apple?<em> [update: ActuallyÂ the &lsquo;owner&rsquo; of the iPhone was the San Bernadino county, which meant it belonged to a government, not the individual, definitively ending any 4th amendment protections]</em></p><p>To be fair, there&rsquo;s only a very smallÂ chance that these &lsquo;self-radicalized&rsquo; lone wolves have anything of grave importance on their old iPhone, but when words like terrorism are bandied about, a 1% is more than enough.</p><p>Which is why I feel it&rsquo;s a bit snarky for the FBI to use this case, it seems the perfect case. It involves terrorism (which pushes everyone&rsquo;s emotional buttons), an old iPhone that can be cracked and violates no constitutional protections. But you can bet, that if it succeeds on a legal level, that the precedent set by this case, will be used for other cases that don&rsquo;t involve terrorismÂ and affect more recent versions of iPhones.</p><p>And that&rsquo;s what scares Apple.</p><p>That if it sets a precedent by complying with <em>this</em> court order, then it will have to comply with all <em>other</em> court order, potentially thousands more, requesting the very same thing, and the perception of iPhone security will take a severe beating&ndash;after all there&rsquo;s a reason drug lords and terrorist use iPhones. Let&rsquo;s also accept, that if one Judge rules for Apple to comply for this iPhone, explaining to futures court Judges the technical intricacies of the secure enclave, RAM Disk implications, and digital signatures of future iPhones would be a task so monumental that from a legal standpoint <span style=text-decoration:underline>this iPhone 5c is all iPhones</span>.</p><p>The question of &lsquo;should&rsquo; must also take into account the precedent this might set, and look beyond the specifics of this one case.</p><p>Plus it really <em>would</em> be burdensome on Apple to replicate what they might do in this one case, thousands of times. Everytime they do so, they&rsquo;d have to sign the new operating system, which means they&rsquo;d have to get their super-secret iOS signing key and digitally sign the software. Apple&rsquo;s entire iOS security is premised on keeping that key secret, and since iOS operates on all of their iPhones, and iPhones are a $50 Billion dollar a year business, the signing key is quite possible the <strong>most valuable secret in the world</strong>. If you have it, you would have broken all of iOS security, plain and simple.</p><p>If you were a shareholder, would you like Apple to get out this secret key, every time the FBI, DEA or NYPD come knocking? And what&rsquo;s to stop this precedent from being enforced internationally? Why would China, Apple&rsquo;s largest growing market, feel that their law-enforcement isn&rsquo;t entitled to the same privilege as the FBI?</p><p>How would you feel if Apple complied with the request to the FBI for this terrorist case, and then complied with other request from Israel, China, Australia&mldr;or Malaysia?</p><p>And finally to circle back to the Bank analogy, the ATM has no personal data,Â and the data in the iPhone doesn&rsquo;t represent a clear and imminent threat, it was details of an event that had already happened.</p><p>If you were CEO of Apple what would your stand be?</p><h2>Post Script</h2>The iPhone has a hardware limit of 80ms per password attempt, which would take about 15 minutes to crack a 4-digit passcode, and 1 day to crack a 6-digit one. However, a sufficiently lengthy alphanumeric passcode (composed of both digits and characters)would still take years to crack even if the FBI obtained their special iOS, so you know what to do if you want to protect your future drug empire.<p>Apple has <a href=http://techcrunch.com/2016/02/18/no-apple-has-not-unlocked-70-iphones-for-law-enforcement/>never complied to similar requests before</a>, even though you&rsquo;ve read that<a href=http://www.thedailybeast.com/articles/2016/02/17/apple-unlocked-iphones-for-the-feds-70-times-before.html> it acquiesced 70 times prior</a>, that&rsquo;s just technically wrong. I love Shane Harris and his <a href=https://spaghettionthewallproductions.com/rational-security/>rational securityÂ podcast</a>, but he doesn&rsquo;t get the nuance of this issue. First of all the number 70 is a government estimate, not something Apple agreed with, andÂ while we don&rsquo;t have the specifics of each case, some argue that this was much older iOS7 devices, which don&rsquo;t have built-in encryption&ndash;which meant data could be extracted without the need for a passcode.</p><p>But think about what that means, if Apple couldÂ extract the data from the phone without a passcode, what&rsquo;s to stopÂ the likes ofÂ Russian cyber-criminals or Chinese State-Sponsored hackers from doing it as well? Hence, Apple improved the security of their software to ensure that no one without the passcode could access the contents&ndash;andÂ what the FBI is requesting now is a tailor-made iOS that intentionally circumvents the protections, un-doing all of what Apple has engineered. This is the first time the FBI has requested this, and not the 70th time.</p><p><strong>#Update 1</strong>: As it turns-out the iPhone actually belong to the owners employer, the county Health Department, and part of the reason we&rsquo;re in the mess is that<a href=http://www.politico.com/story/2016/02/apple-iphone-privacy-justice-department-219505> they reset the password for the Apple ID account shortly after the attack</a>. Had they not done that, Apple may have been able to provide the data from the cloud back-up without all this kerfufle.</p><p><strong>#Update 2:</strong> Cybersecurity &lsquo;Legend&rsquo;, John McAffee has come out and said that his team of eliteÂ hackers will be able to <a href=http://www.businessinsider.com/john-mcafee-ill-decrypt-san-bernardino-phone-for-free-2016-2>crack the encryption of the iPhone in 3 weeks</a>. Now we know this is legit, because if you Google <a href=\"https://www.google.com/search?q=cybersecurity+legend\">&lsquo;Cybersecurity Legend&rsquo;</a> his name does come up&ndash;a lot! But McAfee is legendary for the wrong reasons, apart from having his name associated with a <a href=\"http://www.urbandictionary.com/define.php?term=McAfee\">&ldquo;A barely passable virus scanning program that updates at the worst possible times. &ldquo;</a>, he&rsquo;s also famous for <a href=\"https://www.youtube.com/watch?v=bKgf5PaBzyg\">producing youtube videos of him snorting drugs</a>Â with prostitutes, <a href=https://mcafee16.com/>running for president</a> and, believe it or not,Â <a href=\"http://www.amazon.com/s/?search-alias=stripbooks&amp;field-author-exact=John%20McAfee\">writing books about Yoga</a>.Â But jokes aside, the offer that he makes to the Feds are that he will crack the iPhone 5C in under 3 weeks, and he&rsquo;ll use social engineering. Now unless he&rsquo;s able to psychologically<a href=https://www.grahamcluley.com/2016/02/good-luck-john-mcafee-socially-engineering-corpse/> trick a dead person to reveal their passcodes</a>, or charm an iPhone into revealing its encryption key&ndash;this<strong> is a joke of an offer</strong>, more worthy of being reported in the Onion thanÂ on the countless media outlets who probably don&rsquo;t get it.</p><hr><p>For the best articles on the topic:</p><p><a href=https://www.eff.org/deeplinks/2016/02/technical-perspective-apple-iphone-case>EFF Deeplinks blog,</a> that answers all your high level technical question</p><p>Jonathan Zdziarski&rsquo;s <a href=\"http://www.zdziarski.com/blog/?p=5645\">amazing post</a>, which covers more deeply why this isn&rsquo;t only for one iPhone, and why legally Apple would have to make their &lsquo;broken&rsquo; iOS available to the defence counsel.</p><p>A <a href=\"http://www.zdziarski.com/blog/?p=5714#more-5714\">second Zdziarski post</a> on Bill Gates ribbon and ribbon-cutter analogy.</p></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><a href=https://keithrozario.com/2016/02/bitcoin-miners-dilema-sabotage-profitable/ class=\"button inline prev\">&lt; [<span class=button__text>The miners dilemma - Bitcoin sabotage can be profitable</span>]\n\"><meta property=\"og:url\" content=\"https://keithrozario.com/2015/10/chip-and-pin-an-intro-for-malaysians/\"><meta property=\"og:site_name\" content=\"keithrozario.com\"><meta property=\"og:image\" content=\"https://keithrozario.com/og-image.png\"><meta property=\"og:image:width\" content=\"1200\"><meta property=\"og:image:height\" content=\"627\"><meta property=\"article:section\" content=\"Keith's Favorite Post\"><meta property=\"article:section\" content=\"Malaysia\"><meta property=\"article:section\" content=\"Security &amp;amp; Privacy\"><meta property=\"article:published_time\" content=\"2015-10-29 11:59:54 +0000 UTC\"><link rel=\"shortcut icon\" href=https://keithrozario.com/favicon.ico></head><body><div class=container><header class=header><div class=header__inner><div class=header__logo><a href=https://keithrozario.com/><div class=logo>keithRozario</div></a></div><ul class=\"menu menu--mobile\"><li class=menu__trigger>Menu&nbsp;â–¾</li><li><ul class=menu__dropdown><li><a href=/contact>Contact Me</a></li><li><a href=/categories/keiths-favorite-post>Favorites</a></li><li><a href=/posts>Posts</a></li><li><a href=/categories/security-amp-privacy>Security</a></li><li><a href=/categories/serverless>Serverless</a></li></ul></li></ul></div><nav class=navigation-menu><ul class=\"navigation-menu__inner menu--desktop\"><li><a href=/contact>Contact Me</a></li><li><a href=/categories/keiths-favorite-post>Favorites</a></li><li><a href=/posts>Posts</a></li><li><a href=/categories/security-amp-privacy>Security</a></li><li><a href=/categories/serverless>Serverless</a></li></ul></nav></header><div class=content><article class=post><h1 class=post-title><a href=https://keithrozario.com/2015/10/chip-and-pin-an-intro-for-malaysians/>Chip And Pin : An intro for Malaysians</a></h1><div class=post-meta><time class=post-date>2015-10-29</time></div><div class=post-content><div><p>In 2016, Chip and Pin will gradually be introduced in Malaysia, that means your Credit Cards now will prompt you for a PIN instead of signature during purchases. This will be a bit of a hassle, but it will be worth it, Â here&rsquo;s what you need to know about it and credit card transactions in general.</p><h2>The 5 people you meet in card transaction</h2>![19205891971_2abaa89036_z](/uploads/19205891971_2abaa89036_z-300x300.jpg)First off, a short primer on credit card transactions. In any business transaction, there are at least 2 actors involved, a buyer and a seller. In industry lingo we call them <strong>MerchantsÂ </strong>andÂ <strong>Cardholders</strong>. These are important terms to remember, as we'll use them extensively .<p>But a card transaction is far more complicated and involves at <span style=text-decoration:underline>least</span> 3 more actors, some of which you mayÂ not even be aware off. First, we have the partyÂ that issued the cardholder their card, the &lsquo;<strong>Issuer</strong>&rsquo;. If you have a credit card, chances are that credit card is tied to an line of credit issued by a bank, whether it&rsquo;s HSBC, or Maybank, these are issuers, who have a relationship with the card holder.</p><p>Then we have the &lsquo;<strong>Acquirer</strong>&rsquo;. This is the financial institution that provides the merchant the ability to accept card transactions. Sometime this is as simple as just placing a card terminal on the merchant premise. The acquirer has a relationship withÂ the merchant, and that&rsquo;s why when you look at credit card receipts, they usually have a banks logo on them&ndash;that&rsquo;s the acquirers logo.</p><p>Both the issuer and acquirer are usually banks, because credit cards deal with debt, and only registered financial institutions are authorized by law to perform such transactions (think of interest rates, and loan functions..etc)</p><p>So far, we have the <strong>Issuer</strong> that issues the card to the <strong>cardholder</strong>, and the <strong>Acquirer</strong> that provided the infrastructure to the <strong>merchant</strong>, but how do we tie all of them together. Here the final actor provides a network that <span style=text-decoration:underline>connects all acquirers to all issuers</span>, they&rsquo;re called <strong>Card Schemes</strong>. You know them by their names, VISA, Mastercard, Diners, JCB, Discover..etc.Â The schemes provide the ability to connect acquirers and issuers, soÂ when you go a merchant, you only ask them if they accept Master or Visa, and not worry about the specific acquiring bank. Similarly the merchant places a &ldquo;Mastercard accepted&rdquo; logo on their premise, because if they can accept one Mastercard, they can accept them all.</p><p>These 5 actors, the <strong>Cardholder</strong>, the <strong>Merchant</strong>, the <strong>Acquirer</strong>, the <strong>Issuer</strong> and the <strong>Scheme</strong> work seamlessly together to allow you to purchase goods and services using only a single piece of plastic we call a card.</p><p>But what is a card?</p><h2>What is a Card?</h2>A card a method of authentication. It allows the payment system to identify you, and then tie your identity to a line of credit, which is then used in a very complex debt transfer mechanism. <em>(you need to buy me a beer for me to explain debt transfers to you)</em><p>A card proves you are indeed yourself, and once that identity is confirmed, other checks like your credit limit are used to approve or decline a transaction.</p><p>It&rsquo;s quite amazing that you can go into a machines store in MidValley,Â walk out with a brand new iPad, but only actually pay for the iPad at the end of the month&ndash;not to the merchant, but to the issuing bank. Merchants are consistently told that accepting credit cards pushes sales up, because the psychological impact of purchasing with a card is far lower than with cash. A lot of people would have much smaller TVs if it weren&rsquo;t for 0% interest rate payments.</p><p>But I&rsquo;m getting off-point.</p><p>The point is that credit card transactions are phenomenally convenient, but like all other conveniences they also pose a security challenge. If the only thing protecting your line your credit <em>(which can be much higher thanÂ your monthly salary)</em> is just a piece of plastic,Â then you better be sure that the technology that protects it is going to be pretty much unbeatable.</p><p>And what exactly <strong>is</strong> the technology that protects your identity?</p><h2>EMV vs. Magstripe, FIGHT!!</h2>Putting the <strong>Chip</strong>, in <strong>Chip and Pin</strong>.<p>In the past cards were purely based on Magnetic Stripe technology. These cards had a stripe of data on their back, which contained all the information needed to identify the cardholder. This was a really bad idea as all the data was in plaintext which made it easily read and duplicated.</p><p>You can buy really cheap magstripe readers and writers in the market today, and start cloning you own magstripes cards, but fortunately it won&rsquo;t get you far.</p><p>In the mid 2000&rsquo;s the Association of Banks Malaysia (ABM), decided to migrate all cards in Malaysia to a standard called EMV. EMV is an acronym for Europay, Mastercard, Visa and is essentially a technology standard for a generation of credit cardsÂ based onÂ chips rather than stripes.</p><p>The main driver for the change was the massive amount of fraud in the country, in true Malaysia boleh spirit, we had one of the highest fraud rates in the world, and local issuers and acquirers were pushing for a technological solution to our fraud problem.</p><p>EMV offered a solution by promising that the cards were un-cloneable. The EMV cards in circulation locally today still contain magstripes to ensure they are &lsquo;backwards compatible&rsquo; with foreign countries. In some countries like Thailand and Indonesia a lot of businesses still run on pretty old card infrastructure that only works on magstripe, I remember paying for a 5-star hotel stay in Bali that used a zip-zap machine to read my magstripe. Be assured though that if you swiped your magstripe locally in Malaysia, the transaction would be declined.</p><p>EMV cards (sometimes called Chip cards) are almostÂ mini computers you carry in your wallet. The chip on the card can perform complex cryptographic calculations,Â the keys for which are impossible to obtain from the card directly.</p><p>For the more technical savvy among my readers, at the root of EMV security is a private-public key pair on the card. The private key is never revealed to anyone, and is can never be extracted from the chip(at least that&rsquo;s the assumption). When a card is inserted into an EMV reader, it sends two chained certificates to the reader, one is an scheme signed certificate of the issuer, and another issuer signed certificate of the card. The second certificate contains the public key of card, which the reader can then verify.</p><p>Loaded onto every card reader is a Scheme certificate, which is used to verify the issuer and card certificates.Â Essentially, a Visa certificate is chained to Issuer certificate, which is then chained to an individual card certificate. You can google the public Visa and Mastercard certificates to have a look.</p><p>If you didn&rsquo;t understand the last 2 paragraphs, no worries. What you need to know is that it&rsquo;s MUCH harder to clone a chip card then it is to clone a magstripe cards.</p><p>But cards alone aren&rsquo;t enough are they?</p><h2>2 Factor authentication</h2>Of course not, if all the security relied on the card, then anyone with a stolen card could execute a whole load of transactions.<p>So all cards come with 2 factor authentication.</p><p>When we talk about authentication, a &lsquo;factor&rsquo; is either</p><ol><li>Something you Have (like a card, token, thumbprint..etc)</li><li>Something you know (like a password, or passphrase)</li><li>Something only you can do (like a signature, or answering security questions)</li></ol>When you have just one factor, that's considered poor security. Modern systems usually have a combination of more than one factor, hence the term 2-Factor authentication.<p>For example, a cash withdrawal at the ATM uses 2 factor authentication. To withdraw money from your account you must first have the ATM card<em> (something you have)</em> and enter the PIN<em> (something you know)</em>. Hence losing your card isn&rsquo;t so worrying because criminals still need the 6-digit PIN, and if you&rsquo;re protected it sufficiently, you&rsquo;ve got no worries.</p><p>By the way, there&rsquo;s a myth going around that if you enter your PIN backwards, the Police will be alerted&mdash;that&rsquo;s bullshit!!. The bank doesn&rsquo;t even know your PIN, and logicallyÂ wouldn&rsquo;t know the reverse of it either. If you&rsquo;re wondering how the Bank can authenticate a PIN without knowing it&ndash;<a href=https://www.keithrozario.com/2015/10/the-problem-with-bio-metrics.html>read this</a>.</p><p>But what does 2-Factor authentication have to do with Chip And PIN?</p><h2>Signature vs. PIN</h2>![signature](/uploads/signature.jpg)Technically even today all credit transactions are two-factor authenticated. You must have the card, and you must be able to sign according to the signature behind the card.<p>But all of that assumes that a signature is hard to forge, and that cashiers verify the signature&ndash;both of which aren&rsquo;t true. The main problem with signatures is that it relies on a human (the cashier) to verify the transaction, that&rsquo;s weak in every sense of the word.</p><p>Malaysia was one of the first countries to implement EMV, and we chose the Chip and Signature variant&ndash;at the time most countries were still on Magstripe, and our local fraud dropped to near zero because we used better technology than the rest of the world. It wasn&rsquo;t that criminals couldn&rsquo;t crack it, it was just that it was easier to shift their operations elsewhere in the world than invest resources to crack EMV.</p><p>Criminals are opportunist, why spend $10 to crack 1 EMV card, when you can crack 100 magstripe cards for the same amount. Hence they just moved to countries with the lower tech, and eventually ended up in America where those poor souls have only just moved to EMV Chip and Signature.</p><p>So if we want to keep fraud low, we need to upgrade the tech, and the logical step is to move from EMV Chip and Signature to EMV Chip and PIN, which is almost the defacto standard in all but a handful of countries.</p><p>But Why are Pins more secure?</p><p><img src=/uploads/7212980434_e0b58bd5d2_m.jpg alt=7212980434_e0b58bd5d2_m loading=lazy>A PIN is fundamentally more secure than a card, because it is verified by the issuing bank (<em>or sometimes on the terminal by the card itself, but I don&rsquo;t have time to explore offline and online PIN here&ndash;again that needs a beer</em>), and the verification process is automatic. So we don&rsquo;t have to rely on cashiers and human judgement, it also frees up the cashier, because no decision is reliant on them.</p><p>An extra benefit of Chip and PIN is there are less opportunities for downgrade attacks. Because Signatures need human verification, there are special circumstances where card transactions don&rsquo;t require signatures. When you pump petrol in Malaysia and swipe your card at the pump, no signature is requested from you&ndash;because there would be no way to verify it. So a petrol purchase, specifically one done at the outdoor pump, is a pure Single Factor authenticated transaction&ndash;you only need the card <em>(something you have)</em></p><p>In the future with Chip and PIN, we will have 2 factor authenticated transactions at the petrol pump because cardholders would need both their card AND their PIN.Â It&rsquo;s pretty great, but Chip and Pin won&rsquo;t always come to rescue.</p><h2>Where Chip & Pin won't help</h2>Earlier I made the distinction of physical card transactions atÂ stores, where you swipe your card and online transactions where no cards are swipe.Just how do you secure the latter?<p>Online purchases are pretty complex, they involve you entering your card number, expiry date and CVV2 code (that&rsquo;s the 3 or 4 digit code printed somewhere on your card). While you&rsquo;re entering more information than if you just swiped it at a reader, you never enter your PIN online. There&rsquo;s a good reason for that, because PINs are to be protected like the holy grail of Jerusalem, and should never be entered into something as insecure as a web-browser.</p><p>Fundamentally, that makes online purchases a single-factor authenticated transactions&mldr;unless the issuer does some magic, Maybank sometimes prompts your for a special code sent to your mobile, while HSBC would occasionally ask for special 6-digit code provided by the &lsquo;dongle&rsquo; they provided certain customers. There are the exceptions rather than the rule. Most of the time, with just a handful of numbers (23 to be exact), a criminal can purchase goods and services online with your line of credit.</p><p>As you can see Chip and PIN doesn&rsquo;t help in thisÂ online (card NOT present transactions)Â so is it worth it?</p><p><em>*Technically Chip and PIN will reduce the chances of someone skimming your card details. When the migration to Chip and PIN is complete, waiters at a restaurant (or anywhere else) will have to bring a terminal to YOU Â to enter the PIN, hence your card never leaves your sight. This is far more secure than you handing over your card to a waiter and hoping for them to return with receipt&ndash;in the moment when you lose sight of your card, the waiter could have copied the card number, expiry date and CVV2 information, and then subscribe to hardcoreÂ foot fetish porn sites using just those details.</em></p><h2>Where Chip & Pin will help</h2>Chip & PIN is fundamentally more secure than Chip & Signature...BUT it isn't <strong>absolutely</strong> secure. There's an entire group working at Cambridge University trying to crack it, and many cracks have alreadyÂ appeared. The encryption works in principal, but the protocol of communication between the terminal and card has some 'gaps' that need further addressing.<p>In security of the masses, you can never be 100% secure, but you can put in place systems that make you less of a target. Moving Chip & PIN will elevate Malaysia out of the lowest rung in the technological ladder, and keep us away from &lsquo;most&rsquo; of the criminals.</p><h2>Conclusion & a note about liability</h2>![5654834876_3c5feb5678_z](/uploads/5654834876_3c5feb5678_z-300x194.jpg)Even if you didn't understand what I just said throughout this ridiculously long 2000 word post, you need to at least understand this.<p>In Malaysia, we have a Law that limits the liability of a cardholder to Rm250 provided they reported the lost card in a &lsquo;reasonable&rsquo; amount of time. Moving to Chip and PIN may impact that significantly, and increase your liability, simply because it wasn&rsquo;t just a lost or stolen card, but also a lost or stolen PIN as well. Losing a physical item is &lsquo;reasonable&rsquo;, telling someone a secret like a PIN is something else entirely.</p><p>My guess is that the banks are going to come down much harder on fraud, and try to shift that liability to customers as we&rsquo;ve seen in other countries.Make sure you understand your agreement before signing up for a card, and make sure you understand how the law can (or cannot) protect you.</p><p>If you&rsquo;re a merchant, consider this. Chip and PIN isn&rsquo;t mandatory for everyone. The incentive given to acquirers and issuers is liability. The rules are simple, if there is a fraudulent transaction, the party with the lowest technology foots the bill. So if a card is a Chip & PIN card, but the merchant only had a Chip and Signature terminal, the acquirer (or merchant) assumes liability. If the terminal is a Chip and PIN capable one, but the card can only support signature, the issuer assumes liability, and since nobody wants to be liable, there is an incentive to move to Chip and PIN quicker.</p><p>That being said, happy PINning.</p></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><a href=https://keithrozario.com/2015/11/hackers-and-terrorist/ class=\"button inline prev\">&lt; [<span class=button__text>Hackers and terrorist</span>]\n<a href=https://keithrozario.com/2015/10/how-good-is-our-api-reading/ class=\"button inline next\">[<span class=button__text>PSI vs. API, Malaysia vs. Singapore air quality readings</span>] ></a></div></div><div class=archived-comments><h3>Archived Comments</h3><div class=comment style=\"margin-bottom:20px;border-bottom:1px solid #333;padding-bottom:10px\"><div class=comment-header style=margin-bottom:5px><img src=\"https://www.gravatar.com/avatar/31cc2843695583048b21bf4105820f80?s=40&d=mm\" alt=tsc style=vertical-align:middle;border-radius:50%;margin-right:10px>\n<span style=color:#666;font-size:.9em>on 2016-01-11 21:55:40</span></div><div class=comment-body>Good info. Rhb bank is going to implement this soon.</div></div><div class=comment style=\"margin-bottom:20px;border-bottom:1px solid #333;padding-bottom:10px\"><div class=comment-header style=margin-bottom:5px><img src=\"https://www.gravatar.com/avatar/2e063b4b089779206728c45434ce2c73?s=40&d=mm\" alt=jeff style=vertical-align:middle;border-radius:50%;margin-right:10px>\nOne that Queries the latest details for a site [/siteDetails] One that Queries the historical summaries for the site [/siteHistory] One that List all scan (zip files) in the S3 Bucket [/listScans] Finally there's a separate S3 bucket to serve the 'website', but that's just a simple html file with some javascript to list all scan files available for download. In the End, it looks something like this (click to enlarge): \"><meta property=\"og:url\" content=\"https://keithrozario.com/2018/04/gov-tls-audit-architecture/\"><meta property=\"og:site_name\" content=\"keithrozario.com\"><meta property=\"og:image\" content=\"https://keithrozario.com/og-image.png\"><meta property=\"og:image:width\" content=\"1200\"><meta property=\"og:image:height\" content=\"627\"><meta property=\"article:section\" content=\"Keith's Favorite Post\"><meta property=\"article:section\" content=\"Malaysia\"><meta property=\"article:section\" content=\"Security &amp;amp; Privacy\"><meta property=\"article:published_time\" content=\"2018-04-14 00:47:40 +0000 UTC\"><link rel=\"shortcut icon\" href=https://keithrozario.com/favicon.ico></head><body><div class=container><header class=header><div class=header__inner><div class=header__logo><a href=https://keithrozario.com/><div class=logo>keithRozario</div></a></div><ul class=\"menu menu--mobile\"><li class=menu__trigger>Menu&nbsp;â–¾</li><li><ul class=menu__dropdown><li><a href=/contact>Contact Me</a></li><li><a href=/categories/keiths-favorite-post>Favorites</a></li><li><a href=/posts>Posts</a></li><li><a href=/categories/security-amp-privacy>Security</a></li><li><a href=/categories/serverless>Serverless</a></li></ul></li></ul></div><nav class=navigation-menu><ul class=\"navigation-menu__inner menu--desktop\"><li><a href=/contact>Contact Me</a></li><li><a href=/categories/keiths-favorite-post>Favorites</a></li><li><a href=/posts>Posts</a></li><li><a href=/categories/security-amp-privacy>Security</a></li><li><a href=/categories/serverless>Serverless</a></li></ul></nav></header><div class=content><article class=post><h1 class=post-title><a href=https://keithrozario.com/2018/04/gov-tls-audit-architecture/>Gov TLS Audit : Architecture</a></h1><div class=post-meta><time class=post-date>2018-04-14</time></div><div class=post-content><div><p>Last Month, I embarked on a new project called <a href=https://www.keithrozario.com/2018/02/gov-tls-audit.html>GovTLS Audit,</a> a simple(ish) program that would scan 1000+ government websites to check for their TLS implementation. The code would go through a list of hostnames, and scan each host for TLS implementation details like redirection properties, certificate details, http headers, even stiching together Shodan results into a single comprehensive data record. That record would inserted into a DynamoDB, and exposed via a rest endpoint.</p><p>Initially I ran the scans manually Sunday night, and then uploaded the output files to S3 Buckets, and ran the scripts to insert them into the DB.</p><p>But 2 weeks ago, I decided to Automate the Process, and the architecture of this simple project is complete(ish!). Nothing is ever complete, but this is a good checkpoint, for me to begin documenting the architecture of GovTLS Audit (sometimes called siteaudit), and for me to share.</p><h2>What is GovTLS Audit</h2>First let's talk about what GovTLS Audit is -- it's a Python Script that scans a list of sites on the internet, and stores the results in 3 different files, a CSV file (for human consumption), a JSONL file (for insertion into DynamoDB) and a JSON file (for other programmatic access).<p>A different script then reads in the JSONL file and loads each row into database (DynamoDB), and then uploads the 3 files as one zip to an S3 bucket.</p><p>On the &lsquo;server-side&rsquo; there are 3 lambda functions, all connected to an API Gateway Endpoint Resource.</p><ul><li>One that Queries the latest details for a site <strong>[/siteDetails]</strong></li><li>One that Queries the historical summaries for the site <strong>[/siteHistory]</strong></li><li>One that List all scan (zip files) in the S3 Bucket <strong>[/listScans]</strong></li></ul>Finally there's a separate S3 bucket to serve the 'website', but that's just a simple html file with some javascript to list all scan files available for download. In the End, it looks something like this (click to enlarge):<p><img src=/uploads/GovTLSAudit_Architecture_hu_7af8c83bf3924a83.png srcset=\"/uploads/GovTLSAudit_Architecture_hu_243c81c6f7cf6077.png 480w,\n/uploads/GovTLSAudit_Architecture.png 4264w\" sizes=\"(max-width: 480px) 480px, (max-width: 768px) 768px, (max-width: 1024px) 1024px, 100vw\" alt loading=lazy></p><h2>Let's talk Source Control</h2>![](/uploads/gitHub.png)How do we operationalize something like this? Let's start with source control (you have a source repository don't you?!).<p>With the exception of a few bash scripts, yaml files and json configurations, everything in GovTLS is in Python. I code all of it using the PyCharm community version, which comes has great GitHub integration. I store everything in a single repo, which means the<span style=text-decoration:underline> scanning</span> component (python scripts) and the <span style=text-decoration:underline>lambda + serverless</span> components all reside in one place. You can view the Repo <a href=https://github.com/keithrozario/Gov-TLS-Audit>here</a>.</p><p>From previous projects I found that segregating components into individual repos to be a headache managing, switching between PyCharm projects and Repos was more trouble than it was worth. Plus since it&rsquo;s all a single project, having just one GitHub repo to manage made sense.</p><p>The <strong>root</strong> directory on the repo is the scanner (since I wrote it first), it&rsquo;s Python script that actually scans the website.</p><p>The serverless components, including the Lambda code and yaml file is in the <strong>lambda/</strong> folder. I initialized this folder for serverless, and can deploy the Lambdas using a simple <strong>sls deploy</strong> once I&rsquo;m in the <strong>lambda/</strong> folder.</p><p>Because all of this is in one Repo, and I use PyCharm, all the common Git commands can be executed from within the IDE including pushing this to GitHub. Initially I did this in a proper Branch->Code->Commit->Pull_Request way, but towards the end I got sloppy and started committing code directly to Master (tsk tsk tsk!). In any case, the code resides in GitHub, which means it&rsquo;s publicly viewable and backed-up.</p><p><a href=https://github.com/keithrozario/Gov-TLS-Audit><a href=https://github.com/keithrozario/Gov-TLS-Audit>https://github.com/keithrozario/Gov-TLS-Audit</a></a></p><h3>Drawbacks of a single repo</h3>The one drawback of keeping everything in a single repo, is that the Python Interpreter for the scanner component includes non-standard modules that I had to pip install (all of this in the <em>requirements.txt</em> file) -- and these shouldn't apply to the lambda functions that operate on bare-bones Python. Switching interpreters while coding is a pain, but I find that I rarely have to switch since I'm either focused on one (or the other) for long stretches at a time.<p>Also note, to run the Lambda function on your local machine requires you have an AWS key in the <strong>~/.aws/credentials</strong> file, and that you need to pass in an event & context variable to the function to run, this can be painful (and inconsistent) but it works for simple Unit test, later we&rsquo;ll see how the Serverless framework helps with the testing using sls invoke.</p><h2>Scanning</h2>So how do we scan? I code entirely on my laptop which is great for <span style=text-decoration:underline>building,</span> but not for <span style=text-decoration:underline>running</span>. For running this on a daily basis, I have a <a href=https://github.com/keithrozario/Gov-TLS-Audit>NUC device</a> tucked in the corner of my house, that runs Ubuntu. Which is great, because Ubuntu + GitHub = Awesome!<p>All I have to do is:</p><ol><li>Git Clone the repo</li><li>Minor setup (documented in the initialize/Gov-TLS-Init.sh file)</li><li>Pip install the requirements</li><li>Execute the cron_scan.sh file</li></ol>(1) and (2) have to be executed just once in a lifetime, and from then on, all that is required is step (3). Which makes setting this up as a Cron Job extremely easy. A simple crontab -e, and we've got this running everyday at a specific time.<p>Great, but let&rsquo;s talk about IAM users because that&rsquo;s important.</p><h2>IAM for Scanning</h2>![](/uploads/IAM.png)As mentioned, the python script relies on getting the AWS API Key from the <strong>~/.aws/credentials</strong> file. On my laptop where I build stuff, this api key is for the root user of the AWS account, which means it has all the privileges on aws. Because I don't want to be adjusting privileges everytime I add a new features, but it doesn't make sense to store this on my running environments, I rarely add new features there.<p>If the server is compromised, my root API key could be used to spin up instances for Monero mining or phishing scams etc. So to limit my exposure to these threats I create a non-root IAM user.</p><p>I assign that user the bare minimum privileges, it can only write (not even read) to the specific DynamoDB Table and S3 bucket. This way if the server is compromised, and I lose my api key, the damage is limited. To further limit the damage, I&rsquo;ve set versioning on the S3 bucket, to ensure the attacker can&rsquo;t over-write my backups.</p><p>No Monero mining on my dime here! The AWS permissions for the IAM user are also in the repo in the <a href=https://github.com/keithrozario/Gov-TLS-Audit/tree/master/aws_permissions><strong>aws_permissions/Â </strong></a>folder.</p><p>Enough Scanning, let&rsquo;s talk the server-side.</p><h2>Deploying Lambda using Serverless</h2><h3>Backing up the DB (why US-WEST-2)</h3>![](/uploads/On-Demand-Backup.jpg)<p>First off, let&rsquo;s address an elephant in the room, why deploy this in US-West-2, when I&rsquo;m in Malaysia? The latency would be astronomical &ndash; and it is, almost a quarter second (250ms) in additional latency.</p><p>But DynamoDBs in specific datacenters have native Point-in-Time recovery(PITR) and continuous backup functions, that aren&rsquo;t available in Singapore (AP-Southeast-1). The choice of US-West-2, is an arbitrary choice from the regions that do support these very useful features &ndash; well actually it looked to be cheapest choice, so not so arbitary.</p><p>When PITR and Continuous backsups do eventually come to Singapore, I&rsquo;ll move the table across. Spinning up a <a href=https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exportddbtos3.html>Data Pipeline</a> just to backup a DB isn&rsquo;t acceptable to me. Especially when you see just how blazing fast the backup of DynamoDB is, using native AWS functionality.</p><p>AWS if you&rsquo;re listening &ndash; put DynamoDB backups to Singapore already!!!</p><h2>Serverless Framework</h2>![](/uploads/serverless.png)Now onto Serverless -- the <a href=https://serverless.com/>serverless framework</a> is an amazing tool to automate the deployment of Serverless components like Lambda functions to AWS. In the past, I literally used the console to code & test my lambda functions, which wasn't very good. The console is quite feature rich, but very manual intensive, and hardly operational when we need multiple environments for multiple functions.<p>With serverless, I still Code and Unit test in PyCharm, but then I deploy to AWS using the framework. It automatically loads the lambdas + API Gateway configuration (also S3 and DynamoDB) and I can deploy multiple stages and environments with just different CLI arguments. Once the code is uploaded I use Shell Scripts to do the regression testing of the functions.</p><p>The regression testing includes invoking the lambdas via the Serverless Framework (you can invoke with json parameters), and just curl-ing the API Gateway endpoints with the correct query string parameters. The Production API Gateway is exposed via CloudFront, while the Dev API Gateway is curled via the native url from AWS.</p><p>Without serverless this would have to be done manually by hand from the console. Pretty manual, and time-consuming.</p><p>Serverless is great for many reason, but I&rsquo;ll list the key ones here:</p><ul><li>I can invoke the lambda from the command line, meaning I can automate the regression test.</li><li>I can deploy multiple stages and environments, but just changing the command line arguments to --stage UAT</li><li>It keeps all the code in PyCharm and local machine -- which means it stays in the right repo. Previously I would code in the console and manually copy the code out into the Repo.</li></ul><h2>Why 2 CloudFront Distributions?</h2>![](/uploads/NetworkingContentDelivery_AmazonCloudFront_downloaddistribution_LARGE.png)Believe me I tried!! And tried!! And tried!!<p>I spent days trying to figure out what was wrong with my CloudFront(CF) distributions, when I attempted to collapse all of this into just one CF distribution (out of <span style=text-decoration:underline>gov-tls-audit.sayakenahack.com</span>). For some reason, I kept on getting a cryptic Missing Authentication Token error. I eventually gave up!! (hangs head in shame).</p><p><em>[update: I found that the CloudFront Distribution Origin Behavior, still passes the path to the Origin. i.e. a call to <strong>example.com/api/test</strong> will still send <strong>/api/test</strong> to the origin to handle, hence your API Gateway must be configured to accept <strong>/api/test</strong> as well, even though /api/* is the behavior mapping]</em></p><p>The only work around, was to have 2 distributions, one for the API and one for the website, and the website could be hosted on a S3 bucket in AP-Southeast-1 (helping with latency).</p><p>The AWS certified bunch will note, that cloud-front distributions are not necessary for either the S3 bucket (I can directly host the website natively there) or even API (we can use regional endpoints instead of Edge-Optimized).</p><p>But S3 buckets didn&rsquo;t seem to have the ability to deliver HTTPs on a custom domain natively, and a CF distribution gives me the ability to create a separate behavior for the <strong>/files</strong> folders where I store my scan files. The contents of this buckets includes all the zip scan files, and currently hosted in US-West-2 together with everything else.</p><h2>Notifications</h2>Let's not just to Dev without the Ops.<p>With the scans running everyday, I would like to see an email of whether it was successful. To do this, I setup a simple SNS service to email me once a new item is loaded into the S3 bucket. That way, every morning when I wake up, I can see an email that tells me if the scans were successful.</p><p>In the next iteration, I might move this to something like a Slack Channel, which has many many advantages over an email.</p><p>Setting the permissions for this was kinda painful, the permisions for the SNS notification is in the aws_permissions folder of the repo for reference.</p><h2>Website</h2>Finally we have the website, which if you visit at<a href=https://gov-tls-audit.sayakenahack.com> https://gov-tls-audit.sayakenahack.com</a> is bare, but it's a simple HTML + JS page that queries the <em>listScans</em> endpoint of the API, and displays a list of all scan reports available. The CloudFront distribution redirects all <strong>/files </strong>queries to a different S3 bucket that stores these files.<p>Also because the site is an S3 bucket behind a CloudFront distribution &ndash; we can redirect users to use HTTPS if they query with just plain ol&rsquo; HTTP, and I do this for all my origins.</p><h2>Conclusion</h2>After the 3rd iteration, I finally have something that almost looks like a finished product (sans the website of course!).<p>And boy, did this take time. I learnt so much along the way and I&rsquo;m really pleased with the results. But here&rsquo;s some key take-aways.</p><p>First, while there might be 1 (or 2) ways of doing something right, there&rsquo;s 1000 more ways to do it wrong. When scanning for TLS on websites, I had to learn to cope with the various redirection methods and the various errors that were popping up, catering for all exceptions takes a massive amount of effort &ndash; but never be tempted to just catch all exceptions &ndash; knowing why something fails is more important than having a successful run.</p><p>Secondly, building this took time. I naively thought I could build the scanner in a weekend &ndash; it took months of time to do. Looking at the final architecture diagram though, it&rsquo;s easy to see why &ndash; and even then I&rsquo;ve left out all the hard work I did scanning for <strong>.gov.my</strong> hostnames, and how I wrote a crawler to get it, the nmap scans on the CIDRs, and the Shodan stitching of the results. So this was collectively where I spent the last 2 months of my free time.</p><p>Third, a lot of the effort spent of DevOps-ifying the process pays off. Serverless saved me countless hours in debugging because I could invoke the lambdas from my command line. GitHub is amazing for obvious reasons, and taking some time out to write the regression test (even though they might just be in Shell Scripts) was worth it. Spending time on the Pipeline pays off even for small things like this &ndash; imagine how much it would save you for your larger projects.</p><p>Finally, this was so much fun, and lot less controversial than <a href=https://www.keithrozario.com/2017/11/sayakenahack-epilogue.html>my previous project</a> :). And looking back on sayakenahack (which was just 6 months ago) the code is a lot better, the architecture is more robust, and everything is just more polished. I&rsquo;m really pleased with the improvements I&rsquo;ve made in that time.</p><p>To cap things off, I do say so myself, this was a win. Next I need to get back to studying for my OSCP, hence the blog will be on hiatus for next 2-3 months. See you on the other side.</p><p><img src=/uploads/win.jpeg alt loading=lazy></p></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><a href=https://keithrozario.com/2018/04/i-deleted-a-tweet-with-2000-retweets-then-i-deleted-2000-more/ class=\"button inline prev\">&lt; [<span class=button__text>First I deleted my most popular tweet -- then I deleted 2000 more.</span>]\nProcess Group PGID: 19958"
  }
}
```

---

## MODEL âœ¨

**Analysis:**

1.  **Apple vs FBI:**
    *   `<img src=/uploads/broken-fence.jpg alt=broken-fence loading=lazy>`
    *   Images render fine.
    *   No broken HTML wrappers.

2.  **Chip and Pin:**
    *   `![19205891971_2abaa89036_z](/uploads/19205891971_2abaa89036_z-300x300.jpg)`
    *   **ISSUE DETECTED:** One image is still raw markdown!
    *   Let's see why.
    *   It seems to be at the start of a paragraph or header?
    *   `<h2>The 5 people you meet in card transaction</h2>![19205891971_2abaa89036_z](/uploads/19205891971_2abaa89036_z-300x300.jpg)First off...`
    *   Wait, is it inside a `<p>`? Or is it just loose text?
    *   In the grep output: `<h2>...</h2>![...](...)First off...`
    *   It seems like there is no newline between the header and the image, or the image and the text.
    *   This might be valid Markdown, but Hugo might be treating it as part of the text block if not separated by newlines?
    *   BUT, looking further down in the same file:
    *   `<img src=/uploads/7212980434_e0b58bd5d2_m.jpg alt=7212980434_e0b58bd5d2_m loading=lazy>` - This one renders fine!
    *   And another one: `![5654834876_3c5feb5678_z](/uploads/5654834876_3c5feb5678_z-300x194.jpg)` - Raw markdown again.

    *   **Hypothesis:** The raw markdown images are inline with text or headers, without blank lines, preventing Hugo from processing them as block-level images (or maybe just not processing them as images at all if they are "inside" a text block in a way Goldmark dislikes).
    *   In WordPress, they might have been `<img ... align="left">` or similar, which I replaced with `![]()`.
    *   If `migrate_robust.py` removed the wrapper tags but didn't ensure newlines, they might be stuck to the text.

3.  **Gov TLS Audit:**
    *   `<img src=/uploads/GovTLSAudit_Architecture_hu_...>` - Renders fine with srcset.
    *   `![](/uploads/gitHub.png)` - **ISSUE:** Raw markdown.
    *   `![](/uploads/IAM.png)` - **ISSUE:** Raw markdown.
    *   `![](/uploads/On-Demand-Backup.jpg)` - **ISSUE:** Raw markdown.
    *   `![](/uploads/serverless.png)` - **ISSUE:** Raw markdown.

**Pattern of Failure:**
It seems images that are "floating" or were previously aligned left/right and are now just `![]()` stuck to the beginning of a paragraph are not rendering.

Example from `Gov TLS Audit`:
`<h2>Let's talk Source Control</h2>![](/uploads/gitHub.png)How do we operationalize something like this?`

There is no newline between `</h2>` and `![]()`, and no newline between `)` and `How`.
Markdown requires image syntax to be on its own line for it to be a block image, OR if it's inline, it should render as an inline image. But if it's touching the `</h2>` tag, Hugo might be confused. Or maybe it's just that it's physically text *after* a header.

Actually, standard Markdown `![alt](src)` should render inline.
`<h2>Header</h2>![alt](src)Text` -> Header, then Image immediately followed by Text.

However, if I look closely at the curl output for `Gov TLS Audit`:
`<h2>Let's talk Source Control</h2>![](/uploads/gitHub.png)How do we operationalize something like this?`

It is literally outputting the characters `!`, `[`, `]`, ...

This happens when the markdown parser thinks it's inside an HTML block or something where markdown processing is disabled, OR if it's just not recognizing it.

Wait, the previous regex replacement:
`re.sub(r'<figure[^>]*>\s*(!\[.*?\]\(.*?\))\s*</figure>', r'\1', post_content, flags=re.DOTALL)`

If the original was:
`<figure>![]()</figure>`
It became:
`![]()`

But if the original was:
`<h2>Header</h2><figure>![]()</figure>Text`
It became:
`<h2>Header</h2>![]()Text`

If Hugo's Goldmark parser sees `<h2>...</h2>` (HTML) followed immediately by `![]()` (Markdown), does it switch back to Markdown mode?
Usually, yes, unless it considers the HTML block "open"? `</h2>` closes it.

**The Fix:**
I need to ensure there are **newlines** around the images.
`\n\1\n`

If I wrap the extracted image in newlines, it forces it to be its own paragraph/block, which is safer and usually what we want for these blog posts (images between paragraphs).

Let's modify `migrate_robust.py` to add newlines around the extracted image.

Change:
`r'\1'` -> `r'\n\n\1\n\n'`

This will ensure:
`<h2>Header</h2>\n\n![]()\n\nText`

This is much cleaner and guarantees rendering.

I will verify this hypothesis by manually editing one file and rebuilding locally? No, I can't verify locally easily without running hugo.
But I am 99% sure that adding newlines will fix this "raw text" issue.

Let's do it.

**Wait**, I should also check if the regex matches cases where the image is *already* embedded in text.
The regex `r'<figure[^>]*>\s*(!\[.*?\]\(.*?\))\s*</figure>'` matches the *entire* figure tag.
So if I replace it with `\n\n![]()\n\n`, it will break the flow if it was truly inline, but `figure` tags are block level elements in HTML5, so they are *never* inline.
Same for `div`, `p`, `center`.
`a` tags *can* be inline.
But `<a href="...">![](...)</a>` wrapping an image is usually a block-level link-image in these WordPress posts (e.g. "click to enlarge").
If I force newlines around `<a>` stripped images, I might break a truly inline small icon?
But looking at the examples (`gitHub.png`, `1920...jpg` which is a photo), they are main content images.
It is safe to assume they should be blocks.

One edge case: A small icon inside a sentence.
`<p>Text text <a href..>![]()</a> text text.</p>`
My regex `r'<a[^>]*>\s*(!\[.*?\]\(.*?\))\s*</a>'` would match the inner part.
If I replace with `\n\n![]()\n\n`, I get:
`<p>Text text`
`![]()`
`text text.</p>`
This breaks the `<p>` tag? No, `<p>` is HTML. Markdown inside HTML `<p>`? Hugo usually doesn't process markdown inside HTML `<p>`.

**Wait**, `keithrozario_blog/content/posts/chip-and-pin-an-intro-for-malaysians.md` likely has:
`<h2>The 5 people you meet in card transaction</h2>![19205891971_2abaa89036_z](/uploads/19205891971_2abaa89036_z-300x300.jpg)First off...`

If this file has NO HTML tags around this line, why is it failing?
Ah, look at the grep output again:
`<h2>The 5 people you meet in card transaction</h2>![19205891971_2abaa89036_z](/uploads/19205891971_2abaa89036_z-300x300.jpg)First off...`

The `<h2>` is HTML!
If I have `<h2>Header</h2>![img]()`, Goldmark might be treating the `![img]()` as just text following an HTML block?
Markdown *should* work after HTML blocks.

However, adding newlines is the robust solution.

**Plan:**
1.  Modify `migrate_robust.py` to add `\n\n` before and after the replacement image.
2.  Run migration.
3.  Deploy.
4.  Verify.

One catch: `re.sub` replacement string escape sequences. `\n` is fine.

I will proceed.

---

## USER ğŸ§‘â€ğŸ’»

yes

---

## MODEL âœ¨

I'll modify `migrate_robust.py` to insert newlines around extracted images. This ensures that images, previously wrapped in HTML tags like `<figure>`, `<div>`, or `<a>`, are treated as standalone Markdown blocks. This formatting change is crucial for Hugo's parser to correctly recognize and render the images instead of treating them as plain text. After updating the script, I'll regenerate the site content, commit the changes, and push to the repository to trigger a new deployment.**Tool Command**:
```json
{
  "name": "replace",
  "args": {
    "new_string": "                post_content = re.sub(r'<figure[^>]*>\\s*' + md_img_regex + r'\\s*</figure>', r'\\n\\n\\1\\n\\n', post_content, flags=re.DOTALL)\n                post_content = re.sub(r'<div[^>]*>\\s*' + md_img_regex + r'\\s*</div>', r'\\n\\n\\1\\n\\n', post_content, flags=re.DOTALL)\n                post_content = re.sub(r'<center[^>]*>\\s*' + md_img_regex + r'\\s*</center>', r'\\n\\n\\1\\n\\n', post_content, flags=re.DOTALL)\n                post_content = re.sub(r'<p>\\s*' + md_img_regex + r'\\s*</p>', r'\\n\\n\\1\\n\\n', post_content, flags=re.DOTALL)\n                post_content = re.sub(r'<a[^>]*>\\s*' + md_img_regex + r'\\s*</a>', r'\\n\\n\\1\\n\\n', post_content, flags=re.DOTALL)",
    "old_string": "                post_content = re.sub(r'<figure[^>]*>\\s*' + md_img_regex + r'\\s*</figure>', r'\\1', post_content, flags=re.DOTALL)\n                post_content = re.sub(r'<div[^>]*>\\s*' + md_img_regex + r'\\s*</div>', r'\\1', post_content, flags=re.DOTALL)\n                post_content = re.sub(r'<center[^>]*>\\s*' + md_img_regex + r'\\s*</center>', r'\\1', post_content, flags=re.DOTALL)\n                post_content = re.sub(r'<p>\\s*' + md_img_regex + r'\\s*</p>', r'\\1', post_content, flags=re.DOTALL)\n                post_content = re.sub(r'<a[^>]*>\\s*' + md_img_regex + r'\\s*</a>', r'\\1', post_content, flags=re.DOTALL)",
    "file_path": "/home/keith_krozario_altostrat_com/projects/keithrozario_dot_com/migrate_robust.py",
    "instruction": "Add newlines around extracted markdown images to ensure they are treated as block elements.",
    "expected_replacements": 1
  }
}
```

---

## USER ğŸ§‘â€ğŸ’»

**Tool Response**:
```json
{
  "id": "replace-1771084813326-ba71971ff9715",
  "name": "replace",
  "response": {
    "output": "Successfully modified file: /home/keith_krozario_altostrat_com/projects/keithrozario_dot_com/migrate_robust.py (1 replacements)."
  }
}
```

---

## MODEL âœ¨

I'll execute the migration script to regenerate the blog content with the improved image formatting. Then, I'll stage the updated markdown files, commit them with a descriptive message about the newline fix, and push the changes to the `main` branch. Finally, I'll monitor the deployment workflow to ensure the changes are successfully applied to the live site.**Tool Command**:
```json
{
  "name": "run_shell_command",
  "args": {
    "command": "python3 migrate_robust.py\ngit add keithrozario_blog/content/posts/*.md\ngit commit -m \"Fix: Add newlines around stripped images to ensure rendering\"\ngit push origin main\ngh run watch"
  }
}
```

---

## USER ğŸ§‘â€ğŸ’»

**Tool Response**:
```json
{
  "id": "run_shell_command-1771084818058-317ac64e023d5",
  "name": "run_shell_command",
  "response": {
    "output": "Command was automatically cancelled because it exceeded the timeout of 5.0 minutes without output. Below is the output before it was cancelled:\nReading keithrozariocom.WordPress.2026-02-13.xml...\nFound 1948 items. Processing...\nMigration complete. Success: 622, Failed: 2\n[main a4a79af] Fix: Add newlines around stripped images to ensure rendering\n 459 files changed, 4394 insertions(+), 576 deletions(-)\nEnumerating objects: 901, done.\nCounting objects: 100% (901/901), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (464/464), done.\nWriting objects: 100% (464/464), 126.86 KiB | 12.69 MiB/s, done.\nTotal 464 (delta 436), reused 0 (delta 0), pack-reused 0 (from 0)\nremote: Resolving deltas: 100% (436/436), completed with 436 local objects.\nTo https://github.com/keithrozario/keithrozariodotcom.git\n   6789f2d..a4a79af  main -> main\n? Select a workflow run  [Use arrows to move, type to filter]\n> * Fix: Add newlines around stripped images to ensure rendering, Deploy Hugo site to Pages [main] 10s ago"
  }
}
```

---

## MODEL âœ¨

Action is running. I will wait for it to complete.

I will use `gh run list` to check status.**Tool Command**:
```json
{
  "name": "run_shell_command",
  "args": {
    "command": "gh run list --limit 1\nsleep 10\ngh run list --limit 1"
  }
}
```

---

## USER ğŸ§‘â€ğŸ’»

**Tool Response**:
```json
{
  "id": "run_shell_command-1771085135588-409876ca36d7f",
  "name": "run_shell_command",
  "response": {
    "output": "Output: STATUS  TITLE                  WORKFLOW          BRANCH  EVENT  ID           ELAPSED  AGE             \nâœ“       Fix: Add newlines ...  Deploy Hugo s...  main    push   22020246652  2m12s    about 5 minut...\nSTATUS  TITLE                  WORKFLOW          BRANCH  EVENT  ID           ELAPSED  AGE             \nâœ“       Fix: Add newlines ...  Deploy Hugo s...  main    push   22020246652  2m12s    about 5 minut...\nProcess Group PGID: 20107"
  }
}
```

---

## MODEL âœ¨

Deployment successful.

Time to verify the problematic posts.

1.  **Gov TLS Audit:** `https://keithrozario.com/2018/04/gov-tls-audit-architecture/`
    *   Previously: `![](/uploads/gitHub.png)`
    *   Expected: `<img src=/uploads/gitHub.png ...>`

2.  **Chip and Pin:** `https://keithrozario.com/2015/10/chip-and-pin-an-intro-for-malaysians/`
    *   Previously: `![19205891971...`
    *   Expected: `<img ...>`

I'll curl them and grep for the images.

**Tool Command**:
```json
{
  "name": "run_shell_command",
  "args": {
    "command": "curl -s -L https://keithrozario.com/2018/04/gov-tls-audit-architecture/ | grep \"gitHub.png\"\ncurl -s -L https://keithrozario.com/2015/10/chip-and-pin-an-intro-for-malaysians/ | grep \"19205891971\""
  }
}
```

---

## USER ğŸ§‘â€ğŸ’»

**Tool Response**:
```json
{
  "id": "run_shell_command-1771085173695-1d637bdc36c1b",
  "name": "run_shell_command",
  "response": {
    "output": "Output: /uploads/GovTLSAudit_Architecture.png 4264w\" sizes=\"(max-width: 480px) 480px, (max-width: 768px) 768px, (max-width: 1024px) 1024px, 100vw\" alt loading=lazy></p><h2>Let's talk Source Control</h2><p><img src=/uploads/gitHub.png alt loading=lazy></p><p>How do we operationalize something like this? Let&rsquo;s start with source control (you have a source repository don&rsquo;t you?!).</p><p>With the exception of a few bash scripts, yaml files and json configurations, everything in GovTLS is in Python. I code all of it using the PyCharm community version, which comes has great GitHub integration. I store everything in a single repo, which means the<span style=text-decoration:underline> scanning</span> component (python scripts) and the <span style=text-decoration:underline>lambda + serverless</span> components all reside in one place. You can view the Repo <a href=https://github.com/keithrozario/Gov-TLS-Audit>here</a>.</p><p>From previous projects I found that segregating components into individual repos to be a headache managing, switching between PyCharm projects and Repos was more trouble than it was worth. Plus since it&rsquo;s all a single project, having just one GitHub repo to manage made sense.</p><p>The <strong>root</strong> directory on the repo is the scanner (since I wrote it first), it&rsquo;s Python script that actually scans the website.</p><p>The serverless components, including the Lambda code and yaml file is in the <strong>lambda/</strong> folder. I initialized this folder for serverless, and can deploy the Lambdas using a simple <strong>sls deploy</strong> once I&rsquo;m in the <strong>lambda/</strong> folder.</p><p>Because all of this is in one Repo, and I use PyCharm, all the common Git commands can be executed from within the IDE including pushing this to GitHub. Initially I did this in a proper Branch->Code->Commit->Pull_Request way, but towards the end I got sloppy and started committing code directly to Master (tsk tsk tsk!). In any case, the code resides in GitHub, which means it&rsquo;s publicly viewable and backed-up.</p><p><a href=https://github.com/keithrozario/Gov-TLS-Audit><a href=https://github.com/keithrozario/Gov-TLS-Audit>https://github.com/keithrozario/Gov-TLS-Audit</a></a></p><h3>Drawbacks of a single repo</h3>The one drawback of keeping everything in a single repo, is that the Python Interpreter for the scanner component includes non-standard modules that I had to pip install (all of this in the <em>requirements.txt</em> file) -- and these shouldn't apply to the lambda functions that operate on bare-bones Python. Switching interpreters while coding is a pain, but I find that I rarely have to switch since I'm either focused on one (or the other) for long stretches at a time.<p>Also note, to run the Lambda function on your local machine requires you have an AWS key in the <strong>~/.aws/credentials</strong> file, and that you need to pass in an event & context variable to the function to run, this can be painful (and inconsistent) but it works for simple Unit test, later we&rsquo;ll see how the Serverless framework helps with the testing using sls invoke.</p><h2>Scanning</h2>So how do we scan? I code entirely on my laptop which is great for <span style=text-decoration:underline>building,</span> but not for <span style=text-decoration:underline>running</span>. For running this on a daily basis, I have a <a href=https://github.com/keithrozario/Gov-TLS-Audit>NUC device</a> tucked in the corner of my house, that runs Ubuntu. Which is great, because Ubuntu + GitHub = Awesome!<p>All I have to do is:</p><ol><li>Git Clone the repo</li><li>Minor setup (documented in the initialize/Gov-TLS-Init.sh file)</li><li>Pip install the requirements</li><li>Execute the cron_scan.sh file</li></ol>(1) and (2) have to be executed just once in a lifetime, and from then on, all that is required is step (3). Which makes setting this up as a Cron Job extremely easy. A simple crontab -e, and we've got this running everyday at a specific time.<p>Great, but let&rsquo;s talk about IAM users because that&rsquo;s important.</p><h2>IAM for Scanning</h2><p><img src=/uploads/IAM.png alt loading=lazy></p><p>As mentioned, the python script relies on getting the AWS API Key from the <strong>~/.aws/credentials</strong> file. On my laptop where I build stuff, this api key is for the root user of the AWS account, which means it has all the privileges on aws. Because I don&rsquo;t want to be adjusting privileges everytime I add a new features, but it doesn&rsquo;t make sense to store this on my running environments, I rarely add new features there.</p><p>If the server is compromised, my root API key could be used to spin up instances for Monero mining or phishing scams etc. So to limit my exposure to these threats I create a non-root IAM user.</p><p>I assign that user the bare minimum privileges, it can only write (not even read) to the specific DynamoDB Table and S3 bucket. This way if the server is compromised, and I lose my api key, the damage is limited. To further limit the damage, I&rsquo;ve set versioning on the S3 bucket, to ensure the attacker can&rsquo;t over-write my backups.</p><p>No Monero mining on my dime here! The AWS permissions for the IAM user are also in the repo in the <a href=https://github.com/keithrozario/Gov-TLS-Audit/tree/master/aws_permissions><strong>aws_permissions/Â </strong></a>folder.</p><p>Enough Scanning, let&rsquo;s talk the server-side.</p><h2>Deploying Lambda using Serverless</h2><h3>Backing up the DB (why US-WEST-2)</h3><p><img src=/uploads/On-Demand-Backup.jpg alt loading=lazy></p><p>First off, let&rsquo;s address an elephant in the room, why deploy this in US-West-2, when I&rsquo;m in Malaysia? The latency would be astronomical &ndash; and it is, almost a quarter second (250ms) in additional latency.</p><p>But DynamoDBs in specific datacenters have native Point-in-Time recovery(PITR) and continuous backup functions, that aren&rsquo;t available in Singapore (AP-Southeast-1). The choice of US-West-2, is an arbitrary choice from the regions that do support these very useful features &ndash; well actually it looked to be cheapest choice, so not so arbitary.</p><p>When PITR and Continuous backsups do eventually come to Singapore, I&rsquo;ll move the table across. Spinning up a <a href=https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exportddbtos3.html>Data Pipeline</a> just to backup a DB isn&rsquo;t acceptable to me. Especially when you see just how blazing fast the backup of DynamoDB is, using native AWS functionality.</p><p>AWS if you&rsquo;re listening &ndash; put DynamoDB backups to Singapore already!!!</p><h2>Serverless Framework</h2><p><img src=/uploads/serverless.png alt loading=lazy></p><p>Now onto Serverless &ndash; the <a href=https://serverless.com/>serverless framework</a> is an amazing tool to automate the deployment of Serverless components like Lambda functions to AWS. In the past, I literally used the console to code & test my lambda functions, which wasn&rsquo;t very good. The console is quite feature rich, but very manual intensive, and hardly operational when we need multiple environments for multiple functions.</p><p>With serverless, I still Code and Unit test in PyCharm, but then I deploy to AWS using the framework. It automatically loads the lambdas + API Gateway configuration (also S3 and DynamoDB) and I can deploy multiple stages and environments with just different CLI arguments. Once the code is uploaded I use Shell Scripts to do the regression testing of the functions.</p><p>The regression testing includes invoking the lambdas via the Serverless Framework (you can invoke with json parameters), and just curl-ing the API Gateway endpoints with the correct query string parameters. The Production API Gateway is exposed via CloudFront, while the Dev API Gateway is curled via the native url from AWS.</p><p>Without serverless this would have to be done manually by hand from the console. Pretty manual, and time-consuming.</p><p>Serverless is great for many reason, but I&rsquo;ll list the key ones here:</p><ul><li>I can invoke the lambda from the command line, meaning I can automate the regression test.</li><li>I can deploy multiple stages and environments, but just changing the command line arguments to --stage UAT</li><li>It keeps all the code in PyCharm and local machine -- which means it stays in the right repo. Previously I would code in the console and manually copy the code out into the Repo.</li></ul><h2>Why 2 CloudFront Distributions?</h2><p><img src=/uploads/NetworkingContentDelivery_AmazonCloudFront_downloaddistribution_LARGE.png alt loading=lazy></p><p>Believe me I tried!! And tried!! And tried!!</p><p>I spent days trying to figure out what was wrong with my CloudFront(CF) distributions, when I attempted to collapse all of this into just one CF distribution (out of <span style=text-decoration:underline>gov-tls-audit.sayakenahack.com</span>). For some reason, I kept on getting a cryptic Missing Authentication Token error. I eventually gave up!! (hangs head in shame).</p><p><em>[update: I found that the CloudFront Distribution Origin Behavior, still passes the path to the Origin. i.e. a call to <strong>example.com/api/test</strong> will still send <strong>/api/test</strong> to the origin to handle, hence your API Gateway must be configured to accept <strong>/api/test</strong> as well, even though /api/* is the behavior mapping]</em></p><p>The only work around, was to have 2 distributions, one for the API and one for the website, and the website could be hosted on a S3 bucket in AP-Southeast-1 (helping with latency).</p><p>The AWS certified bunch will note, that cloud-front distributions are not necessary for either the S3 bucket (I can directly host the website natively there) or even API (we can use regional endpoints instead of Edge-Optimized).</p><p>But S3 buckets didn&rsquo;t seem to have the ability to deliver HTTPs on a custom domain natively, and a CF distribution gives me the ability to create a separate behavior for the <strong>/files</strong> folders where I store my scan files. The contents of this buckets includes all the zip scan files, and currently hosted in US-West-2 together with everything else.</p><h2>Notifications</h2>Let's not just to Dev without the Ops.<p>With the scans running everyday, I would like to see an email of whether it was successful. To do this, I setup a simple SNS service to email me once a new item is loaded into the S3 bucket. That way, every morning when I wake up, I can see an email that tells me if the scans were successful.</p><p>In the next iteration, I might move this to something like a Slack Channel, which has many many advantages over an email.</p><p>Setting the permissions for this was kinda painful, the permisions for the SNS notification is in the aws_permissions folder of the repo for reference.</p><h2>Website</h2>Finally we have the website, which if you visit at<a href=https://gov-tls-audit.sayakenahack.com> https://gov-tls-audit.sayakenahack.com</a> is bare, but it's a simple HTML + JS page that queries the <em>listScans</em> endpoint of the API, and displays a list of all scan reports available. The CloudFront distribution redirects all <strong>/files </strong>queries to a different S3 bucket that stores these files.<p>Also because the site is an S3 bucket behind a CloudFront distribution &ndash; we can redirect users to use HTTPS if they query with just plain ol&rsquo; HTTP, and I do this for all my origins.</p><h2>Conclusion</h2>After the 3rd iteration, I finally have something that almost looks like a finished product (sans the website of course!).<p>And boy, did this take time. I learnt so much along the way and I&rsquo;m really pleased with the results. But here&rsquo;s some key take-aways.</p><p>First, while there might be 1 (or 2) ways of doing something right, there&rsquo;s 1000 more ways to do it wrong. When scanning for TLS on websites, I had to learn to cope with the various redirection methods and the various errors that were popping up, catering for all exceptions takes a massive amount of effort &ndash; but never be tempted to just catch all exceptions &ndash; knowing why something fails is more important than having a successful run.</p><p>Secondly, building this took time. I naively thought I could build the scanner in a weekend &ndash; it took months of time to do. Looking at the final architecture diagram though, it&rsquo;s easy to see why &ndash; and even then I&rsquo;ve left out all the hard work I did scanning for <strong>.gov.my</strong> hostnames, and how I wrote a crawler to get it, the nmap scans on the CIDRs, and the Shodan stitching of the results. So this was collectively where I spent the last 2 months of my free time.</p><p>Third, a lot of the effort spent of DevOps-ifying the process pays off. Serverless saved me countless hours in debugging because I could invoke the lambdas from my command line. GitHub is amazing for obvious reasons, and taking some time out to write the regression test (even though they might just be in Shell Scripts) was worth it. Spending time on the Pipeline pays off even for small things like this &ndash; imagine how much it would save you for your larger projects.</p><p>Finally, this was so much fun, and lot less controversial than <a href=https://www.keithrozario.com/2017/11/sayakenahack-epilogue.html>my previous project</a> :). And looking back on sayakenahack (which was just 6 months ago) the code is a lot better, the architecture is more robust, and everything is just more polished. I&rsquo;m really pleased with the improvements I&rsquo;ve made in that time.</p><p>To cap things off, I do say so myself, this was a win. Next I need to get back to studying for my OSCP, hence the blog will be on hiatus for next 2-3 months. See you on the other side.</p><p><img src=/uploads/win.jpeg alt loading=lazy></p></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><a href=https://keithrozario.com/2018/04/i-deleted-a-tweet-with-2000-retweets-then-i-deleted-2000-more/ class=\"button inline prev\">&lt; [<span class=button__text>First I deleted my most popular tweet -- then I deleted 2000 more.</span>]\n\"><meta property=\"og:url\" content=\"https://keithrozario.com/2015/10/chip-and-pin-an-intro-for-malaysians/\"><meta property=\"og:site_name\" content=\"keithrozario.com\"><meta property=\"og:image\" content=\"https://keithrozario.com/og-image.png\"><meta property=\"og:image:width\" content=\"1200\"><meta property=\"og:image:height\" content=\"627\"><meta property=\"article:section\" content=\"Keith's Favorite Post\"><meta property=\"article:section\" content=\"Malaysia\"><meta property=\"article:section\" content=\"Security &amp;amp; Privacy\"><meta property=\"article:published_time\" content=\"2015-10-29 11:59:54 +0000 UTC\"><link rel=\"shortcut icon\" href=https://keithrozario.com/favicon.ico></head><body><div class=container><header class=header><div class=header__inner><div class=header__logo><a href=https://keithrozario.com/><div class=logo>keithRozario</div></a></div><ul class=\"menu menu--mobile\"><li class=menu__trigger>Menu&nbsp;â–¾</li><li><ul class=menu__dropdown><li><a href=/contact>Contact Me</a></li><li><a href=/categories/keiths-favorite-post>Favorites</a></li><li><a href=/posts>Posts</a></li><li><a href=/categories/security-amp-privacy>Security</a></li><li><a href=/categories/serverless>Serverless</a></li></ul></li></ul></div><nav class=navigation-menu><ul class=\"navigation-menu__inner menu--desktop\"><li><a href=/contact>Contact Me</a></li><li><a href=/categories/keiths-favorite-post>Favorites</a></li><li><a href=/posts>Posts</a></li><li><a href=/categories/security-amp-privacy>Security</a></li><li><a href=/categories/serverless>Serverless</a></li></ul></nav></header><div class=content><article class=post><h1 class=post-title><a href=https://keithrozario.com/2015/10/chip-and-pin-an-intro-for-malaysians/>Chip And Pin : An intro for Malaysians</a></h1><div class=post-meta><time class=post-date>2015-10-29</time></div><div class=post-content><div><p>In 2016, Chip and Pin will gradually be introduced in Malaysia, that means your Credit Cards now will prompt you for a PIN instead of signature during purchases. This will be a bit of a hassle, but it will be worth it, Â here&rsquo;s what you need to know about it and credit card transactions in general.</p><h2>The 5 people you meet in card transaction</h2><p><img src=/uploads/19205891971_2abaa89036_z-300x300.jpg alt=19205891971_2abaa89036_z loading=lazy></p><p>First off, a short primer on credit card transactions. In any business transaction, there are at least 2 actors involved, a buyer and a seller. In industry lingo we call them <strong>MerchantsÂ </strong>andÂ <strong>Cardholders</strong>. These are important terms to remember, as we&rsquo;ll use them extensively .</p><p>But a card transaction is far more complicated and involves at <span style=text-decoration:underline>least</span> 3 more actors, some of which you mayÂ not even be aware off. First, we have the partyÂ that issued the cardholder their card, the &lsquo;<strong>Issuer</strong>&rsquo;. If you have a credit card, chances are that credit card is tied to an line of credit issued by a bank, whether it&rsquo;s HSBC, or Maybank, these are issuers, who have a relationship with the card holder.</p><p>Then we have the &lsquo;<strong>Acquirer</strong>&rsquo;. This is the financial institution that provides the merchant the ability to accept card transactions. Sometime this is as simple as just placing a card terminal on the merchant premise. The acquirer has a relationship withÂ the merchant, and that&rsquo;s why when you look at credit card receipts, they usually have a banks logo on them&ndash;that&rsquo;s the acquirers logo.</p><p>Both the issuer and acquirer are usually banks, because credit cards deal with debt, and only registered financial institutions are authorized by law to perform such transactions (think of interest rates, and loan functions..etc)</p><p>So far, we have the <strong>Issuer</strong> that issues the card to the <strong>cardholder</strong>, and the <strong>Acquirer</strong> that provided the infrastructure to the <strong>merchant</strong>, but how do we tie all of them together. Here the final actor provides a network that <span style=text-decoration:underline>connects all acquirers to all issuers</span>, they&rsquo;re called <strong>Card Schemes</strong>. You know them by their names, VISA, Mastercard, Diners, JCB, Discover..etc.Â The schemes provide the ability to connect acquirers and issuers, soÂ when you go a merchant, you only ask them if they accept Master or Visa, and not worry about the specific acquiring bank. Similarly the merchant places a &ldquo;Mastercard accepted&rdquo; logo on their premise, because if they can accept one Mastercard, they can accept them all.</p><p>These 5 actors, the <strong>Cardholder</strong>, the <strong>Merchant</strong>, the <strong>Acquirer</strong>, the <strong>Issuer</strong> and the <strong>Scheme</strong> work seamlessly together to allow you to purchase goods and services using only a single piece of plastic we call a card.</p><p>But what is a card?</p><h2>What is a Card?</h2>A card a method of authentication. It allows the payment system to identify you, and then tie your identity to a line of credit, which is then used in a very complex debt transfer mechanism. <em>(you need to buy me a beer for me to explain debt transfers to you)</em><p>A card proves you are indeed yourself, and once that identity is confirmed, other checks like your credit limit are used to approve or decline a transaction.</p><p>It&rsquo;s quite amazing that you can go into a machines store in MidValley,Â walk out with a brand new iPad, but only actually pay for the iPad at the end of the month&ndash;not to the merchant, but to the issuing bank. Merchants are consistently told that accepting credit cards pushes sales up, because the psychological impact of purchasing with a card is far lower than with cash. A lot of people would have much smaller TVs if it weren&rsquo;t for 0% interest rate payments.</p><p>But I&rsquo;m getting off-point.</p><p>The point is that credit card transactions are phenomenally convenient, but like all other conveniences they also pose a security challenge. If the only thing protecting your line your credit <em>(which can be much higher thanÂ your monthly salary)</em> is just a piece of plastic,Â then you better be sure that the technology that protects it is going to be pretty much unbeatable.</p><p>And what exactly <strong>is</strong> the technology that protects your identity?</p><h2>EMV vs. Magstripe, FIGHT!!</h2>Putting the <strong>Chip</strong>, in <strong>Chip and Pin</strong>.<p>In the past cards were purely based on Magnetic Stripe technology. These cards had a stripe of data on their back, which contained all the information needed to identify the cardholder. This was a really bad idea as all the data was in plaintext which made it easily read and duplicated.</p><p>You can buy really cheap magstripe readers and writers in the market today, and start cloning you own magstripes cards, but fortunately it won&rsquo;t get you far.</p><p>In the mid 2000&rsquo;s the Association of Banks Malaysia (ABM), decided to migrate all cards in Malaysia to a standard called EMV. EMV is an acronym for Europay, Mastercard, Visa and is essentially a technology standard for a generation of credit cardsÂ based onÂ chips rather than stripes.</p><p>The main driver for the change was the massive amount of fraud in the country, in true Malaysia boleh spirit, we had one of the highest fraud rates in the world, and local issuers and acquirers were pushing for a technological solution to our fraud problem.</p><p>EMV offered a solution by promising that the cards were un-cloneable. The EMV cards in circulation locally today still contain magstripes to ensure they are &lsquo;backwards compatible&rsquo; with foreign countries. In some countries like Thailand and Indonesia a lot of businesses still run on pretty old card infrastructure that only works on magstripe, I remember paying for a 5-star hotel stay in Bali that used a zip-zap machine to read my magstripe. Be assured though that if you swiped your magstripe locally in Malaysia, the transaction would be declined.</p><p>EMV cards (sometimes called Chip cards) are almostÂ mini computers you carry in your wallet. The chip on the card can perform complex cryptographic calculations,Â the keys for which are impossible to obtain from the card directly.</p><p>For the more technical savvy among my readers, at the root of EMV security is a private-public key pair on the card. The private key is never revealed to anyone, and is can never be extracted from the chip(at least that&rsquo;s the assumption). When a card is inserted into an EMV reader, it sends two chained certificates to the reader, one is an scheme signed certificate of the issuer, and another issuer signed certificate of the card. The second certificate contains the public key of card, which the reader can then verify.</p><p>Loaded onto every card reader is a Scheme certificate, which is used to verify the issuer and card certificates.Â Essentially, a Visa certificate is chained to Issuer certificate, which is then chained to an individual card certificate. You can google the public Visa and Mastercard certificates to have a look.</p><p>If you didn&rsquo;t understand the last 2 paragraphs, no worries. What you need to know is that it&rsquo;s MUCH harder to clone a chip card then it is to clone a magstripe cards.</p><p>But cards alone aren&rsquo;t enough are they?</p><h2>2 Factor authentication</h2>Of course not, if all the security relied on the card, then anyone with a stolen card could execute a whole load of transactions.<p>So all cards come with 2 factor authentication.</p><p>When we talk about authentication, a &lsquo;factor&rsquo; is either</p><ol><li>Something you Have (like a card, token, thumbprint..etc)</li><li>Something you know (like a password, or passphrase)</li><li>Something only you can do (like a signature, or answering security questions)</li></ol>When you have just one factor, that's considered poor security. Modern systems usually have a combination of more than one factor, hence the term 2-Factor authentication.<p>For example, a cash withdrawal at the ATM uses 2 factor authentication. To withdraw money from your account you must first have the ATM card<em> (something you have)</em> and enter the PIN<em> (something you know)</em>. Hence losing your card isn&rsquo;t so worrying because criminals still need the 6-digit PIN, and if you&rsquo;re protected it sufficiently, you&rsquo;ve got no worries.</p><p>By the way, there&rsquo;s a myth going around that if you enter your PIN backwards, the Police will be alerted&mdash;that&rsquo;s bullshit!!. The bank doesn&rsquo;t even know your PIN, and logicallyÂ wouldn&rsquo;t know the reverse of it either. If you&rsquo;re wondering how the Bank can authenticate a PIN without knowing it&ndash;<a href=https://www.keithrozario.com/2015/10/the-problem-with-bio-metrics.html>read this</a>.</p><p>But what does 2-Factor authentication have to do with Chip And PIN?</p><h2>Signature vs. PIN</h2>![signature](/uploads/signature.jpg)Technically even today all credit transactions are two-factor authenticated. You must have the card, and you must be able to sign according to the signature behind the card.<p>But all of that assumes that a signature is hard to forge, and that cashiers verify the signature&ndash;both of which aren&rsquo;t true. The main problem with signatures is that it relies on a human (the cashier) to verify the transaction, that&rsquo;s weak in every sense of the word.</p><p>Malaysia was one of the first countries to implement EMV, and we chose the Chip and Signature variant&ndash;at the time most countries were still on Magstripe, and our local fraud dropped to near zero because we used better technology than the rest of the world. It wasn&rsquo;t that criminals couldn&rsquo;t crack it, it was just that it was easier to shift their operations elsewhere in the world than invest resources to crack EMV.</p><p>Criminals are opportunist, why spend $10 to crack 1 EMV card, when you can crack 100 magstripe cards for the same amount. Hence they just moved to countries with the lower tech, and eventually ended up in America where those poor souls have only just moved to EMV Chip and Signature.</p><p>So if we want to keep fraud low, we need to upgrade the tech, and the logical step is to move from EMV Chip and Signature to EMV Chip and PIN, which is almost the defacto standard in all but a handful of countries.</p><p>But Why are Pins more secure?</p><p><img src=/uploads/7212980434_e0b58bd5d2_m.jpg alt=7212980434_e0b58bd5d2_m loading=lazy>A PIN is fundamentally more secure than a card, because it is verified by the issuing bank (<em>or sometimes on the terminal by the card itself, but I don&rsquo;t have time to explore offline and online PIN here&ndash;again that needs a beer</em>), and the verification process is automatic. So we don&rsquo;t have to rely on cashiers and human judgement, it also frees up the cashier, because no decision is reliant on them.</p><p>An extra benefit of Chip and PIN is there are less opportunities for downgrade attacks. Because Signatures need human verification, there are special circumstances where card transactions don&rsquo;t require signatures. When you pump petrol in Malaysia and swipe your card at the pump, no signature is requested from you&ndash;because there would be no way to verify it. So a petrol purchase, specifically one done at the outdoor pump, is a pure Single Factor authenticated transaction&ndash;you only need the card <em>(something you have)</em></p><p>In the future with Chip and PIN, we will have 2 factor authenticated transactions at the petrol pump because cardholders would need both their card AND their PIN.Â It&rsquo;s pretty great, but Chip and Pin won&rsquo;t always come to rescue.</p><h2>Where Chip & Pin won't help</h2>Earlier I made the distinction of physical card transactions atÂ stores, where you swipe your card and online transactions where no cards are swipe.Just how do you secure the latter?<p>Online purchases are pretty complex, they involve you entering your card number, expiry date and CVV2 code (that&rsquo;s the 3 or 4 digit code printed somewhere on your card). While you&rsquo;re entering more information than if you just swiped it at a reader, you never enter your PIN online. There&rsquo;s a good reason for that, because PINs are to be protected like the holy grail of Jerusalem, and should never be entered into something as insecure as a web-browser.</p><p>Fundamentally, that makes online purchases a single-factor authenticated transactions&mldr;unless the issuer does some magic, Maybank sometimes prompts your for a special code sent to your mobile, while HSBC would occasionally ask for special 6-digit code provided by the &lsquo;dongle&rsquo; they provided certain customers. There are the exceptions rather than the rule. Most of the time, with just a handful of numbers (23 to be exact), a criminal can purchase goods and services online with your line of credit.</p><p>As you can see Chip and PIN doesn&rsquo;t help in thisÂ online (card NOT present transactions)Â so is it worth it?</p><p><em>*Technically Chip and PIN will reduce the chances of someone skimming your card details. When the migration to Chip and PIN is complete, waiters at a restaurant (or anywhere else) will have to bring a terminal to YOU Â to enter the PIN, hence your card never leaves your sight. This is far more secure than you handing over your card to a waiter and hoping for them to return with receipt&ndash;in the moment when you lose sight of your card, the waiter could have copied the card number, expiry date and CVV2 information, and then subscribe to hardcoreÂ foot fetish porn sites using just those details.</em></p><h2>Where Chip & Pin will help</h2>Chip & PIN is fundamentally more secure than Chip & Signature...BUT it isn't <strong>absolutely</strong> secure. There's an entire group working at Cambridge University trying to crack it, and many cracks have alreadyÂ appeared. The encryption works in principal, but the protocol of communication between the terminal and card has some 'gaps' that need further addressing.<p>In security of the masses, you can never be 100% secure, but you can put in place systems that make you less of a target. Moving Chip & PIN will elevate Malaysia out of the lowest rung in the technological ladder, and keep us away from &lsquo;most&rsquo; of the criminals.</p><h2>Conclusion & a note about liability</h2><p><img src=/uploads/5654834876_3c5feb5678_z-300x194.jpg alt=5654834876_3c5feb5678_z loading=lazy></p><p>Even if you didn&rsquo;t understand what I just said throughout this ridiculously long 2000 word post, you need to at least understand this.</p><p>In Malaysia, we have a Law that limits the liability of a cardholder to Rm250 provided they reported the lost card in a &lsquo;reasonable&rsquo; amount of time. Moving to Chip and PIN may impact that significantly, and increase your liability, simply because it wasn&rsquo;t just a lost or stolen card, but also a lost or stolen PIN as well. Losing a physical item is &lsquo;reasonable&rsquo;, telling someone a secret like a PIN is something else entirely.</p><p>My guess is that the banks are going to come down much harder on fraud, and try to shift that liability to customers as we&rsquo;ve seen in other countries.Make sure you understand your agreement before signing up for a card, and make sure you understand how the law can (or cannot) protect you.</p><p>If you&rsquo;re a merchant, consider this. Chip and PIN isn&rsquo;t mandatory for everyone. The incentive given to acquirers and issuers is liability. The rules are simple, if there is a fraudulent transaction, the party with the lowest technology foots the bill. So if a card is a Chip & PIN card, but the merchant only had a Chip and Signature terminal, the acquirer (or merchant) assumes liability. If the terminal is a Chip and PIN capable one, but the card can only support signature, the issuer assumes liability, and since nobody wants to be liable, there is an incentive to move to Chip and PIN quicker.</p><p>That being said, happy PINning.</p></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><a href=https://keithrozario.com/2015/11/hackers-and-terrorist/ class=\"button inline prev\">&lt; [<span class=button__text>Hackers and terrorist</span>]\nProcess Group PGID: 20131"
  }
}
```

---

## MODEL âœ¨

**Results:**

1.  **Gov TLS Audit:**
    *   `<img src=/uploads/gitHub.png ...>` - **FIXED**.
    *   `<img src=/uploads/IAM.png ...>` - **FIXED**.
    *   `<img src=/uploads/On-Demand-Backup.jpg ...>` - **FIXED**.
    *   `<img src=/uploads/serverless.png ...>` - **FIXED**.
    *   `<img src=/uploads/NetworkingContentDelivery_AmazonCloudFront_downloaddistribution_LARGE.png ...>` - **FIXED**.

2.  **Chip and Pin:**
    *   `<img src=/uploads/19205891971_2abaa89036_z-300x300.jpg ...>` - **FIXED**.
    *   Wait, the curl output says: `![signature](/uploads/signature.jpg)`
    *   **FAIL:** One image in "Chip and Pin" is still raw markdown!
    *   `![signature](/uploads/signature.jpg)`
    *   Let's see the context from curl output: `<h2>Signature vs. PIN</h2>![signature](/uploads/signature.jpg)Technically even today...`

**Why did this one fail?**
It probably didn't have any wrapper tags to begin with!
In the original WordPress export, this image might have been just `![]()` or `<img ...>` without a wrapper like `<figure>` or `<div>`.
My script processes `<img>` tags by converting them to `![]()` first, then it strips wrappers around `![]()`.
But if it was converted to `![]()` and had NO wrappers, the `re.sub(wrapper... \1 ...)` logic wouldn't touch it.
And if it was originally stuck to the header like `<h2>...</h2><img ...>`, the conversion would make it `<h2>...</h2>![]()`.
And since there are no wrappers to strip, my "Add newline" logic (which is inside the wrapper stripping regex) didn't apply!

**The Missing Logic:**
I need to ensure that **ALL** markdown images are surrounded by newlines, even if they didn't have wrappers.

**However**, indiscriminately adding newlines around *every* `![]()` might break inline images (icons).
But as we discussed, most images in these posts are block images.

**Alternative:**
I should add a final pass to ensure `![]()` patterns that are immediately preceded by `>` (closing HTML tag) or followed by text have newlines.

Or, I can just do a global replacement of `!\[` with `\n\n![` and `\)` (end of image) with `)\n\n`.
But that's dangerous if there are inline images.

Let's look at the failing line again:
`<h2>Signature vs. PIN</h2>![signature](/uploads/signature.jpg)Technically even today...`

It was likely `<h2>...</h2><img ...>` in the source XML.
My script does this:
```python
        # Convert images
        if 'src="' in content:
            # ... regex to convert <img ...> to ![]() ...
```

I need to check the `<img` conversion logic. If I ensure that converted images have newlines around them *during* the conversion from `<img>` to `![]()`, that would solve it for unwrapped images.

Let's check `migrate_robust.py` again.

**Tool Command**:
```json
{
  "name": "read_file",
  "args": {
    "file_path": "migrate_robust.py"
  }
}
```

---

## USER ğŸ§‘â€ğŸ’»

**Tool Response**:
```json
{
  "id": "read_file-1771085184865-20e8b67741bb7",
  "name": "read_file",
  "response": {
    "output": "import xml.etree.ElementTree as ET\nimport os\nimport re\nimport html\nfrom datetime import datetime\n\n# Namespaces for parsing\nnamespaces_decl = 'xmlns:excerpt=\"http://wordpress.org/export/1.2/excerpt/\" xmlns:content=\"http://purl.org/rss/1.0/modules/content/\" xmlns:wfw=\"http://wellformedweb.org/CommentAPI/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:wp=\"http://wordpress.org/export/1.2/\"'\nnamespaces = {\n    'content': 'http://purl.org/rss/1.0/modules/content/',\n    'wp': 'http://wordpress.org/export/1.2/',\n    'dc': 'http://purl.org/dc/elements/1.1/'\n}\n\ndef parse_wordpress_xml_robust(xml_file, output_dir):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    print(f\"Reading {xml_file}...\")\n    try:\n        with open(xml_file, 'r', encoding='utf-8', errors='replace') as f:\n            content = f.read()\n    except Exception as e:\n        print(f\"Error reading file: {e}\")\n        return\n\n    # Find all items\n    item_pattern = re.compile(r'<item>(.*?)</item>', re.DOTALL)\n    items = item_pattern.findall(content)\n    \n    print(f\"Found {len(items)} items. Processing...\")\n    \n    success_count = 0\n    fail_count = 0\n    \n    newline = chr(10)\n\n    for i, item_content in enumerate(items):\n        wrapped_content = f'<root {namespaces_decl}><item>{item_content}</item></root>'\n        \n        try:\n            root = ET.fromstring(wrapped_content)\n            item = root.find('item')\n            \n            post_type_elem = item.find('wp:post_type', namespaces)\n            if post_type_elem is None: continue\n            post_type = post_type_elem.text\n            \n            if post_type not in ['post', 'page']: continue\n            if item.find('wp:status', namespaces).text != 'publish': continue\n\n            title_elem = item.find('title')\n            title = title_elem.text if title_elem is not None else \"Untitled\"\n            if title is None: title = \"Untitled\"\n            # ESCAPE QUOTES FOR TOML\n            title = title.replace('\"', '\\\\\"')\n                \n            post_name_elem = item.find('wp:post_name', namespaces)\n            if post_name_elem is not None and post_name_elem.text:\n                post_name = post_name_elem.text\n            else:\n                post_name = re.sub(r'[^a-z0-9]+', '-', title.lower()).strip('-')\n                \n            post_date_elem = item.find('wp:post_date', namespaces)\n            if post_date_elem is not None:\n                try:\n                    date_obj = datetime.strptime(post_date_elem.text, '%Y-%m-%d %H:%M:%S')\n                    date_str = date_obj.isoformat()\n                except:\n                    date_str = datetime.now().isoformat()\n            else:\n                date_str = datetime.now().isoformat()\n                \n            content_elem = item.find('content:encoded', namespaces)\n            post_content = content_elem.text if content_elem is not None else \"\"\n            if post_content is None: post_content = \"\"\n                \n            post_content = re.sub(r'src=\".*?(?:wp-content/uploads|files)/(.*?)\"', r'src=\"/uploads/\\1\"', post_content)\n            post_content = re.sub(r'href=\".*?(?:wp-content/uploads|files)/(.*?)\"', r'href=\"/uploads/\\1\"', post_content)\n            \n            # CONVERT HTML IMG TAGS TO MARKDOWN\n            # This regex captures the src attribute and optionally the alt attribute from an img tag\n            # It handles various attribute orders and quotes\n            def img_replacer(match):\n                attrs = match.group(1)\n                src_match = re.search(r'src=[\"\\'](.*?)[\"\\']', attrs)\n                if not src_match: return match.group(0)\n                src = src_match.group(1)\n                \n                alt_match = re.search(r'alt=[\"\\'](.*?)[\"\\']', attrs)\n                alt = alt_match.group(1) if alt_match else \"\"\n                \n                title_match = re.search(r'title=[\"\\'](.*?)[\"\\']', attrs)\n                title = title_match.group(1) if title_match else \"\"\n                \n                md = f\"![{alt}]({src}\"\n                if title: md += f' \"{title}\"'\n                md += \")\"\n                return md\n\n            post_content = re.sub(r'<img\\s+([^>]+)>', img_replacer, post_content)\n            \n            # STRIP FIGURE AND CENTER TAGS AROUND IMAGES\n            # Recursively strip wrapper tags until no more changes occur\n            prev_content = \"\"\n            while prev_content != post_content:\n                prev_content = post_content\n                # Specifically target markdown image syntax inside wrappers\n                # Capture ![alt](src \"title\")\n                md_img_regex = r'(!\\[.*?\\]\\(.*?\\))'\n                \n                post_content = re.sub(r'<figure[^>]*>\\s*' + md_img_regex + r'\\s*</figure>', r'\\n\\n\\1\\n\\n', post_content, flags=re.DOTALL)\n                post_content = re.sub(r'<div[^>]*>\\s*' + md_img_regex + r'\\s*</div>', r'\\n\\n\\1\\n\\n', post_content, flags=re.DOTALL)\n                post_content = re.sub(r'<center[^>]*>\\s*' + md_img_regex + r'\\s*</center>', r'\\n\\n\\1\\n\\n', post_content, flags=re.DOTALL)\n                post_content = re.sub(r'<p>\\s*' + md_img_regex + r'\\s*</p>', r'\\n\\n\\1\\n\\n', post_content, flags=re.DOTALL)\n                post_content = re.sub(r'<a[^>]*>\\s*' + md_img_regex + r'\\s*</a>', r'\\n\\n\\1\\n\\n', post_content, flags=re.DOTALL)\n\n            post_content = post_content.replace('&nbsp;', ' ')\n            \n            tags = []\n            categories = []\n            for cat in item.findall('category'):\n                domain = cat.get('domain')\n                if cat.text:\n                    if domain == 'post_tag': tags.append(cat.text)\n                    elif domain == 'category': categories.append(cat.text)\n                    \n            fm_lines = []\n            fm_lines.append(\"+++\")\n            fm_lines.append(f'title = \"{title}\"')\n            fm_lines.append(f'slug = \"{post_name}\"')\n            fm_lines.append(f'date = \"{date_str}\"')\n            fm_lines.append('draft = false')\n            if tags: fm_lines.append(f'tags = {tags}')\n            if categories: fm_lines.append(f'categories = {categories}')\n            fm_lines.append(\"+++\")\n            fm_lines.append(\"\")\n            \n            front_matter = newline.join(fm_lines)\n            \n            file_path = os.path.join(output_dir, f\"{post_name}.md\")\n            with open(file_path, 'w', encoding='utf-8') as f:\n                f.write(front_matter)\n                f.write(newline)\n                f.write(post_content)\n            \n            success_count += 1\n            \n        except ET.ParseError as e:\n            fail_count += 1\n            continue\n        except Exception as e:\n            fail_count += 1\n            continue\n\n    print(f\"Migration complete. Success: {success_count}, Failed: {fail_count}\")\n\nif __name__ == \"__main__\":\n    xml_file = \"keithrozariocom.WordPress.2026-02-13.xml\"\n    output_dir = \"keithrozario_blog/content/posts\"\n    parse_wordpress_xml_robust(xml_file, output_dir)\n"
  }
}
```

---

## MODEL âœ¨

I see the issue.

The function `img_replacer` is defined inside the loop (which is inefficient but fine), and it constructs the markdown string `md`.

```python
            def img_replacer(match):
                # ...
                md = f"![{alt}]({src})"
                # ...
                return md
            
            post_content = re.sub(r'<img\s+([^>]+)>', img_replacer, post_content)
```

The regex `r'<img\s+([^>]+)>'` replaces the `<img>` tag with the returned markdown.
If the original HTML was `<h2>Header</h2><img ...>Text`, the result is `<h2>Header</h2>![...](...)Text`.

I need to make sure that `img_replacer` returns the markdown surrounded by newlines.

**Proposed Change:**
Change `return md` to `return f"\n\n{md}\n\n"` inside `img_replacer`.

This will ensure that **every single converted image** is surrounded by newlines, regardless of whether it had wrappers or not.

**But wait**, what if it *does* have wrappers?
Original: `<figure><img ...></figure>`
Step 1 (img_replace): `<figure>\n\n![...](...)\n\n</figure>`
Step 2 (wrapper strip):
My regex for wrapper stripping is: `r'<figure[^>]*>\s*(!\[.*?\]\(.*?\))\s*</figure>'`

The `\s*` inside the regex matches whitespace, including newlines.
So `\s*` will match the `\n\n` I added.
And the replacement `r'\n\n\1\n\n'` will effectively preserve (or enforce) the newlines.

So:
`<figure>\n\n![...](...)\n\n</figure>`
Matches `\s*` -> `\n\n`
Matches `(!\[...\]\(...\))` -> `![...](...)`
Matches `\s*` -> `\n\n`
Replaces with `\n\n![...](...)\n\n`

Result: `\n\n![...](...)\n\n`

This seems correct and robust.

**Potential Issue:**
If I add `\n\n` inside `img_replacer`, and then the wrapper stripper *also* adds `\n\n`, do I get `\n\n\n\n`?
The wrapper stripper regex matches the *entire* content between the wrapper tags, including the newlines I added.
So it replaces `<figure>...newlines...image...newlines...</figure>` with just `newlines...image...newlines`.
So it shouldn't duplicate them excessively, but even if I have 4 newlines, Markdown treats multiple blank lines as a single block separator (mostly).

**Inline Images:**
If there is a legitimate inline image (icon) inside a sentence:
`Text <img src="icon.png"> text.`
It becomes:
`Text \n\n![icon](icon.png)\n\n text.`
This will break the paragraph.
`Text`
`![icon](icon.png)`
`text.`

Is this acceptable?
For a blog migration, usually yes. Most images are block images. Icons inside text are rare in standard WordPress posts unless specifically designed.
Given the current issues with "Chip and Pin" having images stuck to headers, this is a necessary tradeoff.

I will proceed with modifying `img_replacer` to return newlines.

AND, I should also correct the regex for `img_replacer`.
It currently uses `re.sub(r'<img\s+([^>]+)>', img_replacer, post_content)`.
This regex is a bit fragile (doesn't handle `/>` self-closing well if there's no space before `/>`? valid HTML usually has space or just `>`).
But looking at the WordPress XML, `<img>` tags are usually `<img ... />` or `<img ... >`.
The regex `<img\s+([^>]+)>` matches `<img attributes...>` or `<img attributes... />` (the `>` stops the match).
Wait, `[^>]+` matches everything *except* `>`.
So if the tag is `<img src="..." />`, the group 1 is `src="..." /`.
Then inside `img_replacer`:
`src_match = re.search(r'src=["\'](.*?)["\']', attrs)` works fine.
`alt_match` works fine.

The issue might be if `post_content` has multiple images on one line? `re.sub` handles that.

One small fix in `migrate_robust.py`:
`md = f"![{alt}]({src}"`
It constructs `![alt](src`.
I should change this to `md = f"![{alt}]({src})"` (oops, the original code had `md += ")"`).
Original:
```python
                md = f"![{alt}]({src}"
                if title: md += f' "{title}"'
                md += ")"
                return md
```
This is correct.

I will just change `return md` to `return f"\n\n{md}\n\n"`.

**One more thing:**
The wrapper stripping regex logic:
`post_content = re.sub(r'<figure[^>]*>\s*' + md_img_regex + r'\s*</figure>', r'\n\n\1\n\n', post_content, flags=re.DOTALL)`

If `md_img_regex` is `(!\[.*?\]\(.*?\))`, it matches `![...](...)`.
It does **NOT** match `![...](...)` if there are newlines *inside* the image syntax? No, `.*?` matches newlines if `re.DOTALL` is on.
But `md_img_regex` definition: `md_img_regex = r'(!\[.*?\]\(.*?\))'`
It uses `.` which usually doesn't match newlines unless DOTALL is passed.
The `re.sub` call **HAS** `flags=re.DOTALL`.
So `md_img_regex` will match correctly even if I add newlines inside `[...]` or `(...)` (which I won't).

However, if I add newlines *around* the image in `img_replacer`:
`<figure>\n\n![...](...)\n\n</figure>`

The regex `r'<figure[^>]*>\s*' + md_img_regex + r'\s*</figure>'`
`\s*` matches `\n\n`.
`md_img_regex` matches `![...](...)`.
`\s*` matches `\n\n`.

So it will match.

**Plan:**
1.  Modify `migrate_robust.py` to add `\n\n` in `img_replacer`.
2.  Regenerate.
3.  Deploy.

This should cover the "Chip and Pin" case where there were no wrappers.

One detail: `post_content` might typically look like: `...text<img ...>text...`
Replacing with `...text\n\n![...](...)\n\ntext...` is exactly what we want.

I'll make the change.

---

## USER ğŸ§‘â€ğŸ’»

how is it coming along so far?

---

## MODEL âœ¨

I've made good progress: the initial "Lambda" post is fixed, but my audit of the "Security & Privacy" and "Favorite" categories revealed that several other posts still show raw Markdown text for images. This is happening because many images are "stuck" to headers or text without proper spacing (newlines), causing Hugo to ignore them.

I'm currently updating the migration script to ensure **all** images are universally surrounded by newlines during conversion. This will guarantee they are recognized as standalone blocks and rendered correctly across the entire site. I'll have the next update once the deployment finishes.

---

## USER ğŸ§‘â€ğŸ’»

OK is the deployment complete?

---

## MODEL âœ¨

I am applying the final fix to the `img_replacer` function in `migrate_robust.py` to ensure every image is separated by newlines, even if it wasn't wrapped in extra HTML. This will solve the remaining rendering issues in the "Chip and Pin" and "Gov TLS Audit" posts. I'll start the deployment immediately after.

**Tool Command**:
```json
{
  "name": "replace",
  "args": {
    "file_path": "/home/keith_krozario_altostrat_com/projects/keithrozario_dot_com/migrate_robust.py",
    "new_string": "                md = f\"![{alt}]({src}\"\n                if title: md += f' \"{title}\"'\n                md += \")\"\n                return f\"\\n\\n{md}\\n\\n\"\n\n            post_content = re.sub(r'<img\\s+([^>]+)>', img_replacer, post_content)",
    "instruction": "Ensure all converted images are surrounded by newlines in the img_replacer function.",
    "expected_replacements": 1,
    "old_string": "                md = f\"![{alt}]({src}\"\n                if title: md += f' \"{title}\"'\n                md += \")\"\n                return md\n\n            post_content = re.sub(r'<img\\s+([^>]+)>', img_replacer, post_content)"
  }
}
```

---

## USER ğŸ§‘â€ğŸ’»

**Tool Response**:
```json
{
  "id": "replace-1771085628548-7277ae8388a6e",
  "name": "replace",
  "response": {
    "output": "Successfully modified file: /home/keith_krozario_altostrat_com/projects/keithrozario_dot_com/migrate_robust.py (1 replacements)."
  }
}
```

---

## MODEL âœ¨

I'll now regenerate the content, commit the changes, and push them to the repository. This will trigger the final deployment to fix the remaining image rendering issues. I'll watch the deployment progress and verify the results once it's finished.**Tool Command**:
```json
{
  "name": "run_shell_command",
  "args": {
    "command": "python3 migrate_robust.py\ngit add keithrozario_blog/content/posts/*.md\ngit commit -m \"Universal fix for image rendering: Ensure newlines around all ![]() patterns\"\ngit push origin main\ngh run watch"
  }
}
```

---

## USER ğŸ§‘â€ğŸ’»

**Tool Response**:
```json
{
  "id": "run_shell_command-1771085631033-9ac52fa7d93c5",
  "name": "run_shell_command",
  "response": {
    "output": "Command was automatically cancelled because it exceeded the timeout of 5.0 minutes without output. Below is the output before it was cancelled:\nReading keithrozariocom.WordPress.2026-02-13.xml...\nFound 1948 items. Processing...\nMigration complete. Success: 622, Failed: 2\n[main a65e73e] Universal fix for image rendering: Ensure newlines around all ![]() patterns\n 65 files changed, 486 insertions(+), 84 deletions(-)\nEnumerating objects: 139, done.\nCounting objects: 100% (139/139), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (70/70), done.\nWriting objects: 100% (70/70), 5.72 KiB | 2.86 MiB/s, done.\nTotal 70 (delta 68), reused 0 (delta 0), pack-reused 0 (from 0)\nremote: Resolving deltas: 100% (68/68), completed with 68 local objects.\nTo https://github.com/keithrozario/keithrozariodotcom.git\n   a4a79af..a65e73e  main -> main\n? Select a workflow run  [Use arrows to move, type to filter]\n> * Universal fix for image rendering: Ensure newlines around all ![]() pâ€¦, Deploy Hugo site to Pages [main] 10s ago"
  }
}
```

---

## MODEL âœ¨

The final deployment is now running. I will check its status in about 2 minutes.

Once complete, I will verify the "Chip and Pin" and "Gov TLS Audit" posts again. If those are fixed, we can be confident the issue is resolved globally.**Tool Command**:
```json
{
  "name": "run_shell_command",
  "args": {
    "command": "gh run list --limit 1\nsleep 10\ngh run list --limit 1"
  }
}
```

---

## USER ğŸ§‘â€ğŸ’»

**Tool Response**:
```json
{
  "id": "run_shell_command-1771085947912-9c87bce88171e",
  "name": "run_shell_command",
  "response": {
    "output": "Output: STATUS  TITLE                  WORKFLOW          BRANCH  EVENT  ID           ELAPSED  AGE             \nâœ“       Universal fix for ...  Deploy Hugo s...  main    push   22020428820  2m17s    about 5 minut...\nSTATUS  TITLE                  WORKFLOW          BRANCH  EVENT  ID           ELAPSED  AGE             \nâœ“       Universal fix for ...  Deploy Hugo s...  main    push   22020428820  2m17s    about 5 minut...\nProcess Group PGID: 20278"
  }
}
```

---

## MODEL âœ¨

Deployment successful.

Final verification:

1.  **Chip and Pin:** `https://keithrozario.com/2015/10/chip-and-pin-an-intro-for-malaysians/`
2.  **Gov TLS Audit:** `https://keithrozario.com/2018/04/gov-tls-audit-architecture/`

I'll check for the `signature.jpg` and `gitHub.png` images specifically.

**Tool Command**:
```json
{
  "name": "run_shell_command",
  "args": {
    "command": "curl -s -L https://keithrozario.com/2015/10/chip-and-pin-an-intro-for-malaysians/ | grep \"signature.jpg\"\ncurl -s -L https://keithrozario.com/2018/04/gov-tls-audit-architecture/ | grep \"gitHub.png\""
  }
}
```

---

## USER ğŸ§‘â€ğŸ’»

**Tool Response**:
```json
{
  "id": "run_shell_command-1771085984223-d6c2fd3717ad8",
  "name": "run_shell_command",
  "response": {
    "output": "Output: \"><meta property=\"og:url\" content=\"https://keithrozario.com/2015/10/chip-and-pin-an-intro-for-malaysians/\"><meta property=\"og:site_name\" content=\"keithrozario.com\"><meta property=\"og:image\" content=\"https://keithrozario.com/og-image.png\"><meta property=\"og:image:width\" content=\"1200\"><meta property=\"og:image:height\" content=\"627\"><meta property=\"article:section\" content=\"Keith's Favorite Post\"><meta property=\"article:section\" content=\"Malaysia\"><meta property=\"article:section\" content=\"Security &amp;amp; Privacy\"><meta property=\"article:published_time\" content=\"2015-10-29 11:59:54 +0000 UTC\"><link rel=\"shortcut icon\" href=https://keithrozario.com/favicon.ico></head><body><div class=container><header class=header><div class=header__inner><div class=header__logo><a href=https://keithrozario.com/><div class=logo>keithRozario</div></a></div><ul class=\"menu menu--mobile\"><li class=menu__trigger>Menu&nbsp;â–¾</li><li><ul class=menu__dropdown><li><a href=/contact>Contact Me</a></li><li><a href=/categories/keiths-favorite-post>Favorites</a></li><li><a href=/posts>Posts</a></li><li><a href=/categories/security-amp-privacy>Security</a></li><li><a href=/categories/serverless>Serverless</a></li></ul></li></ul></div><nav class=navigation-menu><ul class=\"navigation-menu__inner menu--desktop\"><li><a href=/contact>Contact Me</a></li><li><a href=/categories/keiths-favorite-post>Favorites</a></li><li><a href=/posts>Posts</a></li><li><a href=/categories/security-amp-privacy>Security</a></li><li><a href=/categories/serverless>Serverless</a></li></ul></nav></header><div class=content><article class=post><h1 class=post-title><a href=https://keithrozario.com/2015/10/chip-and-pin-an-intro-for-malaysians/>Chip And Pin : An intro for Malaysians</a></h1><div class=post-meta><time class=post-date>2015-10-29</time></div><div class=post-content><div><p>In 2016, Chip and Pin will gradually be introduced in Malaysia, that means your Credit Cards now will prompt you for a PIN instead of signature during purchases. This will be a bit of a hassle, but it will be worth it, Â here&rsquo;s what you need to know about it and credit card transactions in general.</p><h2>The 5 people you meet in card transaction</h2><p><img src=/uploads/19205891971_2abaa89036_z-300x300.jpg alt=19205891971_2abaa89036_z loading=lazy></p><p>First off, a short primer on credit card transactions. In any business transaction, there are at least 2 actors involved, a buyer and a seller. In industry lingo we call them <strong>MerchantsÂ </strong>andÂ <strong>Cardholders</strong>. These are important terms to remember, as we&rsquo;ll use them extensively .</p><p>But a card transaction is far more complicated and involves at <span style=text-decoration:underline>least</span> 3 more actors, some of which you mayÂ not even be aware off. First, we have the partyÂ that issued the cardholder their card, the &lsquo;<strong>Issuer</strong>&rsquo;. If you have a credit card, chances are that credit card is tied to an line of credit issued by a bank, whether it&rsquo;s HSBC, or Maybank, these are issuers, who have a relationship with the card holder.</p><p>Then we have the &lsquo;<strong>Acquirer</strong>&rsquo;. This is the financial institution that provides the merchant the ability to accept card transactions. Sometime this is as simple as just placing a card terminal on the merchant premise. The acquirer has a relationship withÂ the merchant, and that&rsquo;s why when you look at credit card receipts, they usually have a banks logo on them&ndash;that&rsquo;s the acquirers logo.</p><p>Both the issuer and acquirer are usually banks, because credit cards deal with debt, and only registered financial institutions are authorized by law to perform such transactions (think of interest rates, and loan functions..etc)</p><p>So far, we have the <strong>Issuer</strong> that issues the card to the <strong>cardholder</strong>, and the <strong>Acquirer</strong> that provided the infrastructure to the <strong>merchant</strong>, but how do we tie all of them together. Here the final actor provides a network that <span style=text-decoration:underline>connects all acquirers to all issuers</span>, they&rsquo;re called <strong>Card Schemes</strong>. You know them by their names, VISA, Mastercard, Diners, JCB, Discover..etc.Â The schemes provide the ability to connect acquirers and issuers, soÂ when you go a merchant, you only ask them if they accept Master or Visa, and not worry about the specific acquiring bank. Similarly the merchant places a &ldquo;Mastercard accepted&rdquo; logo on their premise, because if they can accept one Mastercard, they can accept them all.</p><p>These 5 actors, the <strong>Cardholder</strong>, the <strong>Merchant</strong>, the <strong>Acquirer</strong>, the <strong>Issuer</strong> and the <strong>Scheme</strong> work seamlessly together to allow you to purchase goods and services using only a single piece of plastic we call a card.</p><p>But what is a card?</p><h2>What is a Card?</h2>A card a method of authentication. It allows the payment system to identify you, and then tie your identity to a line of credit, which is then used in a very complex debt transfer mechanism. <em>(you need to buy me a beer for me to explain debt transfers to you)</em><p>A card proves you are indeed yourself, and once that identity is confirmed, other checks like your credit limit are used to approve or decline a transaction.</p><p>It&rsquo;s quite amazing that you can go into a machines store in MidValley,Â walk out with a brand new iPad, but only actually pay for the iPad at the end of the month&ndash;not to the merchant, but to the issuing bank. Merchants are consistently told that accepting credit cards pushes sales up, because the psychological impact of purchasing with a card is far lower than with cash. A lot of people would have much smaller TVs if it weren&rsquo;t for 0% interest rate payments.</p><p>But I&rsquo;m getting off-point.</p><p>The point is that credit card transactions are phenomenally convenient, but like all other conveniences they also pose a security challenge. If the only thing protecting your line your credit <em>(which can be much higher thanÂ your monthly salary)</em> is just a piece of plastic,Â then you better be sure that the technology that protects it is going to be pretty much unbeatable.</p><p>And what exactly <strong>is</strong> the technology that protects your identity?</p><h2>EMV vs. Magstripe, FIGHT!!</h2>Putting the <strong>Chip</strong>, in <strong>Chip and Pin</strong>.<p>In the past cards were purely based on Magnetic Stripe technology. These cards had a stripe of data on their back, which contained all the information needed to identify the cardholder. This was a really bad idea as all the data was in plaintext which made it easily read and duplicated.</p><p>You can buy really cheap magstripe readers and writers in the market today, and start cloning you own magstripes cards, but fortunately it won&rsquo;t get you far.</p><p>In the mid 2000&rsquo;s the Association of Banks Malaysia (ABM), decided to migrate all cards in Malaysia to a standard called EMV. EMV is an acronym for Europay, Mastercard, Visa and is essentially a technology standard for a generation of credit cardsÂ based onÂ chips rather than stripes.</p><p>The main driver for the change was the massive amount of fraud in the country, in true Malaysia boleh spirit, we had one of the highest fraud rates in the world, and local issuers and acquirers were pushing for a technological solution to our fraud problem.</p><p>EMV offered a solution by promising that the cards were un-cloneable. The EMV cards in circulation locally today still contain magstripes to ensure they are &lsquo;backwards compatible&rsquo; with foreign countries. In some countries like Thailand and Indonesia a lot of businesses still run on pretty old card infrastructure that only works on magstripe, I remember paying for a 5-star hotel stay in Bali that used a zip-zap machine to read my magstripe. Be assured though that if you swiped your magstripe locally in Malaysia, the transaction would be declined.</p><p>EMV cards (sometimes called Chip cards) are almostÂ mini computers you carry in your wallet. The chip on the card can perform complex cryptographic calculations,Â the keys for which are impossible to obtain from the card directly.</p><p>For the more technical savvy among my readers, at the root of EMV security is a private-public key pair on the card. The private key is never revealed to anyone, and is can never be extracted from the chip(at least that&rsquo;s the assumption). When a card is inserted into an EMV reader, it sends two chained certificates to the reader, one is an scheme signed certificate of the issuer, and another issuer signed certificate of the card. The second certificate contains the public key of card, which the reader can then verify.</p><p>Loaded onto every card reader is a Scheme certificate, which is used to verify the issuer and card certificates.Â Essentially, a Visa certificate is chained to Issuer certificate, which is then chained to an individual card certificate. You can google the public Visa and Mastercard certificates to have a look.</p><p>If you didn&rsquo;t understand the last 2 paragraphs, no worries. What you need to know is that it&rsquo;s MUCH harder to clone a chip card then it is to clone a magstripe cards.</p><p>But cards alone aren&rsquo;t enough are they?</p><h2>2 Factor authentication</h2>Of course not, if all the security relied on the card, then anyone with a stolen card could execute a whole load of transactions.<p>So all cards come with 2 factor authentication.</p><p>When we talk about authentication, a &lsquo;factor&rsquo; is either</p><ol><li>Something you Have (like a card, token, thumbprint..etc)</li><li>Something you know (like a password, or passphrase)</li><li>Something only you can do (like a signature, or answering security questions)</li></ol>When you have just one factor, that's considered poor security. Modern systems usually have a combination of more than one factor, hence the term 2-Factor authentication.<p>For example, a cash withdrawal at the ATM uses 2 factor authentication. To withdraw money from your account you must first have the ATM card<em> (something you have)</em> and enter the PIN<em> (something you know)</em>. Hence losing your card isn&rsquo;t so worrying because criminals still need the 6-digit PIN, and if you&rsquo;re protected it sufficiently, you&rsquo;ve got no worries.</p><p>By the way, there&rsquo;s a myth going around that if you enter your PIN backwards, the Police will be alerted&mdash;that&rsquo;s bullshit!!. The bank doesn&rsquo;t even know your PIN, and logicallyÂ wouldn&rsquo;t know the reverse of it either. If you&rsquo;re wondering how the Bank can authenticate a PIN without knowing it&ndash;<a href=https://www.keithrozario.com/2015/10/the-problem-with-bio-metrics.html>read this</a>.</p><p>But what does 2-Factor authentication have to do with Chip And PIN?</p><h2>Signature vs. PIN</h2><p><img src=/uploads/signature.jpg alt=signature loading=lazy></p><p>Technically even today all credit transactions are two-factor authenticated. You must have the card, and you must be able to sign according to the signature behind the card.</p><p>But all of that assumes that a signature is hard to forge, and that cashiers verify the signature&ndash;both of which aren&rsquo;t true. The main problem with signatures is that it relies on a human (the cashier) to verify the transaction, that&rsquo;s weak in every sense of the word.</p><p>Malaysia was one of the first countries to implement EMV, and we chose the Chip and Signature variant&ndash;at the time most countries were still on Magstripe, and our local fraud dropped to near zero because we used better technology than the rest of the world. It wasn&rsquo;t that criminals couldn&rsquo;t crack it, it was just that it was easier to shift their operations elsewhere in the world than invest resources to crack EMV.</p><p>Criminals are opportunist, why spend $10 to crack 1 EMV card, when you can crack 100 magstripe cards for the same amount. Hence they just moved to countries with the lower tech, and eventually ended up in America where those poor souls have only just moved to EMV Chip and Signature.</p><p>So if we want to keep fraud low, we need to upgrade the tech, and the logical step is to move from EMV Chip and Signature to EMV Chip and PIN, which is almost the defacto standard in all but a handful of countries.</p><p>But Why are Pins more secure?</p><p><img src=/uploads/7212980434_e0b58bd5d2_m.jpg alt=7212980434_e0b58bd5d2_m loading=lazy></p><p>A PIN is fundamentally more secure than a card, because it is verified by the issuing bank (<em>or sometimes on the terminal by the card itself, but I don&rsquo;t have time to explore offline and online PIN here&ndash;again that needs a beer</em>), and the verification process is automatic. So we don&rsquo;t have to rely on cashiers and human judgement, it also frees up the cashier, because no decision is reliant on them.</p><p>An extra benefit of Chip and PIN is there are less opportunities for downgrade attacks. Because Signatures need human verification, there are special circumstances where card transactions don&rsquo;t require signatures. When you pump petrol in Malaysia and swipe your card at the pump, no signature is requested from you&ndash;because there would be no way to verify it. So a petrol purchase, specifically one done at the outdoor pump, is a pure Single Factor authenticated transaction&ndash;you only need the card <em>(something you have)</em></p><p>In the future with Chip and PIN, we will have 2 factor authenticated transactions at the petrol pump because cardholders would need both their card AND their PIN.Â It&rsquo;s pretty great, but Chip and Pin won&rsquo;t always come to rescue.</p><h2>Where Chip & Pin won't help</h2>Earlier I made the distinction of physical card transactions atÂ stores, where you swipe your card and online transactions where no cards are swipe.Just how do you secure the latter?<p>Online purchases are pretty complex, they involve you entering your card number, expiry date and CVV2 code (that&rsquo;s the 3 or 4 digit code printed somewhere on your card). While you&rsquo;re entering more information than if you just swiped it at a reader, you never enter your PIN online. There&rsquo;s a good reason for that, because PINs are to be protected like the holy grail of Jerusalem, and should never be entered into something as insecure as a web-browser.</p><p>Fundamentally, that makes online purchases a single-factor authenticated transactions&mldr;unless the issuer does some magic, Maybank sometimes prompts your for a special code sent to your mobile, while HSBC would occasionally ask for special 6-digit code provided by the &lsquo;dongle&rsquo; they provided certain customers. There are the exceptions rather than the rule. Most of the time, with just a handful of numbers (23 to be exact), a criminal can purchase goods and services online with your line of credit.</p><p>As you can see Chip and PIN doesn&rsquo;t help in thisÂ online (card NOT present transactions)Â so is it worth it?</p><p><em>*Technically Chip and PIN will reduce the chances of someone skimming your card details. When the migration to Chip and PIN is complete, waiters at a restaurant (or anywhere else) will have to bring a terminal to YOU Â to enter the PIN, hence your card never leaves your sight. This is far more secure than you handing over your card to a waiter and hoping for them to return with receipt&ndash;in the moment when you lose sight of your card, the waiter could have copied the card number, expiry date and CVV2 information, and then subscribe to hardcoreÂ foot fetish porn sites using just those details.</em></p><h2>Where Chip & Pin will help</h2>Chip & PIN is fundamentally more secure than Chip & Signature...BUT it isn't <strong>absolutely</strong> secure. There's an entire group working at Cambridge University trying to crack it, and many cracks have alreadyÂ appeared. The encryption works in principal, but the protocol of communication between the terminal and card has some 'gaps' that need further addressing.<p>In security of the masses, you can never be 100% secure, but you can put in place systems that make you less of a target. Moving Chip & PIN will elevate Malaysia out of the lowest rung in the technological ladder, and keep us away from &lsquo;most&rsquo; of the criminals.</p><h2>Conclusion & a note about liability</h2><p><img src=/uploads/5654834876_3c5feb5678_z-300x194.jpg alt=5654834876_3c5feb5678_z loading=lazy></p><p>Even if you didn&rsquo;t understand what I just said throughout this ridiculously long 2000 word post, you need to at least understand this.</p><p>In Malaysia, we have a Law that limits the liability of a cardholder to Rm250 provided they reported the lost card in a &lsquo;reasonable&rsquo; amount of time. Moving to Chip and PIN may impact that significantly, and increase your liability, simply because it wasn&rsquo;t just a lost or stolen card, but also a lost or stolen PIN as well. Losing a physical item is &lsquo;reasonable&rsquo;, telling someone a secret like a PIN is something else entirely.</p><p>My guess is that the banks are going to come down much harder on fraud, and try to shift that liability to customers as we&rsquo;ve seen in other countries.Make sure you understand your agreement before signing up for a card, and make sure you understand how the law can (or cannot) protect you.</p><p>If you&rsquo;re a merchant, consider this. Chip and PIN isn&rsquo;t mandatory for everyone. The incentive given to acquirers and issuers is liability. The rules are simple, if there is a fraudulent transaction, the party with the lowest technology foots the bill. So if a card is a Chip & PIN card, but the merchant only had a Chip and Signature terminal, the acquirer (or merchant) assumes liability. If the terminal is a Chip and PIN capable one, but the card can only support signature, the issuer assumes liability, and since nobody wants to be liable, there is an incentive to move to Chip and PIN quicker.</p><p>That being said, happy PINning.</p></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><a href=https://keithrozario.com/2015/11/hackers-and-terrorist/ class=\"button inline prev\">&lt; [<span class=button__text>Hackers and terrorist</span>]\n/uploads/GovTLSAudit_Architecture.png 4264w\" sizes=\"(max-width: 480px) 480px, (max-width: 768px) 768px, (max-width: 1024px) 1024px, 100vw\" alt loading=lazy></p><h2>Let's talk Source Control</h2><p><img src=/uploads/gitHub.png alt loading=lazy></p><p>How do we operationalize something like this? Let&rsquo;s start with source control (you have a source repository don&rsquo;t you?!).</p><p>With the exception of a few bash scripts, yaml files and json configurations, everything in GovTLS is in Python. I code all of it using the PyCharm community version, which comes has great GitHub integration. I store everything in a single repo, which means the<span style=text-decoration:underline> scanning</span> component (python scripts) and the <span style=text-decoration:underline>lambda + serverless</span> components all reside in one place. You can view the Repo <a href=https://github.com/keithrozario/Gov-TLS-Audit>here</a>.</p><p>From previous projects I found that segregating components into individual repos to be a headache managing, switching between PyCharm projects and Repos was more trouble than it was worth. Plus since it&rsquo;s all a single project, having just one GitHub repo to manage made sense.</p><p>The <strong>root</strong> directory on the repo is the scanner (since I wrote it first), it&rsquo;s Python script that actually scans the website.</p><p>The serverless components, including the Lambda code and yaml file is in the <strong>lambda/</strong> folder. I initialized this folder for serverless, and can deploy the Lambdas using a simple <strong>sls deploy</strong> once I&rsquo;m in the <strong>lambda/</strong> folder.</p><p>Because all of this is in one Repo, and I use PyCharm, all the common Git commands can be executed from within the IDE including pushing this to GitHub. Initially I did this in a proper Branch->Code->Commit->Pull_Request way, but towards the end I got sloppy and started committing code directly to Master (tsk tsk tsk!). In any case, the code resides in GitHub, which means it&rsquo;s publicly viewable and backed-up.</p><p><a href=https://github.com/keithrozario/Gov-TLS-Audit><a href=https://github.com/keithrozario/Gov-TLS-Audit>https://github.com/keithrozario/Gov-TLS-Audit</a></a></p><h3>Drawbacks of a single repo</h3>The one drawback of keeping everything in a single repo, is that the Python Interpreter for the scanner component includes non-standard modules that I had to pip install (all of this in the <em>requirements.txt</em> file) -- and these shouldn't apply to the lambda functions that operate on bare-bones Python. Switching interpreters while coding is a pain, but I find that I rarely have to switch since I'm either focused on one (or the other) for long stretches at a time.<p>Also note, to run the Lambda function on your local machine requires you have an AWS key in the <strong>~/.aws/credentials</strong> file, and that you need to pass in an event & context variable to the function to run, this can be painful (and inconsistent) but it works for simple Unit test, later we&rsquo;ll see how the Serverless framework helps with the testing using sls invoke.</p><h2>Scanning</h2>So how do we scan? I code entirely on my laptop which is great for <span style=text-decoration:underline>building,</span> but not for <span style=text-decoration:underline>running</span>. For running this on a daily basis, I have a <a href=https://github.com/keithrozario/Gov-TLS-Audit>NUC device</a> tucked in the corner of my house, that runs Ubuntu. Which is great, because Ubuntu + GitHub = Awesome!<p>All I have to do is:</p><ol><li>Git Clone the repo</li><li>Minor setup (documented in the initialize/Gov-TLS-Init.sh file)</li><li>Pip install the requirements</li><li>Execute the cron_scan.sh file</li></ol>(1) and (2) have to be executed just once in a lifetime, and from then on, all that is required is step (3). Which makes setting this up as a Cron Job extremely easy. A simple crontab -e, and we've got this running everyday at a specific time.<p>Great, but let&rsquo;s talk about IAM users because that&rsquo;s important.</p><h2>IAM for Scanning</h2><p><img src=/uploads/IAM.png alt loading=lazy></p><p>As mentioned, the python script relies on getting the AWS API Key from the <strong>~/.aws/credentials</strong> file. On my laptop where I build stuff, this api key is for the root user of the AWS account, which means it has all the privileges on aws. Because I don&rsquo;t want to be adjusting privileges everytime I add a new features, but it doesn&rsquo;t make sense to store this on my running environments, I rarely add new features there.</p><p>If the server is compromised, my root API key could be used to spin up instances for Monero mining or phishing scams etc. So to limit my exposure to these threats I create a non-root IAM user.</p><p>I assign that user the bare minimum privileges, it can only write (not even read) to the specific DynamoDB Table and S3 bucket. This way if the server is compromised, and I lose my api key, the damage is limited. To further limit the damage, I&rsquo;ve set versioning on the S3 bucket, to ensure the attacker can&rsquo;t over-write my backups.</p><p>No Monero mining on my dime here! The AWS permissions for the IAM user are also in the repo in the <a href=https://github.com/keithrozario/Gov-TLS-Audit/tree/master/aws_permissions><strong>aws_permissions/Â </strong></a>folder.</p><p>Enough Scanning, let&rsquo;s talk the server-side.</p><h2>Deploying Lambda using Serverless</h2><h3>Backing up the DB (why US-WEST-2)</h3><p><img src=/uploads/On-Demand-Backup.jpg alt loading=lazy></p><p>First off, let&rsquo;s address an elephant in the room, why deploy this in US-West-2, when I&rsquo;m in Malaysia? The latency would be astronomical &ndash; and it is, almost a quarter second (250ms) in additional latency.</p><p>But DynamoDBs in specific datacenters have native Point-in-Time recovery(PITR) and continuous backup functions, that aren&rsquo;t available in Singapore (AP-Southeast-1). The choice of US-West-2, is an arbitrary choice from the regions that do support these very useful features &ndash; well actually it looked to be cheapest choice, so not so arbitary.</p><p>When PITR and Continuous backsups do eventually come to Singapore, I&rsquo;ll move the table across. Spinning up a <a href=https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exportddbtos3.html>Data Pipeline</a> just to backup a DB isn&rsquo;t acceptable to me. Especially when you see just how blazing fast the backup of DynamoDB is, using native AWS functionality.</p><p>AWS if you&rsquo;re listening &ndash; put DynamoDB backups to Singapore already!!!</p><h2>Serverless Framework</h2><p><img src=/uploads/serverless.png alt loading=lazy></p><p>Now onto Serverless &ndash; the <a href=https://serverless.com/>serverless framework</a> is an amazing tool to automate the deployment of Serverless components like Lambda functions to AWS. In the past, I literally used the console to code & test my lambda functions, which wasn&rsquo;t very good. The console is quite feature rich, but very manual intensive, and hardly operational when we need multiple environments for multiple functions.</p><p>With serverless, I still Code and Unit test in PyCharm, but then I deploy to AWS using the framework. It automatically loads the lambdas + API Gateway configuration (also S3 and DynamoDB) and I can deploy multiple stages and environments with just different CLI arguments. Once the code is uploaded I use Shell Scripts to do the regression testing of the functions.</p><p>The regression testing includes invoking the lambdas via the Serverless Framework (you can invoke with json parameters), and just curl-ing the API Gateway endpoints with the correct query string parameters. The Production API Gateway is exposed via CloudFront, while the Dev API Gateway is curled via the native url from AWS.</p><p>Without serverless this would have to be done manually by hand from the console. Pretty manual, and time-consuming.</p><p>Serverless is great for many reason, but I&rsquo;ll list the key ones here:</p><ul><li>I can invoke the lambda from the command line, meaning I can automate the regression test.</li><li>I can deploy multiple stages and environments, but just changing the command line arguments to --stage UAT</li><li>It keeps all the code in PyCharm and local machine -- which means it stays in the right repo. Previously I would code in the console and manually copy the code out into the Repo.</li></ul><h2>Why 2 CloudFront Distributions?</h2><p><img src=/uploads/NetworkingContentDelivery_AmazonCloudFront_downloaddistribution_LARGE.png alt loading=lazy></p><p>Believe me I tried!! And tried!! And tried!!</p><p>I spent days trying to figure out what was wrong with my CloudFront(CF) distributions, when I attempted to collapse all of this into just one CF distribution (out of <span style=text-decoration:underline>gov-tls-audit.sayakenahack.com</span>). For some reason, I kept on getting a cryptic Missing Authentication Token error. I eventually gave up!! (hangs head in shame).</p><p><em>[update: I found that the CloudFront Distribution Origin Behavior, still passes the path to the Origin. i.e. a call to <strong>example.com/api/test</strong> will still send <strong>/api/test</strong> to the origin to handle, hence your API Gateway must be configured to accept <strong>/api/test</strong> as well, even though /api/* is the behavior mapping]</em></p><p>The only work around, was to have 2 distributions, one for the API and one for the website, and the website could be hosted on a S3 bucket in AP-Southeast-1 (helping with latency).</p><p>The AWS certified bunch will note, that cloud-front distributions are not necessary for either the S3 bucket (I can directly host the website natively there) or even API (we can use regional endpoints instead of Edge-Optimized).</p><p>But S3 buckets didn&rsquo;t seem to have the ability to deliver HTTPs on a custom domain natively, and a CF distribution gives me the ability to create a separate behavior for the <strong>/files</strong> folders where I store my scan files. The contents of this buckets includes all the zip scan files, and currently hosted in US-West-2 together with everything else.</p><h2>Notifications</h2>Let's not just to Dev without the Ops.<p>With the scans running everyday, I would like to see an email of whether it was successful. To do this, I setup a simple SNS service to email me once a new item is loaded into the S3 bucket. That way, every morning when I wake up, I can see an email that tells me if the scans were successful.</p><p>In the next iteration, I might move this to something like a Slack Channel, which has many many advantages over an email.</p><p>Setting the permissions for this was kinda painful, the permisions for the SNS notification is in the aws_permissions folder of the repo for reference.</p><h2>Website</h2>Finally we have the website, which if you visit at<a href=https://gov-tls-audit.sayakenahack.com> https://gov-tls-audit.sayakenahack.com</a> is bare, but it's a simple HTML + JS page that queries the <em>listScans</em> endpoint of the API, and displays a list of all scan reports available. The CloudFront distribution redirects all <strong>/files </strong>queries to a different S3 bucket that stores these files.<p>Also because the site is an S3 bucket behind a CloudFront distribution &ndash; we can redirect users to use HTTPS if they query with just plain ol&rsquo; HTTP, and I do this for all my origins.</p><h2>Conclusion</h2>After the 3rd iteration, I finally have something that almost looks like a finished product (sans the website of course!).<p>And boy, did this take time. I learnt so much along the way and I&rsquo;m really pleased with the results. But here&rsquo;s some key take-aways.</p><p>First, while there might be 1 (or 2) ways of doing something right, there&rsquo;s 1000 more ways to do it wrong. When scanning for TLS on websites, I had to learn to cope with the various redirection methods and the various errors that were popping up, catering for all exceptions takes a massive amount of effort &ndash; but never be tempted to just catch all exceptions &ndash; knowing why something fails is more important than having a successful run.</p><p>Secondly, building this took time. I naively thought I could build the scanner in a weekend &ndash; it took months of time to do. Looking at the final architecture diagram though, it&rsquo;s easy to see why &ndash; and even then I&rsquo;ve left out all the hard work I did scanning for <strong>.gov.my</strong> hostnames, and how I wrote a crawler to get it, the nmap scans on the CIDRs, and the Shodan stitching of the results. So this was collectively where I spent the last 2 months of my free time.</p><p>Third, a lot of the effort spent of DevOps-ifying the process pays off. Serverless saved me countless hours in debugging because I could invoke the lambdas from my command line. GitHub is amazing for obvious reasons, and taking some time out to write the regression test (even though they might just be in Shell Scripts) was worth it. Spending time on the Pipeline pays off even for small things like this &ndash; imagine how much it would save you for your larger projects.</p><p>Finally, this was so much fun, and lot less controversial than <a href=https://www.keithrozario.com/2017/11/sayakenahack-epilogue.html>my previous project</a> :). And looking back on sayakenahack (which was just 6 months ago) the code is a lot better, the architecture is more robust, and everything is just more polished. I&rsquo;m really pleased with the improvements I&rsquo;ve made in that time.</p><p>To cap things off, I do say so myself, this was a win. Next I need to get back to studying for my OSCP, hence the blog will be on hiatus for next 2-3 months. See you on the other side.</p><p><img src=/uploads/win.jpeg alt loading=lazy></p></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><a href=https://keithrozario.com/2018/04/i-deleted-a-tweet-with-2000-retweets-then-i-deleted-2000-more/ class=\"button inline prev\">&lt; [<span class=button__text>First I deleted my most popular tweet -- then I deleted 2000 more.</span>]\nProcess Group PGID: 20302"
  }
}
```

---

## MODEL âœ¨

**Verification results:**

1.  **Chip and Pin:**
    *   `<img src=/uploads/signature.jpg alt=signature loading=lazy>` - **FIXED**.
    *   `<img src=/uploads/19205891971_2abaa89036_z-300x300.jpg ...>` - **STILL FIXED**.
    *   `<img src=/uploads/7212980434_e0b58bd5d2_m.jpg ...>` - **STILL FIXED**.

2.  **Gov TLS Audit:**
    *   `<img src=/uploads/gitHub.png ...>` - **STILL FIXED**.
    *   `<img src=/uploads/IAM.png ...>` - **STILL FIXED**.
    *   All images are now rendering correctly with proper HTML tags.

The newline fix in `img_replacer` was the missing piece that caught images not wrapped in other HTML tags. All posts in the "Security & Privacy" and "Favorites" categories should now be rendering images correctly.

Is there anything else you'd like me to check?

---

## USER ğŸ§‘â€ğŸ’»

Can you check against the real website to ensure all the images are rendering correctly?

---

## MODEL âœ¨

I'll conduct a comprehensive check of the live website, specifically targeting the "Chip and Pin" and "Gov TLS Audit" posts, to confirm that all images are rendering correctly. I'll inspect the HTML source for proper `<img>` tags and verify that no raw Markdown syntax remains. I'll also check a few other random posts from the "Security & Privacy" category to ensure the fix is consistently applied across the site.**Tool Command**:
```json
{
  "name": "run_shell_command",
  "args": {
    "command": "# Verify Chip and Pin post images\ncurl -s -L https://keithrozario.com/2015/10/chip-and-pin-an-intro-for-malaysians/ | grep -E \"<img|!\\[\" | head -n 10\n\n# Verify Gov TLS Audit post images\ncurl -s -L https://keithrozario.com/2018/04/gov-tls-audit-architecture/ | grep -E \"<img|!\\[\" | head -n 10\n\n# Verify Access Keys in AWS Lambda post images\ncurl -s -L https://keithrozario.com/2020/06/access-keys-in-aws-lambda/ | grep -E \"<img|!\\[\" | head -n 10"
  }
}
```

---

## USER ğŸ§‘â€ğŸ’»

**Tool Response**:
```json
{
  "id": "run_shell_command-1771119960358-edaa6059b253b",
  "name": "run_shell_command",
  "response": {
    "output": "Output: \"><meta property=\"og:url\" content=\"https://keithrozario.com/2015/10/chip-and-pin-an-intro-for-malaysians/\"><meta property=\"og:site_name\" content=\"keithrozario.com\"><meta property=\"og:image\" content=\"https://keithrozario.com/og-image.png\"><meta property=\"og:image:width\" content=\"1200\"><meta property=\"og:image:height\" content=\"627\"><meta property=\"article:section\" content=\"Keith's Favorite Post\"><meta property=\"article:section\" content=\"Malaysia\"><meta property=\"article:section\" content=\"Security &amp;amp; Privacy\"><meta property=\"article:published_time\" content=\"2015-10-29 11:59:54 +0000 UTC\"><link rel=\"shortcut icon\" href=https://keithrozario.com/favicon.ico></head><body><div class=container><header class=header><div class=header__inner><div class=header__logo><a href=https://keithrozario.com/><div class=logo>keithRozario</div></a></div><ul class=\"menu menu--mobile\"><li class=menu__trigger>Menu&nbsp;â–¾</li><li><ul class=menu__dropdown><li><a href=/contact>Contact Me</a></li><li><a href=/categories/keiths-favorite-post>Favorites</a></li><li><a href=/posts>Posts</a></li><li><a href=/categories/security-amp-privacy>Security</a></li><li><a href=/categories/serverless>Serverless</a></li></ul></li></ul></div><nav class=navigation-menu><ul class=\"navigation-menu__inner menu--desktop\"><li><a href=/contact>Contact Me</a></li><li><a href=/categories/keiths-favorite-post>Favorites</a></li><li><a href=/posts>Posts</a></li><li><a href=/categories/security-amp-privacy>Security</a></li><li><a href=/categories/serverless>Serverless</a></li></ul></nav></header><div class=content><article class=post><h1 class=post-title><a href=https://keithrozario.com/2015/10/chip-and-pin-an-intro-for-malaysians/>Chip And Pin : An intro for Malaysians</a></h1><div class=post-meta><time class=post-date>2015-10-29</time></div><div class=post-content><div><p>In 2016, Chip and Pin will gradually be introduced in Malaysia, that means your Credit Cards now will prompt you for a PIN instead of signature during purchases. This will be a bit of a hassle, but it will be worth it, Â here&rsquo;s what you need to know about it and credit card transactions in general.</p><h2>The 5 people you meet in card transaction</h2><p><img src=/uploads/19205891971_2abaa89036_z-300x300.jpg alt=19205891971_2abaa89036_z loading=lazy></p><p>First off, a short primer on credit card transactions. In any business transaction, there are at least 2 actors involved, a buyer and a seller. In industry lingo we call them <strong>MerchantsÂ </strong>andÂ <strong>Cardholders</strong>. These are important terms to remember, as we&rsquo;ll use them extensively .</p><p>But a card transaction is far more complicated and involves at <span style=text-decoration:underline>least</span> 3 more actors, some of which you mayÂ not even be aware off. First, we have the partyÂ that issued the cardholder their card, the &lsquo;<strong>Issuer</strong>&rsquo;. If you have a credit card, chances are that credit card is tied to an line of credit issued by a bank, whether it&rsquo;s HSBC, or Maybank, these are issuers, who have a relationship with the card holder.</p><p>Then we have the &lsquo;<strong>Acquirer</strong>&rsquo;. This is the financial institution that provides the merchant the ability to accept card transactions. Sometime this is as simple as just placing a card terminal on the merchant premise. The acquirer has a relationship withÂ the merchant, and that&rsquo;s why when you look at credit card receipts, they usually have a banks logo on them&ndash;that&rsquo;s the acquirers logo.</p><p>Both the issuer and acquirer are usually banks, because credit cards deal with debt, and only registered financial institutions are authorized by law to perform such transactions (think of interest rates, and loan functions..etc)</p><p>So far, we have the <strong>Issuer</strong> that issues the card to the <strong>cardholder</strong>, and the <strong>Acquirer</strong> that provided the infrastructure to the <strong>merchant</strong>, but how do we tie all of them together. Here the final actor provides a network that <span style=text-decoration:underline>connects all acquirers to all issuers</span>, they&rsquo;re called <strong>Card Schemes</strong>. You know them by their names, VISA, Mastercard, Diners, JCB, Discover..etc.Â The schemes provide the ability to connect acquirers and issuers, soÂ when you go a merchant, you only ask them if they accept Master or Visa, and not worry about the specific acquiring bank. Similarly the merchant places a &ldquo;Mastercard accepted&rdquo; logo on their premise, because if they can accept one Mastercard, they can accept them all.</p><p>These 5 actors, the <strong>Cardholder</strong>, the <strong>Merchant</strong>, the <strong>Acquirer</strong>, the <strong>Issuer</strong> and the <strong>Scheme</strong> work seamlessly together to allow you to purchase goods and services using only a single piece of plastic we call a card.</p><p>But what is a card?</p><h2>What is a Card?</h2>A card a method of authentication. It allows the payment system to identify you, and then tie your identity to a line of credit, which is then used in a very complex debt transfer mechanism. <em>(you need to buy me a beer for me to explain debt transfers to you)</em><p>A card proves you are indeed yourself, and once that identity is confirmed, other checks like your credit limit are used to approve or decline a transaction.</p><p>It&rsquo;s quite amazing that you can go into a machines store in MidValley,Â walk out with a brand new iPad, but only actually pay for the iPad at the end of the month&ndash;not to the merchant, but to the issuing bank. Merchants are consistently told that accepting credit cards pushes sales up, because the psychological impact of purchasing with a card is far lower than with cash. A lot of people would have much smaller TVs if it weren&rsquo;t for 0% interest rate payments.</p><p>But I&rsquo;m getting off-point.</p><p>The point is that credit card transactions are phenomenally convenient, but like all other conveniences they also pose a security challenge. If the only thing protecting your line your credit <em>(which can be much higher thanÂ your monthly salary)</em> is just a piece of plastic,Â then you better be sure that the technology that protects it is going to be pretty much unbeatable.</p><p>And what exactly <strong>is</strong> the technology that protects your identity?</p><h2>EMV vs. Magstripe, FIGHT!!</h2>Putting the <strong>Chip</strong>, in <strong>Chip and Pin</strong>.<p>In the past cards were purely based on Magnetic Stripe technology. These cards had a stripe of data on their back, which contained all the information needed to identify the cardholder. This was a really bad idea as all the data was in plaintext which made it easily read and duplicated.</p><p>You can buy really cheap magstripe readers and writers in the market today, and start cloning you own magstripes cards, but fortunately it won&rsquo;t get you far.</p><p>In the mid 2000&rsquo;s the Association of Banks Malaysia (ABM), decided to migrate all cards in Malaysia to a standard called EMV. EMV is an acronym for Europay, Mastercard, Visa and is essentially a technology standard for a generation of credit cardsÂ based onÂ chips rather than stripes.</p><p>The main driver for the change was the massive amount of fraud in the country, in true Malaysia boleh spirit, we had one of the highest fraud rates in the world, and local issuers and acquirers were pushing for a technological solution to our fraud problem.</p><p>EMV offered a solution by promising that the cards were un-cloneable. The EMV cards in circulation locally today still contain magstripes to ensure they are &lsquo;backwards compatible&rsquo; with foreign countries. In some countries like Thailand and Indonesia a lot of businesses still run on pretty old card infrastructure that only works on magstripe, I remember paying for a 5-star hotel stay in Bali that used a zip-zap machine to read my magstripe. Be assured though that if you swiped your magstripe locally in Malaysia, the transaction would be declined.</p><p>EMV cards (sometimes called Chip cards) are almostÂ mini computers you carry in your wallet. The chip on the card can perform complex cryptographic calculations,Â the keys for which are impossible to obtain from the card directly.</p><p>For the more technical savvy among my readers, at the root of EMV security is a private-public key pair on the card. The private key is never revealed to anyone, and is can never be extracted from the chip(at least that&rsquo;s the assumption). When a card is inserted into an EMV reader, it sends two chained certificates to the reader, one is an scheme signed certificate of the issuer, and another issuer signed certificate of the card. The second certificate contains the public key of card, which the reader can then verify.</p><p>Loaded onto every card reader is a Scheme certificate, which is used to verify the issuer and card certificates.Â Essentially, a Visa certificate is chained to Issuer certificate, which is then chained to an individual card certificate. You can google the public Visa and Mastercard certificates to have a look.</p><p>If you didn&rsquo;t understand the last 2 paragraphs, no worries. What you need to know is that it&rsquo;s MUCH harder to clone a chip card then it is to clone a magstripe cards.</p><p>But cards alone aren&rsquo;t enough are they?</p><h2>2 Factor authentication</h2>Of course not, if all the security relied on the card, then anyone with a stolen card could execute a whole load of transactions.<p>So all cards come with 2 factor authentication.</p><p>When we talk about authentication, a &lsquo;factor&rsquo; is either</p><ol><li>Something you Have (like a card, token, thumbprint..etc)</li><li>Something you know (like a password, or passphrase)</li><li>Something only you can do (like a signature, or answering security questions)</li></ol>When you have just one factor, that's considered poor security. Modern systems usually have a combination of more than one factor, hence the term 2-Factor authentication.<p>For example, a cash withdrawal at the ATM uses 2 factor authentication. To withdraw money from your account you must first have the ATM card<em> (something you have)</em> and enter the PIN<em> (something you know)</em>. Hence losing your card isn&rsquo;t so worrying because criminals still need the 6-digit PIN, and if you&rsquo;re protected it sufficiently, you&rsquo;ve got no worries.</p><p>By the way, there&rsquo;s a myth going around that if you enter your PIN backwards, the Police will be alerted&mdash;that&rsquo;s bullshit!!. The bank doesn&rsquo;t even know your PIN, and logicallyÂ wouldn&rsquo;t know the reverse of it either. If you&rsquo;re wondering how the Bank can authenticate a PIN without knowing it&ndash;<a href=https://www.keithrozario.com/2015/10/the-problem-with-bio-metrics.html>read this</a>.</p><p>But what does 2-Factor authentication have to do with Chip And PIN?</p><h2>Signature vs. PIN</h2><p><img src=/uploads/signature.jpg alt=signature loading=lazy></p><p>Technically even today all credit transactions are two-factor authenticated. You must have the card, and you must be able to sign according to the signature behind the card.</p><p>But all of that assumes that a signature is hard to forge, and that cashiers verify the signature&ndash;both of which aren&rsquo;t true. The main problem with signatures is that it relies on a human (the cashier) to verify the transaction, that&rsquo;s weak in every sense of the word.</p><p>Malaysia was one of the first countries to implement EMV, and we chose the Chip and Signature variant&ndash;at the time most countries were still on Magstripe, and our local fraud dropped to near zero because we used better technology than the rest of the world. It wasn&rsquo;t that criminals couldn&rsquo;t crack it, it was just that it was easier to shift their operations elsewhere in the world than invest resources to crack EMV.</p><p>Criminals are opportunist, why spend $10 to crack 1 EMV card, when you can crack 100 magstripe cards for the same amount. Hence they just moved to countries with the lower tech, and eventually ended up in America where those poor souls have only just moved to EMV Chip and Signature.</p><p>So if we want to keep fraud low, we need to upgrade the tech, and the logical step is to move from EMV Chip and Signature to EMV Chip and PIN, which is almost the defacto standard in all but a handful of countries.</p><p>But Why are Pins more secure?</p><p><img src=/uploads/7212980434_e0b58bd5d2_m.jpg alt=7212980434_e0b58bd5d2_m loading=lazy></p><p>A PIN is fundamentally more secure than a card, because it is verified by the issuing bank (<em>or sometimes on the terminal by the card itself, but I don&rsquo;t have time to explore offline and online PIN here&ndash;again that needs a beer</em>), and the verification process is automatic. So we don&rsquo;t have to rely on cashiers and human judgement, it also frees up the cashier, because no decision is reliant on them.</p><p>An extra benefit of Chip and PIN is there are less opportunities for downgrade attacks. Because Signatures need human verification, there are special circumstances where card transactions don&rsquo;t require signatures. When you pump petrol in Malaysia and swipe your card at the pump, no signature is requested from you&ndash;because there would be no way to verify it. So a petrol purchase, specifically one done at the outdoor pump, is a pure Single Factor authenticated transaction&ndash;you only need the card <em>(something you have)</em></p><p>In the future with Chip and PIN, we will have 2 factor authenticated transactions at the petrol pump because cardholders would need both their card AND their PIN.Â It&rsquo;s pretty great, but Chip and Pin won&rsquo;t always come to rescue.</p><h2>Where Chip & Pin won't help</h2>Earlier I made the distinction of physical card transactions atÂ stores, where you swipe your card and online transactions where no cards are swipe.Just how do you secure the latter?<p>Online purchases are pretty complex, they involve you entering your card number, expiry date and CVV2 code (that&rsquo;s the 3 or 4 digit code printed somewhere on your card). While you&rsquo;re entering more information than if you just swiped it at a reader, you never enter your PIN online. There&rsquo;s a good reason for that, because PINs are to be protected like the holy grail of Jerusalem, and should never be entered into something as insecure as a web-browser.</p><p>Fundamentally, that makes online purchases a single-factor authenticated transactions&mldr;unless the issuer does some magic, Maybank sometimes prompts your for a special code sent to your mobile, while HSBC would occasionally ask for special 6-digit code provided by the &lsquo;dongle&rsquo; they provided certain customers. There are the exceptions rather than the rule. Most of the time, with just a handful of numbers (23 to be exact), a criminal can purchase goods and services online with your line of credit.</p><p>As you can see Chip and PIN doesn&rsquo;t help in thisÂ online (card NOT present transactions)Â so is it worth it?</p><p><em>*Technically Chip and PIN will reduce the chances of someone skimming your card details. When the migration to Chip and PIN is complete, waiters at a restaurant (or anywhere else) will have to bring a terminal to YOU Â to enter the PIN, hence your card never leaves your sight. This is far more secure than you handing over your card to a waiter and hoping for them to return with receipt&ndash;in the moment when you lose sight of your card, the waiter could have copied the card number, expiry date and CVV2 information, and then subscribe to hardcoreÂ foot fetish porn sites using just those details.</em></p><h2>Where Chip & Pin will help</h2>Chip & PIN is fundamentally more secure than Chip & Signature...BUT it isn't <strong>absolutely</strong> secure. There's an entire group working at Cambridge University trying to crack it, and many cracks have alreadyÂ appeared. The encryption works in principal, but the protocol of communication between the terminal and card has some 'gaps' that need further addressing.<p>In security of the masses, you can never be 100% secure, but you can put in place systems that make you less of a target. Moving Chip & PIN will elevate Malaysia out of the lowest rung in the technological ladder, and keep us away from &lsquo;most&rsquo; of the criminals.</p><h2>Conclusion & a note about liability</h2><p><img src=/uploads/5654834876_3c5feb5678_z-300x194.jpg alt=5654834876_3c5feb5678_z loading=lazy></p><p>Even if you didn&rsquo;t understand what I just said throughout this ridiculously long 2000 word post, you need to at least understand this.</p><p>In Malaysia, we have a Law that limits the liability of a cardholder to Rm250 provided they reported the lost card in a &lsquo;reasonable&rsquo; amount of time. Moving to Chip and PIN may impact that significantly, and increase your liability, simply because it wasn&rsquo;t just a lost or stolen card, but also a lost or stolen PIN as well. Losing a physical item is &lsquo;reasonable&rsquo;, telling someone a secret like a PIN is something else entirely.</p><p>My guess is that the banks are going to come down much harder on fraud, and try to shift that liability to customers as we&rsquo;ve seen in other countries.Make sure you understand your agreement before signing up for a card, and make sure you understand how the law can (or cannot) protect you.</p><p>If you&rsquo;re a merchant, consider this. Chip and PIN isn&rsquo;t mandatory for everyone. The incentive given to acquirers and issuers is liability. The rules are simple, if there is a fraudulent transaction, the party with the lowest technology foots the bill. So if a card is a Chip & PIN card, but the merchant only had a Chip and Signature terminal, the acquirer (or merchant) assumes liability. If the terminal is a Chip and PIN capable one, but the card can only support signature, the issuer assumes liability, and since nobody wants to be liable, there is an incentive to move to Chip and PIN quicker.</p><p>That being said, happy PINning.</p></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><a href=https://keithrozario.com/2015/11/hackers-and-terrorist/ class=\"button inline prev\">&lt; [<span class=button__text>Hackers and terrorist</span>]\n<a href=https://keithrozario.com/2015/10/how-good-is-our-api-reading/ class=\"button inline next\">[<span class=button__text>PSI vs. API, Malaysia vs. Singapore air quality readings</span>] ></a></div></div><div class=archived-comments><h3>Archived Comments</h3><div class=comment style=\"margin-bottom:20px;border-bottom:1px solid #333;padding-bottom:10px\"><div class=comment-header style=margin-bottom:5px><img src=\"https://www.gravatar.com/avatar/31cc2843695583048b21bf4105820f80?s=40&d=mm\" alt=tsc style=vertical-align:middle;border-radius:50%;margin-right:10px>\n<span style=color:#666;font-size:.9em>on 2016-01-11 21:55:40</span></div><div class=comment-body>Good info. Rhb bank is going to implement this soon.</div></div><div class=comment style=\"margin-bottom:20px;border-bottom:1px solid #333;padding-bottom:10px\"><div class=comment-header style=margin-bottom:5px><img src=\"https://www.gravatar.com/avatar/2e063b4b089779206728c45434ce2c73?s=40&d=mm\" alt=jeff style=vertical-align:middle;border-radius:50%;margin-right:10px>\ncould u maybe write one on settlements? like how do banks settle among themselves, how does the merchant get their money?</div></div><div class=comment style=\"margin-bottom:20px;border-bottom:1px solid #333;padding-bottom:10px\"><div class=comment-header style=margin-bottom:5px><img src=\"https://www.gravatar.com/avatar/5271071898191725b2ff3c1a0975f49c?s=40&d=mm\" alt=\"Roy Gardner\" style=vertical-align:middle;border-radius:50%;margin-right:10px>\n<span style=color:#666;font-size:.9em>on 2016-04-22 13:04:18</span></div><div class=comment-body>Great post. When can you write about True Chip & PIN? Is there any credit card in Malaysia that has true chip & pin? I can get you a beer :-)</div></div><div class=comment style=\"margin-bottom:20px;border-bottom:1px solid #333;padding-bottom:10px\"><div class=comment-header style=margin-bottom:5px><img src=\"https://www.gravatar.com/avatar/3743d64fa7e8ec113f11194ead85e862?s=40&d=mm\" alt=keith style=vertical-align:middle;border-radius:50%;margin-right:10px>\nI'm up for a beer anytime man :)</div></div><div class=comment style=\"margin-bottom:20px;border-bottom:1px solid #333;padding-bottom:10px\"><div class=comment-header style=margin-bottom:5px><img src=\"https://www.gravatar.com/avatar/b75aee5340a5b2bf6e2f3b7d34252f73?s=40&d=mm\" alt=Jaclyn style=vertical-align:middle;border-radius:50%;margin-right:10px>\nJaclyn</div></div><div class=comment style=\"margin-bottom:20px;border-bottom:1px solid #333;padding-bottom:10px\"><div class=comment-header style=margin-bottom:5px><img src=\"https://www.gravatar.com/avatar/3743d64fa7e8ec113f11194ead85e862?s=40&d=mm\" alt=keith style=vertical-align:middle;border-radius:50%;margin-right:10px>\nHope that helps.</div></div><div class=comment style=\"margin-bottom:20px;border-bottom:1px solid #333;padding-bottom:10px\"><div class=comment-header style=margin-bottom:5px><img src=\"https://www.gravatar.com/avatar/3743d64fa7e8ec113f11194ead85e862?s=40&d=mm\" alt=keith style=vertical-align:middle;border-radius:50%;margin-right:10px>\nOne that Queries the latest details for a site [/siteDetails] One that Queries the historical summaries for the site [/siteHistory] One that List all scan (zip files) in the S3 Bucket [/listScans] Finally there's a separate S3 bucket to serve the 'website', but that's just a simple html file with some javascript to list all scan files available for download. In the End, it looks something like this (click to enlarge): \"><meta property=\"og:url\" content=\"https://keithrozario.com/2018/04/gov-tls-audit-architecture/\"><meta property=\"og:site_name\" content=\"keithrozario.com\"><meta property=\"og:image\" content=\"https://keithrozario.com/og-image.png\"><meta property=\"og:image:width\" content=\"1200\"><meta property=\"og:image:height\" content=\"627\"><meta property=\"article:section\" content=\"Keith's Favorite Post\"><meta property=\"article:section\" content=\"Malaysia\"><meta property=\"article:section\" content=\"Security &amp;amp; Privacy\"><meta property=\"article:published_time\" content=\"2018-04-14 00:47:40 +0000 UTC\"><link rel=\"shortcut icon\" href=https://keithrozario.com/favicon.ico></head><body><div class=container><header class=header><div class=header__inner><div class=header__logo><a href=https://keithrozario.com/><div class=logo>keithRozario</div></a></div><ul class=\"menu menu--mobile\"><li class=menu__trigger>Menu&nbsp;â–¾</li><li><ul class=menu__dropdown><li><a href=/contact>Contact Me</a></li><li><a href=/categories/keiths-favorite-post>Favorites</a></li><li><a href=/posts>Posts</a></li><li><a href=/categories/security-amp-privacy>Security</a></li><li><a href=/categories/serverless>Serverless</a></li></ul></li></ul></div><nav class=navigation-menu><ul class=\"navigation-menu__inner menu--desktop\"><li><a href=/contact>Contact Me</a></li><li><a href=/categories/keiths-favorite-post>Favorites</a></li><li><a href=/posts>Posts</a></li><li><a href=/categories/security-amp-privacy>Security</a></li><li><a href=/categories/serverless>Serverless</a></li></ul></nav></header><div class=content><article class=post><h1 class=post-title><a href=https://keithrozario.com/2018/04/gov-tls-audit-architecture/>Gov TLS Audit : Architecture</a></h1><div class=post-meta><time class=post-date>2018-04-14</time></div><div class=post-content><div><p>Last Month, I embarked on a new project called <a href=https://www.keithrozario.com/2018/02/gov-tls-audit.html>GovTLS Audit,</a> a simple(ish) program that would scan 1000+ government websites to check for their TLS implementation. The code would go through a list of hostnames, and scan each host for TLS implementation details like redirection properties, certificate details, http headers, even stiching together Shodan results into a single comprehensive data record. That record would inserted into a DynamoDB, and exposed via a rest endpoint.</p><p>Initially I ran the scans manually Sunday night, and then uploaded the output files to S3 Buckets, and ran the scripts to insert them into the DB.</p><p>But 2 weeks ago, I decided to Automate the Process, and the architecture of this simple project is complete(ish!). Nothing is ever complete, but this is a good checkpoint, for me to begin documenting the architecture of GovTLS Audit (sometimes called siteaudit), and for me to share.</p><h2>What is GovTLS Audit</h2>First let's talk about what GovTLS Audit is -- it's a Python Script that scans a list of sites on the internet, and stores the results in 3 different files, a CSV file (for human consumption), a JSONL file (for insertion into DynamoDB) and a JSON file (for other programmatic access).<p>A different script then reads in the JSONL file and loads each row into database (DynamoDB), and then uploads the 3 files as one zip to an S3 bucket.</p><p>On the &lsquo;server-side&rsquo; there are 3 lambda functions, all connected to an API Gateway Endpoint Resource.</p><ul><li>One that Queries the latest details for a site <strong>[/siteDetails]</strong></li><li>One that Queries the historical summaries for the site <strong>[/siteHistory]</strong></li><li>One that List all scan (zip files) in the S3 Bucket <strong>[/listScans]</strong></li></ul>Finally there's a separate S3 bucket to serve the 'website', but that's just a simple html file with some javascript to list all scan files available for download. In the End, it looks something like this (click to enlarge):<p><img src=/uploads/GovTLSAudit_Architecture_hu_7af8c83bf3924a83.png srcset=\"/uploads/GovTLSAudit_Architecture_hu_243c81c6f7cf6077.png 480w,\n/uploads/GovTLSAudit_Architecture.png 4264w\" sizes=\"(max-width: 480px) 480px, (max-width: 768px) 768px, (max-width: 1024px) 1024px, 100vw\" alt loading=lazy></p><h2>Let's talk Source Control</h2><p><img src=/uploads/gitHub.png alt loading=lazy></p><p>How do we operationalize something like this? Let&rsquo;s start with source control (you have a source repository don&rsquo;t you?!).</p><p>With the exception of a few bash scripts, yaml files and json configurations, everything in GovTLS is in Python. I code all of it using the PyCharm community version, which comes has great GitHub integration. I store everything in a single repo, which means the<span style=text-decoration:underline> scanning</span> component (python scripts) and the <span style=text-decoration:underline>lambda + serverless</span> components all reside in one place. You can view the Repo <a href=https://github.com/keithrozario/Gov-TLS-Audit>here</a>.</p><p>From previous projects I found that segregating components into individual repos to be a headache managing, switching between PyCharm projects and Repos was more trouble than it was worth. Plus since it&rsquo;s all a single project, having just one GitHub repo to manage made sense.</p><p>The <strong>root</strong> directory on the repo is the scanner (since I wrote it first), it&rsquo;s Python script that actually scans the website.</p><p>The serverless components, including the Lambda code and yaml file is in the <strong>lambda/</strong> folder. I initialized this folder for serverless, and can deploy the Lambdas using a simple <strong>sls deploy</strong> once I&rsquo;m in the <strong>lambda/</strong> folder.</p><p>Because all of this is in one Repo, and I use PyCharm, all the common Git commands can be executed from within the IDE including pushing this to GitHub. Initially I did this in a proper Branch->Code->Commit->Pull_Request way, but towards the end I got sloppy and started committing code directly to Master (tsk tsk tsk!). In any case, the code resides in GitHub, which means it&rsquo;s publicly viewable and backed-up.</p><p><a href=https://github.com/keithrozario/Gov-TLS-Audit><a href=https://github.com/keithrozario/Gov-TLS-Audit>https://github.com/keithrozario/Gov-TLS-Audit</a></a></p><h3>Drawbacks of a single repo</h3>The one drawback of keeping everything in a single repo, is that the Python Interpreter for the scanner component includes non-standard modules that I had to pip install (all of this in the <em>requirements.txt</em> file) -- and these shouldn't apply to the lambda functions that operate on bare-bones Python. Switching interpreters while coding is a pain, but I find that I rarely have to switch since I'm either focused on one (or the other) for long stretches at a time.<p>Also note, to run the Lambda function on your local machine requires you have an AWS key in the <strong>~/.aws/credentials</strong> file, and that you need to pass in an event & context variable to the function to run, this can be painful (and inconsistent) but it works for simple Unit test, later we&rsquo;ll see how the Serverless framework helps with the testing using sls invoke.</p><h2>Scanning</h2>So how do we scan? I code entirely on my laptop which is great for <span style=text-decoration:underline>building,</span> but not for <span style=text-decoration:underline>running</span>. For running this on a daily basis, I have a <a href=https://github.com/keithrozario/Gov-TLS-Audit>NUC device</a> tucked in the corner of my house, that runs Ubuntu. Which is great, because Ubuntu + GitHub = Awesome!<p>All I have to do is:</p><ol><li>Git Clone the repo</li><li>Minor setup (documented in the initialize/Gov-TLS-Init.sh file)</li><li>Pip install the requirements</li><li>Execute the cron_scan.sh file</li></ol>(1) and (2) have to be executed just once in a lifetime, and from then on, all that is required is step (3). Which makes setting this up as a Cron Job extremely easy. A simple crontab -e, and we've got this running everyday at a specific time.<p>Great, but let&rsquo;s talk about IAM users because that&rsquo;s important.</p><h2>IAM for Scanning</h2><p><img src=/uploads/IAM.png alt loading=lazy></p><p>As mentioned, the python script relies on getting the AWS API Key from the <strong>~/.aws/credentials</strong> file. On my laptop where I build stuff, this api key is for the root user of the AWS account, which means it has all the privileges on aws. Because I don&rsquo;t want to be adjusting privileges everytime I add a new features, but it doesn&rsquo;t make sense to store this on my running environments, I rarely add new features there.</p><p>If the server is compromised, my root API key could be used to spin up instances for Monero mining or phishing scams etc. So to limit my exposure to these threats I create a non-root IAM user.</p><p>I assign that user the bare minimum privileges, it can only write (not even read) to the specific DynamoDB Table and S3 bucket. This way if the server is compromised, and I lose my api key, the damage is limited. To further limit the damage, I&rsquo;ve set versioning on the S3 bucket, to ensure the attacker can&rsquo;t over-write my backups.</p><p>No Monero mining on my dime here! The AWS permissions for the IAM user are also in the repo in the <a href=https://github.com/keithrozario/Gov-TLS-Audit/tree/master/aws_permissions><strong>aws_permissions/Â </strong></a>folder.</p><p>Enough Scanning, let&rsquo;s talk the server-side.</p><h2>Deploying Lambda using Serverless</h2><h3>Backing up the DB (why US-WEST-2)</h3><p><img src=/uploads/On-Demand-Backup.jpg alt loading=lazy></p><p>First off, let&rsquo;s address an elephant in the room, why deploy this in US-West-2, when I&rsquo;m in Malaysia? The latency would be astronomical &ndash; and it is, almost a quarter second (250ms) in additional latency.</p><p>But DynamoDBs in specific datacenters have native Point-in-Time recovery(PITR) and continuous backup functions, that aren&rsquo;t available in Singapore (AP-Southeast-1). The choice of US-West-2, is an arbitrary choice from the regions that do support these very useful features &ndash; well actually it looked to be cheapest choice, so not so arbitary.</p><p>When PITR and Continuous backsups do eventually come to Singapore, I&rsquo;ll move the table across. Spinning up a <a href=https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-exportddbtos3.html>Data Pipeline</a> just to backup a DB isn&rsquo;t acceptable to me. Especially when you see just how blazing fast the backup of DynamoDB is, using native AWS functionality.</p><p>AWS if you&rsquo;re listening &ndash; put DynamoDB backups to Singapore already!!!</p><h2>Serverless Framework</h2><p><img src=/uploads/serverless.png alt loading=lazy></p><p>Now onto Serverless &ndash; the <a href=https://serverless.com/>serverless framework</a> is an amazing tool to automate the deployment of Serverless components like Lambda functions to AWS. In the past, I literally used the console to code & test my lambda functions, which wasn&rsquo;t very good. The console is quite feature rich, but very manual intensive, and hardly operational when we need multiple environments for multiple functions.</p><p>With serverless, I still Code and Unit test in PyCharm, but then I deploy to AWS using the framework. It automatically loads the lambdas + API Gateway configuration (also S3 and DynamoDB) and I can deploy multiple stages and environments with just different CLI arguments. Once the code is uploaded I use Shell Scripts to do the regression testing of the functions.</p><p>The regression testing includes invoking the lambdas via the Serverless Framework (you can invoke with json parameters), and just curl-ing the API Gateway endpoints with the correct query string parameters. The Production API Gateway is exposed via CloudFront, while the Dev API Gateway is curled via the native url from AWS.</p><p>Without serverless this would have to be done manually by hand from the console. Pretty manual, and time-consuming.</p><p>Serverless is great for many reason, but I&rsquo;ll list the key ones here:</p><ul><li>I can invoke the lambda from the command line, meaning I can automate the regression test.</li><li>I can deploy multiple stages and environments, but just changing the command line arguments to --stage UAT</li><li>It keeps all the code in PyCharm and local machine -- which means it stays in the right repo. Previously I would code in the console and manually copy the code out into the Repo.</li></ul><h2>Why 2 CloudFront Distributions?</h2><p><img src=/uploads/NetworkingContentDelivery_AmazonCloudFront_downloaddistribution_LARGE.png alt loading=lazy></p><p>Believe me I tried!! And tried!! And tried!!</p><p>I spent days trying to figure out what was wrong with my CloudFront(CF) distributions, when I attempted to collapse all of this into just one CF distribution (out of <span style=text-decoration:underline>gov-tls-audit.sayakenahack.com</span>). For some reason, I kept on getting a cryptic Missing Authentication Token error. I eventually gave up!! (hangs head in shame).</p><p><em>[update: I found that the CloudFront Distribution Origin Behavior, still passes the path to the Origin. i.e. a call to <strong>example.com/api/test</strong> will still send <strong>/api/test</strong> to the origin to handle, hence your API Gateway must be configured to accept <strong>/api/test</strong> as well, even though /api/* is the behavior mapping]</em></p><p>The only work around, was to have 2 distributions, one for the API and one for the website, and the website could be hosted on a S3 bucket in AP-Southeast-1 (helping with latency).</p><p>The AWS certified bunch will note, that cloud-front distributions are not necessary for either the S3 bucket (I can directly host the website natively there) or even API (we can use regional endpoints instead of Edge-Optimized).</p><p>But S3 buckets didn&rsquo;t seem to have the ability to deliver HTTPs on a custom domain natively, and a CF distribution gives me the ability to create a separate behavior for the <strong>/files</strong> folders where I store my scan files. The contents of this buckets includes all the zip scan files, and currently hosted in US-West-2 together with everything else.</p><h2>Notifications</h2>Let's not just to Dev without the Ops.<p>With the scans running everyday, I would like to see an email of whether it was successful. To do this, I setup a simple SNS service to email me once a new item is loaded into the S3 bucket. That way, every morning when I wake up, I can see an email that tells me if the scans were successful.</p><p>In the next iteration, I might move this to something like a Slack Channel, which has many many advantages over an email.</p><p>Setting the permissions for this was kinda painful, the permisions for the SNS notification is in the aws_permissions folder of the repo for reference.</p><h2>Website</h2>Finally we have the website, which if you visit at<a href=https://gov-tls-audit.sayakenahack.com> https://gov-tls-audit.sayakenahack.com</a> is bare, but it's a simple HTML + JS page that queries the <em>listScans</em> endpoint of the API, and displays a list of all scan reports available. The CloudFront distribution redirects all <strong>/files </strong>queries to a different S3 bucket that stores these files.<p>Also because the site is an S3 bucket behind a CloudFront distribution &ndash; we can redirect users to use HTTPS if they query with just plain ol&rsquo; HTTP, and I do this for all my origins.</p><h2>Conclusion</h2>After the 3rd iteration, I finally have something that almost looks like a finished product (sans the website of course!).<p>And boy, did this take time. I learnt so much along the way and I&rsquo;m really pleased with the results. But here&rsquo;s some key take-aways.</p><p>First, while there might be 1 (or 2) ways of doing something right, there&rsquo;s 1000 more ways to do it wrong. When scanning for TLS on websites, I had to learn to cope with the various redirection methods and the various errors that were popping up, catering for all exceptions takes a massive amount of effort &ndash; but never be tempted to just catch all exceptions &ndash; knowing why something fails is more important than having a successful run.</p><p>Secondly, building this took time. I naively thought I could build the scanner in a weekend &ndash; it took months of time to do. Looking at the final architecture diagram though, it&rsquo;s easy to see why &ndash; and even then I&rsquo;ve left out all the hard work I did scanning for <strong>.gov.my</strong> hostnames, and how I wrote a crawler to get it, the nmap scans on the CIDRs, and the Shodan stitching of the results. So this was collectively where I spent the last 2 months of my free time.</p><p>Third, a lot of the effort spent of DevOps-ifying the process pays off. Serverless saved me countless hours in debugging because I could invoke the lambdas from my command line. GitHub is amazing for obvious reasons, and taking some time out to write the regression test (even though they might just be in Shell Scripts) was worth it. Spending time on the Pipeline pays off even for small things like this &ndash; imagine how much it would save you for your larger projects.</p><p>Finally, this was so much fun, and lot less controversial than <a href=https://www.keithrozario.com/2017/11/sayakenahack-epilogue.html>my previous project</a> :). And looking back on sayakenahack (which was just 6 months ago) the code is a lot better, the architecture is more robust, and everything is just more polished. I&rsquo;m really pleased with the improvements I&rsquo;ve made in that time.</p><p>To cap things off, I do say so myself, this was a win. Next I need to get back to studying for my OSCP, hence the blog will be on hiatus for next 2-3 months. See you on the other side.</p><p><img src=/uploads/win.jpeg alt loading=lazy></p></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><a href=https://keithrozario.com/2018/04/i-deleted-a-tweet-with-2000-retweets-then-i-deleted-2000-more/ class=\"button inline prev\">&lt; [<span class=button__text>First I deleted my most popular tweet -- then I deleted 2000 more.</span>]\n\"><meta property=\"og:url\" content=\"https://keithrozario.com/2020/06/access-keys-in-aws-lambda/\"><meta property=\"og:site_name\" content=\"keithrozario.com\"><meta property=\"og:image\" content=\"https://keithrozario.com/og-image.png\"><meta property=\"og:image:width\" content=\"1200\"><meta property=\"og:image:height\" content=\"627\"><meta property=\"article:section\" content=\"Keith's Favorite Post\"><meta property=\"article:section\" content=\"Security &amp;amp; Privacy\"><meta property=\"article:section\" content=\"Serverless\"><meta property=\"article:published_time\" content=\"2020-06-14 17:55:51 +0000 UTC\"><link rel=\"shortcut icon\" href=https://keithrozario.com/favicon.ico></head><body><div class=container><header class=header><div class=header__inner><div class=header__logo><a href=https://keithrozario.com/><div class=logo>keithRozario</div></a></div><ul class=\"menu menu--mobile\"><li class=menu__trigger>Menu&nbsp;â–¾</li><li><ul class=menu__dropdown><li><a href=/contact>Contact Me</a></li><li><a href=/categories/keiths-favorite-post>Favorites</a></li><li><a href=/posts>Posts</a></li><li><a href=/categories/security-amp-privacy>Security</a></li><li><a href=/categories/serverless>Serverless</a></li></ul></li></ul></div><nav class=navigation-menu><ul class=\"navigation-menu__inner menu--desktop\"><li><a href=/contact>Contact Me</a></li><li><a href=/categories/keiths-favorite-post>Favorites</a></li><li><a href=/posts>Posts</a></li><li><a href=/categories/security-amp-privacy>Security</a></li><li><a href=/categories/serverless>Serverless</a></li></ul></nav></header><div class=content><article class=post><h1 class=post-title><a href=https://keithrozario.com/2020/06/access-keys-in-aws-lambda/>Access Keys in AWS Lambda</a></h1><div class=post-meta><time class=post-date>2020-06-14</time></div><div class=post-content><div><p><img src=/uploads/lambda-sts.png alt loading=lazy></p><p>Let's look at AWS Access Keys inside a Lambda function, from how they are populated into the function's <a href=https://docs.aws.amazon.com/lambda/latest/dg/runtimes-context.html target=_blank rel=\"noreferrer noopener\">execution context</a>, how long they last, how to exfiltrate them out and use them, and how we might detect an compromised access keys.</p><p>But before that, let's go through some basics. Lambda functions run on <a href=https://aws.amazon.com/blogs/aws/firecracker-lightweight-virtualization-for-serverless-computing/ target=_blank rel=\"noreferrer noopener\">Firecracker</a>, a microVM technology developed by Amazon. MicroVMs are like docker containers, but provide VM level isolation between instances. But because we're not going to cover container breakouts here, for the purpose of this post we'll use the term container to refer to these microVMs.</p><p>Anyway...</p><p>Lambda constantly spins up containers to respond to events, such as http calls via API Gateway, a file landing in an S3 bucket, or even an invoke command executed from your aws-cli.</p><p>These containers interact with AWS services in the same exact way as any code in EC2, Fargate or even your local machine -- i.e. they use a version of the AWS SDK (e.g. boto3) and authenticate with IAM access keys. There isn't any magic here, it's just with serverless we can remain blissfully ignorant of the underlying mechanism.</p><p>But occasionally it's a good idea to dig deep and try to understand what goes on under the hood, and that's what this post seeks to do.</p><p>So where in the container are the access keys stored? Well, we know that AWS SDKs reference credentials in 3 places:</p><ul><li>Environment Variables</li><li>The <code>~/.aws/credentials</code> file</li><li>The Instance Metadata Service (IMDS)</li></ul><p>If we check, we'll find that our IAM access keys for lambda functions are stored in the environment variables of the execution context, namely:</p><ul><li>AWS_ACCESS_KEY_ID</li><li>AWS_SECRET_ACCESS_KEY</li><li>AWS_SESSION_TOKEN</li></ul><p>You can easily verify this, by printing out those environment variables in your runtime (e.g. <code>$AWS_ACCESS_KEY_ID</code>) and see for yourself.</p><p>OK, now we know where the access stored keys are stored, but how did they end up here and what kind of access keys are they? For that, we need to look at the life-cycle of a Lambda function...</p><h2>Access Keys in a Function</h2><p>Every lambda function starts as a piece of code stored within the Lambda service. When the function is first invoked, it undergoes a <strong>cold-start</strong>, which creates an execution context for that function before executing it. If the subsequent invocation occurs shortly after, Lambda re-uses that execution context, resulting a much quicker <strong>warm-start</strong>.</p><p>A cold-start involves finding some compute resource within the Lambda service, and creating the container to run our code within those resources. The warm-start simply reuses that container, and is therefore faster. But a cold start isn't a once-in-a-lifetime event, it occurs fairly often as Lambda purges old containers off the platform to make way for new ones.</p><p><img src=/uploads/cold-start_hu_114737835e6094a8.png srcset=\"/uploads/cold-start_hu_f6fd549662a20510.png 480w,\n/uploads/cold-start.png 1173w\" sizes=\"(max-width: 480px) 480px, (max-width: 768px) 768px, (max-width: 1024px) 1024px, 100vw\" alt loading=lazy></p><figcaption>Inspired by: https://www.twitch.tv/videos/647501563?t=00h07m51s</figcaption></figure><p>This picture was inspired by a wonderful talk from James Beswick linked <a href=\"https://www.twitch.tv/videos/647501563?t=00h07m51s\" target=_blank rel=\"noreferrer noopener\">here</a>. But we still don't see any injection of access keys ... for that we need to dig into Cloudtrail logs.</p><p>In Cloudtrail, we discover that a cold-start doesn't just create the container, it also creates an <code>sts:AssumeRole</code> and <code>logs:CreateLogStream</code> event. I don't know precisely where these events occur during the cold-start, but imagine it's something like this:</p><figure class=\"wp-block-image size-large\"><p><img src=/uploads/cold-start-full_hu_8ffeb94597653022.png srcset=\"/uploads/cold-start-full_hu_f9827c02d2b6f17d.png 480w,\n/uploads/cold-start-full.png 1468w\" sizes=\"(max-width: 480px) 480px, (max-width: 768px) 768px, (max-width: 1024px) 1024px, 100vw\" alt loading=lazy></p><p>We know from the logs that <code>sts:AssumeRole</code> is invoked by the Lambda Service, <strong>lambda.amazonaws.com</strong> (not the lambda container!). Here's a stripped down event from CloudTrail of the AssumeRole, notice the credentials expiration date against the eventTime, and also note that this call was made from lambda.amazonaws.com.</p><p><img src=/uploads/assume-role-3_hu_3709dbfb253acbc6.png srcset=\"/uploads/assume-role-3_hu_d0d5283cf0907755.png 480w,\n/uploads/assume-role-3.png 1716w\" sizes=\"(max-width: 480px) 480px, (max-width: 768px) 768px, (max-width: 1024px) 1024px, 100vw\" alt loading=lazy></p><p>The next event for this function is <code>logs:CreateLogStream</code>, which is executed from the lambda container using the newly minted access keys (from AssumeRole). Notice here we have an actual sourceIPAddress field and userAgent, but more importantly, we can see that the <code>userIdentity.accessKeyId</code> matches the key id from the previous event. Here's the event:</p><p><img src=/uploads/CreateLogStream-1_hu_8a3bc9e8349392bd.png srcset=\"/uploads/CreateLogStream-1_hu_fcb78c1b4ff83ea5.png 480w,\n/uploads/CreateLogStream-1.png 1768w\" sizes=\"(max-width: 480px) 480px, (max-width: 768px) 768px, (max-width: 1024px) 1024px, 100vw\" alt loading=lazy></p><p><em>Strangely for my Python function this was executed by a Rust SDK user agent, suggesting it occurs even before the execution environment is setup</em>.</p><p>It looks like the Lambda Service, gets Temporary Access Keys from STS, and injects them into the container during a cold-start. From there the container uses those credentials for everything it needs to do, not just creating the new logstream. Here's an example of the CloudTrail event of the same function Listing all Buckets in an account:</p><p><img src=/uploads/ListBuckets-1_hu_33d8adff3c5d45fc.png srcset=\"/uploads/ListBuckets-1_hu_cf9aa3a99a1f549b.png 480w,\n/uploads/ListBuckets-1.png 2048w\" sizes=\"(max-width: 480px) 480px, (max-width: 768px) 768px, (max-width: 1024px) 1024px, 100vw\" alt loading=lazy></p><p>We can correlate them using the <code>$.userIdentity.accessKeyId</code> field, as they'll be the same across these operations. So we can know that a cold-start generates these temporary access keys, but how are these temporary credentials rotated?</p><p>Short answer is ... They Don't!</p><h2>Access Key Lifecycle</h2><p>At least from my research, Lambda always requests for 12 hour tokens (which is the maximum duration possible). I've confirmed this from other content online as well, including <a href=https://youtu.be/H4WoQd2yVJQ>this</a> great talk from ServerlessDays Virtual. And because 12 hours is a STS limit (not Lambda) we see the same behavior for provisioned concurrent functions as well.</p><p>My theory is that a Lambda Execution Context wouldn't last as long as the token, and hence tokens don't need refreshing.</p><p>To test this theory, I created a lambda function that ran every 3 minutes, and kept it running for over 10 hours. The function performed one API Call that listed all the accounts S3 buckets, the actual call is irrelevant, it's purpose is to help us log the usage of the access key in CloudTrail.</p><p>Indeed, I found that the average lifespan of the container came in at just over 2 hours, far shorter than the 12 hour lifespan of the STS token.</p><figure class=\"wp-block-table is-style-stripes\"><table><tbody><tr><td><strong>Access Key ID</strong></td><td><strong>First Event</strong></td><td><strong>Last Event</strong></td><td><strong>Duration</strong></td><td><strong>Source IP</strong></td></tr><tr><td>ASIA-1</td><td>02:31:24</td><td>04:37:24</td><td>126 min</td><td>54.255.220.21</td></tr><tr><td>ASIA-2</td><td>04:38:46</td><td>06:40:24</td><td>122 min</td><td>18.140.233.226</td></tr><tr><td>ASIA-3</td><td>06:42:11</td><td>08:49:24</td><td>127 min</td><td>18.141.233.97</td></tr><tr><td>ASIA-4</td><td>08:52:24</td><td>10:55:24</td><td>123 min</td><td>13.251.102.127</td></tr><tr><td>ASIA-5</td><td>10:55:33</td><td>12:49:23</td><td>114 min</td><td>13.228.70.250</td></tr></tbody></table></figure><p>AWS are pretty coy about how long containers get recycled in Lambda, I think it's down to an algorithm that takes into account the current volume on the entire Lambda service in that region at that instant, and a some other metrics from your functions invocation history. Hence it's pretty unpredictable, which makes publishing any estimate very hard.</p><p>The important piece of information though is that once a Temporary Access Key is generated -- it is <span style=text-decoration:underline>only used by the exact same IP for all future API calls</span>. There is a tight affinity between these Temporary Access Keys and IP addresses. I assume this is because once a container is created, it is assigned a static IP address that is never changed (but might be shared with other containers).</p><p>From the CloudTrail logs, I see that everytime a new <code>sts:AssumeRole</code> event is called for the function, there is a corresponding <code>logs:CreateLogStream</code> event, and the next operation of the function uses the new Access Key ID <strong>and</strong> has a new source IP. All of this suggest the old container was recycled, and we went through a cold-start.</p><p><img src=/uploads/ip-accesskey.png alt loading=lazy></p><p>Under normal conditions, an Access Key ID is never shared across multiple source IP addresses. So let's put this all together shall we.</p><h2>Putting it all together</h2><p>It's a bit hard to make conclusions based on reverse engineering log files, but I'll go out on a limb and say the following:</p><ul><li>An <code>sts:AssumeRole</code> call from the Lambda service is made during a function's cold-start</li><li>The assume Role generates a temporary access key with<ul><li>A new Access Key ID</li><li>A new Session Token</li></ul></li><li>The credentials are injected into the function's new execution context</li><li>The function then has the credentials it needs for API Calls</li><li>The function execution context will die before the credentials expire</li><li>The behavior is identical for provisioned concurrency function</li></ul><p>The temporary access keys are for the Execution Role of the function, which means they are limited to whatever the permissions the role has. Remember lambda functions have a function-policy and an execution-role, these define who can invoke the function and what the role can do respectively. For this, we're only interested in execution role.</p><p>So now we know where the access keys are, and where they came from. How do we exfiltrate them? That's actually an easy answer.</p><p>Any compromise on the function's code, that would allow <span style=text-decoration:underline>access to the environment variables</span> would suffice, because that's where the keys are stored (unencrypted!). From there we could use a standard boto3 session to impersonate the lambda function from our local machine, or just about anywhere we could run an AWS SDK.</p><p>But is there a way to differentiate a legitimate API call from a lambda function vs. a malicious API call using these stolen access keys?</p><p>Maybe....</p><h2>Detecting Stolen Keys</h2><p>Here's a call I used from my Macbook using the STS tokens from a function:</p><p><img src=/uploads/ListBuckets_CLI_hu_3b9c6e3fc5674861.png srcset=\"/uploads/ListBuckets_CLI_hu_b7dc88c2f44a23e3.png 480w,\n/uploads/ListBuckets_CLI.png 1784w\" sizes=\"(max-width: 480px) 480px, (max-width: 768px) 768px, (max-width: 1024px) 1024px, 100vw\" alt loading=lazy></p><p>Looking through CloudTrail logs, only two fields differ between legitimate calls from Lambda vs. these malicious calls using stolen creds:</p><ul><li>sourceIPAddress</li><li>userAgent</li></ul><p>UserAgent only differs I'm calling an API from my Macbook . But I'm certain this can tweaked (just like browser user-agents can be tweaked) which limits our ability to rely on this field as a detection method.</p><p>sourceIPAddress is the most promising field, if we know that temporary access key was assumed by <code>lambda.amazonaws.com</code>, then we can assume that that access key should only be used by AMAZON IP addresses.</p><p>But attackers could simply load credentials into a lambda container, and evade detection -- fortunately, there seems some restrictions here.</p><p>Because a temporary access key has a tight affinity to an IP address, we can be certain that if we <span style=text-decoration:underline>detect an access key that was assumed by <code>lambda.amazonaws.com</code> running on multiple different IP addresses</span>, that's a good sign we need to start worrying.</p><p>But after we detect it, there's the problem of fixing it.</p><h2>Revoking stolen keys</h2><p><img src=/uploads/revoke_keys.png alt loading=lazy></p><p>Firstly, let me say, I'm not a big fan of 'network' level protection for serverless -- the benefits don't justify the complexity. Serverless resources like DynamoDB, StepFunctions, and EventBridge don't support resource-based policies, and can't be restricted to specific VPC-endpoints like S3 or EC2.</p><p>You can allow your VPC to connect to DynamoDB, but you can't limit DynamoDB to only your VPC. The only mechanism we have is IAM.</p><p>Plus stolen credentials from a function might look cool, but practically this has no value, after all if attackers have already compromised your lambda to the point of dumping environment variables -- chances are they don't need to exfill those credentials to poke around.</p><p>With those practical considerations in mind, we need two steps to fix this issue, the first is a pro-active measure to ensure function get its own IAM role, with scoped down permissions for only what the function needs to do.</p><p>This means that even if the access keys of a lambda function were compromised, the attackers couldn't do anymore damage, then they could have already done.</p><p>The second fix, is about addressing the problem of the specific lost key. For this we need to fix the compromised function <strong>first</strong> -- revoking any specific access key isn't going help, as the attackers could compromise the same function again to obtain newer access keys, just like a genie that grants more wishes.</p><p>Assuming we plugged the hole in the function, we still have the issue of a 12-hour token, that (as we saw previously) can live long past the expiration of the function.</p><p>We can't revoke the temporary token itself, even if we knew the exact access key with ID, AWS doesn't have this capability (AFAIK). Instead, we could revoke the IAM Role that the token belongs to. This is tough, because revoking that role will damage all instances of your function as well.</p><p>Fortunately, if we use Serverless framework, and the <em><a href=https://www.npmjs.com/package/serverless-iam-roles-per-function target=_blank rel=\"noreferrer noopener\">IAM-Roles-Per-Function</a></em> plugin, we can rename the IAM role of the function, and re-deploy the stack. This will invalidate the previously issued tokens (as the role would be deleted), and redeploy a new version of the function pointing to the new IAM role. Hence, we invalidate the old tokens, but keep our current function executing correctly.</p><p><img src=/uploads/serverless-role_hu_2e92446c21c2f825.png srcset=\"/uploads/serverless-role_hu_25f050c0e86876fc.png 480w,\n<a href=https://keithrozario.com/2020/05/contact-tracing-apps-in-this-context-their-ok/ class=\"button inline next\">[<span class=button__text>Contact Tracing Apps: they're OK.</span>] ></a></div></div><div class=archived-comments><h3>Archived Comments</h3><div class=comment style=\"margin-bottom:20px;border-bottom:1px solid #333;padding-bottom:10px\"><div class=comment-header style=margin-bottom:5px><img src=\"https://www.gravatar.com/avatar/dd5d520617fb26aeb18b8570324c0fcc?s=40&d=mm\" alt=c style=vertical-align:middle;border-radius:50%;margin-right:10px>\nProcess Group PGID: 22488"
  }
}
```

---

## MODEL âœ¨

The verification confirms that the images in all three posts ("Chip and Pin", "Gov TLS Audit", and "Access Keys in AWS Lambda") are now rendering correctly with proper `<img>` tags, and no raw Markdown syntax is visible.

-   **Chip and Pin:** Images like `19205891971_...`, `signature.jpg`, and `7212980434_...` are rendering as `<img>`.
-   **Gov TLS Audit:** Images like `GovTLSAudit_Architecture.png`, `gitHub.png`, `IAM.png`, etc., are rendering as `<img>` (some with `srcset` for responsiveness).
-   **Access Keys in AWS Lambda:** Images like `lambda-sts.png`, `cold-start.png`, etc., are rendering as `<img>`.

The issue is fully resolved.

---

## USER ğŸ§‘â€ğŸ’»

Ok, please update the instructions.md file to ensure the user knows which location to upload images too, and a brief note on the how images get re-sized.

---

## MODEL âœ¨

I will read the `instructions.md` file to determine the best location for the new information. Then, I'll update the file to include the correct image upload path and a brief explanation of the responsive image resizing process.**Tool Command**:
```json
{
  "name": "read_file",
  "args": {
    "file_path": "instructions.md"
  }
}
```

---

## USER ğŸ§‘â€ğŸ’»

**Tool Response**:
```json
{
  "id": "read_file-1771121510800-538eba16fec41",
  "name": "read_file",
  "response": {
    "output": "# Blog Management Instructions\n\nThis blog has been converted from WordPress to Hugo using the `terminal` theme.\n\n## 1. Serving the site locally\n\nTo preview your blog locally with live-reloading:\n\n1.  **Navigate to the blog directory:**\n    ```bash\n    cd keithrozario_blog\n    ```\n2.  **Start the Hugo server:**\n    ```bash\n    hugo server\n    ```\n    *Note: If `hugo` is not installed on your system, you can download it from the [Hugo GitHub Releases](https://github.com/gohugoio/hugo/releases).*\n\n3.  **View the site:**\n    Open your browser and go to `http://localhost:1313`.\n\n## 2. Adding a new post\n\n1.  **Create the post file:**\n    From inside the `keithrozario_blog` directory, run:\n    ```bash\n    hugo new posts/my-new-post.md\n    ```\n2.  **Edit the content:**\n    Open `content/posts/my-new-post.md` in your favorite editor.\n3.  **Update Front Matter:**\n    The top of the file contains \"Front Matter\" in TOML format. Ensure it looks like this:\n    ```toml\n    +++\n    title = \"My New Post\"\n    date = \"2026-02-14T12:00:00Z\"\n    draft = false\n    tags = [\"example\", \"hugo\"]\n    categories = [\"General\"]\n    +++\n    ```\n4.  **Add your content:**\n    Write your post using standard Markdown below the second `+++`.\n\n## 3. Managing Images\n\n*   **Uploads:** All your existing WordPress images are in `static/uploads/`.\n*   **Adding New Images:** Place any new images in `static/uploads/`.\n*   **Referencing Images:** In your Markdown posts, reference them using absolute paths:\n    ```markdown\n    ![Alt text](/uploads/your-image.png)\n    ```\n\n## 4. Building for Production\n\nTo generate the final static files (found in the `public/` folder):\n```bash\ncd keithrozario_blog\nhugo\n```\n"
  }
}
```

---

## MODEL âœ¨

I'll update the `instructions.md` file to reflect the new image management workflow. I'll correct the upload path to `assets/uploads/` and add a section explaining Hugo's responsive image processing, including how it automatically generates `srcset` for large images. I'll also emphasize the use of pure Markdown syntax for images to ensure the render hook functions correctly.**Tool Command**:
```json
{
  "name": "replace",
  "args": {
    "old_string": "## 3. Managing Images\n\n*   **Uploads:** All your existing WordPress images are in `static/uploads/`.\n*   **Adding New Images:** Place any new images in `static/uploads/`.\n*   **Referencing Images:** In your Markdown posts, reference them using absolute paths:\n    ```markdown\n    ![Alt text](/uploads/your-image.png)\n    ```",
    "file_path": "/home/keith_krozario_altostrat_com/projects/keithrozario_dot_com/instructions.md",
    "new_string": "## 3. Managing Images\n\n*   **Uploads Location:** All images are stored in `assets/uploads/`. This allows Hugo to process them using Hugo Pipes (resizing, optimizing).\n*   **Adding New Images:** Place any new images in `assets/uploads/`.\n*   **Referencing Images:** Use standard Markdown syntax. Do NOT use HTML tags (like `<figure>` or `<div>`) around images, as this prevents Hugo from processing them:\n    ```markdown\n    ![Alt text](/uploads/your-image.png)\n    ```\n*   **Responsive Resizing:** \n    - The blog uses a custom image render hook (`layouts/_default/_markup/render-image.html`).\n    - If an image width is greater than 1024px, Hugo automatically generates a `srcset` with multiple sizes (480w, 768w, 1024w) for responsive loading.\n    - Images are lazily loaded by default for performance.",
    "expected_replacements": 1,
    "instruction": "Update image management section with new asset path and resizing note."
  }
}
```

---

## USER ğŸ§‘â€ğŸ’»

**Tool Response**:
```json
{
  "id": "replace-1771121515367-f32b36ad7e069",
  "name": "replace",
  "response": {
    "output": "Successfully modified file: /home/keith_krozario_altostrat_com/projects/keithrozario_dot_com/instructions.md (1 replacements)."
  }
}
```

---

## MODEL âœ¨

The `instructions.md` file has been updated with the correct image upload path and a note on responsive resizing.

Is there anything else I can help you with?

---

## USER ğŸ§‘â€ğŸ’»

Take a look at @instructions.md and make some suggestions on how we can clearly communicate the users how to add new post, with images, and how the post are built and published.
--- Content from referenced files ---
Content from @instructions.md:
# Blog Management Instructions

This blog has been converted from WordPress to Hugo using the `terminal` theme.

## 1. Serving the site locally

To preview your blog locally with live-reloading:

1.  **Navigate to the blog directory:**
    ```bash
    cd keithrozario_blog
    ```
2.  **Start the Hugo server:**
    ```bash
    hugo server
    ```
    *Note: If `hugo` is not installed on your system, you can download it from the [Hugo GitHub Releases](https://github.com/gohugoio/hugo/releases).*

3.  **View the site:**
    Open your browser and go to `http://localhost:1313`.

## 2. Adding a new post

1.  **Create the post file:**
    From inside the `keithrozario_blog` directory, run:
    ```bash
    hugo new posts/my-new-post.md
    ```
2.  **Edit the content:**
    Open `content/posts/my-new-post.md` in your favorite editor.
3.  **Update Front Matter:**
    The top of the file contains "Front Matter" in TOML format. Ensure it looks like this:
    ```toml
    +++
    title = "My New Post"
    date = "2026-02-14T12:00:00Z"
    draft = false
    tags = ["example", "hugo"]
    categories = ["General"]
    +++
    ```
4.  **Add your content:**
    Write your post using standard Markdown below the second `+++`.

## 3. Managing Images

*   **Uploads Location:** All images are stored in `assets/uploads/`. This allows Hugo to process them using Hugo Pipes (resizing, optimizing).
*   **Adding New Images:** Place any new images in `assets/uploads/`.
*   **Referencing Images:** Use standard Markdown syntax. Do NOT use HTML tags (like `<figure>` or `<div>`) around images, as this prevents Hugo from processing them:
    ```markdown
    ![Alt text](/uploads/your-image.png)
    ```
*   **Responsive Resizing:** 
    - The blog uses a custom image render hook (`layouts/_default/_markup/render-image.html`).
    - If an image width is greater than 1024px, Hugo automatically generates a `srcset` with multiple sizes (480w, 768w, 1024w) for responsive loading.
    - Images are lazily loaded by default for performance.

## 4. Building for Production

To generate the final static files (found in the `public/` folder):
```bash
cd keithrozario_blog
hugo
```
--- End of content ---

---

## MODEL âœ¨

I'll start by checking the Hugo configuration and the GitHub Actions workflow to confirm exactly how URLs are generated and how the site is deployed. This will ensure the instructions I write are 100% accurate.

First, I'll read `keithrozario_blog/hugo.toml` and `.github/workflows/deploy.yaml`.**Tool Command**:
```json
{
  "name": "run_shell_command",
  "args": {
    "command": "cat keithrozario_blog/hugo.toml\ncat .github/workflows/deploy.yaml"
  }
}
```

---

## USER ğŸ§‘â€ğŸ’»

**Tool Response**:
```json
{
  "id": "run_shell_command-1771121724527-cd5db161b3eb3",
  "name": "run_shell_command",
  "response": {
    "output": "Output: baseURL = 'https://www.keithrozario.com/'\nlanguageCode = 'en-us'\ntitle = 'keithrozario.com'\ntheme = 'terminal'\n\n[markup]\n  [markup.goldmark]\n    [markup.goldmark.renderer]\n      unsafe = true\n\n[permalinks]\n  posts = \"/:year/:month/:slug/\"\n\n[params]\n  contentTypeName = \"posts\"\n  themeColor = \"viking\"\n  showMenuItems = 5\n  \n  [params.logo]\n    logoText = \"keithRozario\"\n  \n[menu]\n  [[menu.main]]\n    identifier = \"posts\"\n    name = \"Posts\"\n    url = \"/posts\"\n\n  [[menu.main]]\n    identifier = \"security\"\n    name = \"Security\"\n    url = \"/categories/security-amp-privacy\"\n\n  [[menu.main]]\n    identifier = \"serverless\"\n    name = \"Serverless\"\n    url = \"/categories/serverless\"\n\n  [[menu.main]]\n    identifier = \"favorites\"\n    name = \"Favorites\"\n    url = \"/categories/keiths-favorite-post\"\n\n  [[menu.main]]\n    identifier = \"contact\"\n    name = \"Contact Me\"\n    url = \"/contact\"\nname: Deploy Hugo site to Pages\n\non:\n  # Runs on pushes targeting the default branch\n  push:\n    branches: [\"main\"]\n\n  # Allows you to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\n# Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages\npermissions:\n  contents: read\n  pages: write\n  id-token: write\n\n# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.\n# However, do NOT cancel in-progress runs as we want to allow these production deployments to complete.\nconcurrency:\n  group: \"pages\"\n  cancel-in-progress: false\n\n# Default to bash\ndefaults:\n  run:\n    shell: bash\n\njobs:\n  # Build job\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n        with:\n          submodules: recursive\n          fetch-depth: 0\n\n      - name: Setup Hugo\n        uses: peaceiris/actions-hugo@v3\n        with:\n          hugo-version: 'latest'\n          extended: true\n\n      - name: Setup Pages\n        id: pages\n        uses: actions/configure-pages@v5\n\n      - name: Install Node.js dependencies\n        run: \"[[ -f package-lock.json || -f npm-shrinkwrap.json ]] && npm ci || true\"\n\n      - name: Build with Hugo\n        env:\n          # For maximum backward compatibility with Hugo modules\n          HUGO_ENVIRONMENT: production\n          HUGO_ENV: production\n        run: |\n          cd keithrozario_blog && hugo \\\n            --gc \\\n            --minify \\\n            --baseURL \"${{ steps.pages.outputs.base_url }}/\"\n\n      - name: Upload artifact\n        uses: actions/upload-pages-artifact@v3\n        with:\n          path: ./keithrozario_blog/public\n\n  # Deployment job\n  deploy:\n    environment:\n      name: github-pages\n      url: ${{ steps.deployment.outputs.page_url }}\n    runs-on: ubuntu-latest\n    needs: build\n    steps:\n      - name: Deploy to GitHub Pages\n        id: deployment\n        uses: actions/deploy-pages@v4\nProcess Group PGID: 24112"
  }
}
```

---

## MODEL âœ¨

**Analysis:**

1.  **URL Structure:**
    - `[permalinks] posts = "/:year/:month/:slug/"`
    - This confirms that posts will follow the `year/month/slug` structure automatically based on the `date` in front matter.
    - If the user provides `slug = "my-slug"` in front matter, it will use that. If not, it will slugify the title.
    - So for new posts, the user just needs to set a `date` and `title`. They can optionally set `slug` if they want a specific URL segment.

2.  **Deployment:**
    - `deploy.yaml` runs `hugo` command inside the workflow.
    - This means the user **does NOT** need to commit the `public/` folder.
    - They just need to commit the source (`content/`, `assets/`, etc.).
    - The action `peaceiris/actions-hugo` installs Hugo extended.
    - The command is `cd keithrozario_blog && hugo ...`.

**Conclusion:**
The current `instructions.md` says:
> To generate the final static files (found in the public/ folder):
> `hugo`

This is correct for *local* builds, but misleading for *publishing*. I should clarify that committing to `main` triggers the build and deploy.

**Drafting Improvements:**

1.  **Create New Post:**
    - Command: `hugo new posts/title.md`.
    - Front matter: Explain `title`, `date`, `draft`, `categories`.
    - Mention that `slug` is optional but good for clean URLs.
    - Mention the permalink structure is automatic (`/:year/:month/:slug/`).

2.  **Images:**
    - Explicitly state: Place in `keithrozario_blog/assets/uploads/`.
    - Reference: `![Alt](/uploads/image.png)`.
    - Note: No HTML.

3.  **Publishing:**
    - Step 1: `git add .`
    - Step 2: `git commit -m "New post: Title"`
    - Step 3: `git push origin main`
    - Note: GitHub Actions will handle the rest.

I will update `instructions.md` now. I'll rewrite it to be a complete guide.

Wait, I should check if `assets/uploads` exists. I know I moved files there, but let's double check the path relative to project root.
Project root: `/home/keith_krozario_altostrat_com/projects/keithrozario_dot_com/`
Hugo root: `keithrozario_blog/`
So images go into `keithrozario_blog/assets/uploads/`.

Let's do it.

One detail: `migrate_robust.py` was generating slugs explicitly to match legacy URLs. For *new* posts, does the user care about specific slugs? Probably not as much, but mentioning `slug` is good practice.

I will update the file.

---

## USER ğŸ§‘â€ğŸ’»

Go ahead

---

## MODEL âœ¨

I'll revise `instructions.md` to serve as a comprehensive guide for managing the blog. This update will clarify the process for creating new posts, managing images (including the crucial role of the `assets/uploads/` directory), and deploying changes via Git. I'll also add a dedicated section explaining the permalink structure to ensure consistent URL generation.**Tool Command**:
```json
{
  "name": "write_file",
  "args": {
    "file_path": "/home/keith_krozario_altostrat_com/projects/keithrozario_dot_com/instructions.md",
    "content": "# Blog Management Instructions\n\nThis blog has been converted from WordPress to Hugo using the `terminal` theme. The source code is hosted on GitHub, and deployment is handled automatically via GitHub Actions.\n\n## 1. Serving the site locally\n\nTo preview your blog locally with live-reloading:\n\n1.  **Navigate to the blog directory:**\n    ```bash\n    cd keithrozario_blog\n    ```\n2.  **Start the Hugo server:**\n    ```bash\n    hugo server\n    ```\n    *Note: If `hugo` is not installed on your system, you can download it from the [Hugo GitHub Releases](https://github.com/gohugoio/hugo/releases).*\n\n3.  **View the site:**\n    Open your browser and go to `http://localhost:1313`.\n\n## 2. Adding a new post\n\n1.  **Create the post file:**\n    From inside the `keithrozario_blog` directory, run:\n    ```bash\n    hugo new posts/my-new-post.md\n    ```\n2.  **Edit the content:**\n    Open `content/posts/my-new-post.md` in your favorite editor.\n3.  **Update Front Matter:**\n    The top of the file contains \"Front Matter\" in TOML format. Ensure it looks like this:\n    ```toml\n    +++\n    title = \"My New Post\"\n    date = \"2026-02-14T12:00:00Z\"\n    draft = false\n    tags = [\"example\", \"hugo\"]\n    categories = [\"General\"]\n    +++\n    ```\n    *Tip: The URL will automatically be generated as `/year/month/slug/` based on the date and title. You can override the slug by adding `slug = \"my-custom-url\"` to the front matter.*\n\n4.  **Add your content:**\n    Write your post using standard Markdown below the second `+++`.\n\n## 3. Managing Images\n\n*   **Location:** All images MUST be stored in `keithrozario_blog/assets/uploads/`.\n    *   *Why?* Placing images here allows Hugo to process them (resize, optimize) using Hugo Pipes.\n*   **Adding New Images:** Simply copy your image files (e.g., `my-image.png`) into `keithrozario_blog/assets/uploads/`.\n*   **Referencing Images:** Use standard Markdown syntax. **Do NOT use HTML tags** (like `<figure>` or `<div>`) around images, as this prevents Hugo from processing them:\n    ```markdown\n    ![Alt text](/uploads/my-image.png)\n    ```\n*   **Responsive Behavior:** \n    - The site uses a custom render hook (`layouts/_default/_markup/render-image.html`).\n    - Large images (>1024px width) are automatically resized to multiple widths (480px, 768px, 1024px) and served with `srcset` for faster loading on mobile devices.\n    - Images are lazy-loaded by default.\n\n## 4. Publishing Changes\n\nYou do **not** need to manually build the site. GitHub Actions will automatically build and deploy your changes when you push to the `main` branch.\n\n1.  **Stage your changes:**\n    ```bash\n    git add .\n    ```\n2.  **Commit your changes:**\n    ```bash\n    git commit -m \"Add new post: My New Post\"\n    ```\n3.  **Push to GitHub:**\n    ```bash\n    git push origin main\n    ```\n\n4.  **Wait for Deployment:**\n    - Go to the \"Actions\" tab in your GitHub repository to see the deployment progress.\n    - Once the workflow completes (usually 1-2 minutes), your changes will be live at `https://keithrozario.com`.\n"
  }
}
```

---

## USER ğŸ§‘â€ğŸ’»

**Tool Response**:
```json
{
  "id": "write_file-1771121900036-06a3783a5ffa7",
  "name": "write_file",
  "response": {
    "output": "Successfully overwrote file: /home/keith_krozario_altostrat_com/projects/keithrozario_dot_com/instructions.md."
  }
}
```

---

## MODEL âœ¨

I have updated the `instructions.md` file.

I'm done with the request. Is there anything else you need?